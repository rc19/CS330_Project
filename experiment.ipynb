{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Utility functions. \"\"\"\n",
    "import pdb\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "## Loss utilities\n",
    "def cross_entropy_loss(pred, label, k_shot):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data loading scripts\"\"\"\n",
    "from scipy import misc\n",
    "import imageio\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from PIL import Image\n",
    "\n",
    "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    Args:\n",
    "    paths: A list of character folders\n",
    "    labels: List or numpy array of same length as paths\n",
    "    n_samples: Number of images to retrieve per character\n",
    "    Returns:\n",
    "    List of (label, image_path) tuples\n",
    "    \"\"\"\n",
    "    if n_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, n_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "           for i, path in zip(labels, paths)\n",
    "           for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels\n",
    "\n",
    "\n",
    "def image_file_to_array(filename, dim_input, shear=None, scale=None):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns numpy array\n",
    "    Args:\n",
    "    filename: Image filename\n",
    "    dim_input: Flattened shape of image\n",
    "    Returns:\n",
    "    1 channel image\n",
    "    \"\"\"\n",
    "    if shear is not None or scale is not None:\n",
    "        im = Image.open(filename)\n",
    "        im = im.resize((28,28), resample=Image.LANCZOS)\n",
    "        im.save('tmpfile.png')\n",
    "        image = imageio.imread('tmpfile.png')\n",
    "    else:\n",
    "        image = imageio.imread(filename.replace('omniglot','omniglot_resized'))\n",
    "    \n",
    "    if shear is not None:\n",
    "        afine_tf = transform.AffineTransform(shear=shear)\n",
    "        image = transform.warp(image, inverse_map=afine_tf)*255.0\n",
    "\n",
    "    elif scale is not None:\n",
    "        afine_tf = transform.AffineTransform(scale=scale)*255.0\n",
    "        image = transform.warp(image, inverse_map=afine_tf)\n",
    "\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255\n",
    "    image = 1.0 - image\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_classes: Number of classes for classification (K-way)\n",
    "          num_samples_per_class: num samples to generate per class in one batch\n",
    "          num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
    "          num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
    "          batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "        self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
    "        self.num_meta_test_classes = num_meta_test_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './omniglot')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                   for family in os.listdir(data_folder)\n",
    "                   if os.path.isdir(os.path.join(data_folder, family))\n",
    "                   for character in os.listdir(os.path.join(data_folder, family))\n",
    "                   if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "\n",
    "        random.seed(123)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[\n",
    "          num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[\n",
    "          num_train + num_val:]\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False, shear=None, scale=None):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "          batch_type: meta_train/meta_val/meta_test\n",
    "          shuffle: randomly shuffle classes or not\n",
    "          swap: swap number of classes (N) and number of samples per class (K) or not\n",
    "        Returns:\n",
    "          A a tuple of (1) Image batch and (2) Label batch where\n",
    "          image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
    "          where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"meta_train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        elif batch_type == \"meta_val\":\n",
    "            folders = self.metaval_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "            num_classes = self.num_meta_test_classes\n",
    "            num_samples_per_class = self.num_meta_test_samples_per_class\n",
    "\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        for i in range(batch_size):\n",
    "            sampled_character_folders = random.sample(\n",
    "            folders, num_classes)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(\n",
    "            num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
    "            labels = [li[0] for li in labels_and_images]\n",
    "            images = [image_file_to_array(\n",
    "            li[1], self.dim_input,shear,scale) for li in labels_and_images]\n",
    "            images = np.stack(images)\n",
    "            labels = np.array(labels).astype(np.int32)\n",
    "            labels = np.reshape(\n",
    "            labels, (num_classes, num_samples_per_class))\n",
    "            labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
    "            images = np.reshape(\n",
    "            images, (num_classes, num_samples_per_class, -1))\n",
    "\n",
    "            batch = np.concatenate([labels, images], 2)\n",
    "            if shuffle:\n",
    "                for p in range(num_samples_per_class):\n",
    "                    np.random.shuffle(batch[:, p])\n",
    "\n",
    "            labels = batch[:, :, :num_classes]\n",
    "            images = batch[:, :, num_classes:]\n",
    "\n",
    "            if swap:\n",
    "                labels = np.swapaxes(labels, 0, 1)\n",
    "                images = np.swapaxes(images, 0, 1)\n",
    "\n",
    "            all_image_batches.append(images)\n",
    "            all_label_batches.append(labels)\n",
    "        all_image_batches = np.stack(all_image_batches)\n",
    "        all_label_batches = np.stack(all_label_batches)\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RelationNet\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "class RelationNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_filters = 64, hidden_dim = 8):\n",
    "        super(RelationNet, self).__init__()\n",
    "#         self.num_filters = num_filters\n",
    "#         # self.latent_dim = latent_dim\n",
    "#         num_filter_list = [self.num_filters]*2\n",
    "#         self.convs = []\n",
    "#         for i, num_filter in enumerate(num_filter_list):\n",
    "#             block_parts = [\n",
    "#             layers.Conv2D(\n",
    "#               filters=num_filter,\n",
    "#               kernel_size=3,\n",
    "#               padding='SAME',\n",
    "#               activation='linear'),\n",
    "#             ]\n",
    "\n",
    "#             block_parts += [layers.BatchNormalization(momentum = 1)]\n",
    "#             block_parts += [layers.Activation('relu')]\n",
    "#             block_parts += [layers.MaxPool2D()]\n",
    "#             block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
    "#             self.__setattr__(\"conv%d\" % i, block)\n",
    "#             self.convs.append(block)\n",
    "#         self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = layers.Dense(hidden_dim,activation = 'relu')\n",
    "        self.fc2 = layers.Dense(1,activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inp):\n",
    "        out = inp\n",
    "        # for conv in self.convs:\n",
    "          # out = conv(out)\n",
    "        # out = self.flatten(out)\n",
    "        out = self.fc2(self.fc1(out))\n",
    "        return out\n",
    "\n",
    "def RelationLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries, relation_net):\n",
    "    \"\"\"\n",
    "    calculates the prototype network loss using the latent representation of x\n",
    "    and the latent representation of the query set\n",
    "    Args:\n",
    "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "      num_classes: number of classes (N) for classification\n",
    "      num_support: number of examples (S) in the support set\n",
    "      num_queries: number of examples (Q) in the query set\n",
    "    Returns:\n",
    "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "      acc: the accuracy of classification on the queries\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x_latent, shape=[num_classes, num_support, -1])\n",
    "    # print(x.numpy().shape)\n",
    "    prototypes = tf.reduce_mean(x, axis = 1, keepdims=False) #(N, D)\n",
    "    prototypes_ext = tf.repeat(prototypes, repeats = [num_classes*num_queries]*num_classes, axis = 0) #(N*Q*N, D)\n",
    "    q_ext = tf.tile(q_latent, multiples = [num_classes,1]) # (N*N*Q, D)\n",
    "    labels_ = tf.reshape(labels_onehot, [-1, num_classes]) #(N*Q, N)\n",
    "    scores_ = relation_net(tf.concat([q_ext, prototypes_ext], axis = -1)) #(N*NQ, 1)\n",
    "    scores_ = tf.split(scores_,num_classes,0)\n",
    "    scores = tf.concat(scores_, axis = -1) #(NQ,N)\n",
    "\n",
    "    loss = losses.mean_squared_error(labels_, scores) #(NQ,)\n",
    "    loss = tf.reduce_mean(loss) #(1,)\n",
    "\n",
    "    # ce_loss = cross_entropy_loss(dist,tf.reshape(labels_onehot, [-1, num_classes]), k_shot=1)\n",
    "    acc = accuracy(tf.argmax(input=labels_, axis=1), tf.argmax(input=scores, axis=1))\n",
    "    # print(ce_loss.numpy())\n",
    "    # print(acc.numpy())\n",
    "\n",
    "    # print(error)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "#Shape checking\n",
    "\n",
    "# q = tf.random.uniform(shape = [10,5])\n",
    "# print(q.shape)\n",
    "# q_ext = tf.tile(q, multiples = [2,1])\n",
    "# q_rep = tf.repeat(q, repeats = [2], axis = 0)\n",
    "# print(q_rep.shape)\n",
    "# print(q)\n",
    "# print(q_rep)\n",
    "# print(tf.concat([q,q], axis = -1))\n",
    "\n",
    "\n",
    "# t = tf.random.uniform(shape = [12,1])\n",
    "# t_ = tf.split(t,3,0)\n",
    "# t__ = tf.concat(t_, axis = -1)\n",
    "\n",
    "# t1 = tf.random.uniform(shape = [12,1])\n",
    "# t1_ = tf.split(t1,3,0)\n",
    "# t1__ = tf.concat(t1_, axis = -1)\n",
    "\n",
    "# print(losses.mean_squared_error(t__,t1__,))\n",
    "\n",
    "# print(t)\n",
    "# print(t_)\n",
    "# print(t__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/ProtoNet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ProtoNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_filters, latent_dim):\n",
    "    super(ProtoNet, self).__init__()\n",
    "    self.num_filters = num_filters\n",
    "    self.latent_dim = latent_dim\n",
    "    num_filter_list = self.num_filters + [latent_dim]\n",
    "    self.convs = []\n",
    "    for i, num_filter in enumerate(num_filter_list):\n",
    "      block_parts = [\n",
    "        layers.Conv2D(\n",
    "          filters=num_filter,\n",
    "          kernel_size=3,\n",
    "          padding='SAME',\n",
    "          activation='linear'),\n",
    "      ]\n",
    "\n",
    "      block_parts += [layers.BatchNormalization()]\n",
    "      block_parts += [layers.Activation('relu')]\n",
    "      block_parts += [layers.MaxPool2D()]\n",
    "      block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
    "      self.__setattr__(\"conv%d\" % i, block)\n",
    "      self.convs.append(block)\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "  def call(self, inp):\n",
    "    out = inp\n",
    "    for conv in self.convs:\n",
    "      out = conv(out)\n",
    "    out = self.flatten(out)\n",
    "    return out\n",
    "\n",
    "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
    "  \"\"\"\n",
    "    calculates the prototype network loss using the latent representation of x\n",
    "    and the latent representation of the query set\n",
    "    Args:\n",
    "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "      num_classes: number of classes (N) for classification\n",
    "      num_support: number of examples (S) in the support set\n",
    "      num_queries: number of examples (Q) in the query set\n",
    "    Returns:\n",
    "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "      acc: the accuracy of classification on the queries\n",
    "  \"\"\"\n",
    "  x_latent = tf.reshape(x_latent,[num_classes,num_support,-1])\n",
    "  c = tf.reduce_mean(x_latent,axis=1)\n",
    "  def dist(q, c):\n",
    "    N_Q, D = q.shape\n",
    "    N = c.shape[0]\n",
    "    q = tf.tile(tf.expand_dims(q, axis=1), [1, N, 1])\n",
    "    c = tf.tile(tf.expand_dims(c, axis=0), [N_Q, 1, 1])\n",
    "    return tf.reduce_mean(tf.square(q-c), axis=2)\n",
    "  distances = tf.reshape(dist(q_latent, c), [num_classes,num_queries,num_classes])\n",
    "  ce_loss = cross_entropy_loss(-distances, labels_onehot, 1)\n",
    "  acc = accuracy(tf.argmax(labels_onehot, axis=-1),tf.argmax(tf.reshape(tf.nn.softmax(-distances), \n",
    "                                                                 [num_classes, num_queries, -1]), axis=-1))\n",
    "  #############################\n",
    "  return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ProtoNet\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def proto_net_train_step(model, optim, x, q, labels_ph, rel=None):\n",
    "    num_classes, num_support, im_height, im_width, channels = x.shape\n",
    "    num_queries = q.shape[1]\n",
    "    x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
    "    q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_latent = model(x)\n",
    "        q_latent = model(q)\n",
    "        if rel is None:\n",
    "            ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
    "        else:\n",
    "            ce_loss, acc = RelationLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries,rel)\n",
    "\n",
    "    if rel is None:\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    else:\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables+rel.trainable_variables)\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables+rel.trainable_variables))\n",
    "    \n",
    "    return ce_loss, acc\n",
    "\n",
    "def proto_net_eval(model, x, q, labels_ph,rel=None):\n",
    "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
    "  num_queries = q.shape[1]\n",
    "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
    "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
    "\n",
    "  x_latent = model(x)\n",
    "  q_latent = model(q)\n",
    "  if rel is None:\n",
    "    ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
    "  else:\n",
    "    ce_loss, acc = RelationLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries,rel)\n",
    "\n",
    "  return ce_loss, acc \n",
    "\n",
    "def run_protonet(data_path='./omniglot', n_way=20, k_shot=1, n_query=5, n_meta_test_way=20, \n",
    "                 k_meta_test_shot=5, n_meta_test_query=5,shear=None,scale=None, rel=False, n_epochs=20, latent_dim = 16):\n",
    "  \n",
    "  n_episodes = 100\n",
    "\n",
    "  im_width, im_height, channels = 28, 28, 1\n",
    "  num_filters = 32\n",
    "  num_conv_layers = 3\n",
    "  n_meta_test_episodes = 1000\n",
    "\n",
    "  model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
    "  if not rel:\n",
    "    rel_model=None\n",
    "  else:\n",
    "    rel_model = RelationNet()\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # call DataGenerator with k_shot+n_query samples per class\n",
    "  data_generator = DataGenerator(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query)\n",
    "  for ep in range(n_epochs):\n",
    "    for epi in range(n_episodes):\n",
    "      image_batch, label_batch = data_generator.sample_batch('meta_train',1,shuffle=False)\n",
    "      image, label = image_batch[0], label_batch[0]\n",
    "      support,query = image[:,:k_shot,:], image[:,k_shot:,:]\n",
    "      labels = label[:,k_shot:,:]\n",
    "      # Reshape\n",
    "      support = tf.reshape(support,[-1,k_shot,im_width, im_height, channels])\n",
    "      query = tf.reshape(query,[-1,n_query,im_width, im_height, channels])\n",
    "      #############################\n",
    "      ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels, rel=rel_model)\n",
    "      if (epi+1) % 50 == 0:\n",
    "        image_batch, label_batch = data_generator.sample_batch('meta_val',1,shuffle=False)\n",
    "        image, label = image_batch[0], label_batch[0]\n",
    "        support,query = image[:,:k_shot,:], image[:,k_shot:,:]\n",
    "        labels = label[:,k_shot:,:]\n",
    "        # Reshape\n",
    "        support = tf.reshape(support,[-1,k_shot,im_width, im_height, channels])\n",
    "        query = tf.reshape(query,[-1,n_query,im_width, im_height, channels])\n",
    "        #############################\n",
    "        val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels, rel=rel_model)\n",
    "        print('[epo {}/{}, epi {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
    "                                                                    n_epochs,\n",
    "                                                                    epi+1,\n",
    "                                                                    n_episodes,\n",
    "                                                                    ls,\n",
    "                                                                    ac,\n",
    "                                                                    val_ls,\n",
    "                                                                    val_ac))\n",
    "\n",
    "  print('Testing...')\n",
    "  meta_test_accuracies = []\n",
    "  for epi in range(n_meta_test_episodes):\n",
    "    # sample a batch of test data and partition into\n",
    "    # support and query sets\n",
    "    image_batch, label_batch = data_generator.sample_batch('meta_test',1,shuffle=False,shear=shear,scale=scale)\n",
    "    image, label = image_batch[0], label_batch[0]\n",
    "    support,query = image[:,:k_meta_test_shot,:], image[:,k_meta_test_shot:,:]\n",
    "    labels = label[:,k_meta_test_shot:,:]\n",
    "    # Reshape\n",
    "    support = tf.reshape(support,[-1,k_meta_test_shot,im_width, im_height, channels])\n",
    "    query = tf.reshape(query,[-1,n_meta_test_query,im_width, im_height, channels])\n",
    "    #############################\n",
    "    ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels,rel=rel_model)\n",
    "    meta_test_accuracies.append(ac)\n",
    "    if (epi+1) % 50 == 0:\n",
    "      print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
    "  avg_acc = np.mean(meta_test_accuracies)\n",
    "  stds = np.std(meta_test_accuracies)\n",
    "  print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))\n",
    "  # return meta_test_accuracies, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Performing shear=-0.8=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.11286, meta-training acc: 0.52000, meta-val loss: 1.04721, meta-val acc: 0.72000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.55008, meta-training acc: 0.80000, meta-val loss: 0.96159, meta-val acc: 0.60000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.57469, meta-training acc: 0.64000, meta-val loss: 0.43887, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.14046, meta-training acc: 0.48000, meta-val loss: 0.32656, meta-val acc: 0.92000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.40277, meta-training acc: 0.84000, meta-val loss: 0.63474, meta-val acc: 0.68000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.31668, meta-training acc: 0.60000, meta-val loss: 1.05875, meta-val acc: 0.64000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.45109, meta-training acc: 0.76000, meta-val loss: 1.06805, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.35643, meta-training acc: 0.92000, meta-val loss: 1.34486, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.66795, meta-training acc: 0.72000, meta-val loss: 0.54062, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.39953, meta-training acc: 0.88000, meta-val loss: 0.66890, meta-val acc: 0.64000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.71427, meta-training acc: 0.80000, meta-val loss: 0.44437, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.37677, meta-training acc: 0.92000, meta-val loss: 0.76530, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.30600, meta-training acc: 0.88000, meta-val loss: 0.89292, meta-val acc: 0.60000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.65071, meta-training acc: 0.84000, meta-val loss: 0.17466, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.43702, meta-training acc: 0.88000, meta-val loss: 0.32132, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.91698, meta-training acc: 0.60000, meta-val loss: 0.90846, meta-val acc: 0.60000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.67857, meta-training acc: 0.84000, meta-val loss: 0.36720, meta-val acc: 0.80000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.35238, meta-training acc: 0.84000, meta-val loss: 0.31408, meta-val acc: 0.88000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.81417, meta-training acc: 0.68000, meta-val loss: 0.94342, meta-val acc: 0.68000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.57124, meta-training acc: 0.60000, meta-val loss: 1.20247, meta-val acc: 0.56000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.54211, meta-training acc: 0.92000, meta-val loss: 0.21715, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.18298, meta-training acc: 0.96000, meta-val loss: 0.40398, meta-val acc: 0.84000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.38386, meta-training acc: 0.76000, meta-val loss: 0.39081, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.29159, meta-training acc: 0.84000, meta-val loss: 0.05162, meta-val acc: 0.96000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.58237, meta-training acc: 0.88000, meta-val loss: 0.70761, meta-val acc: 0.80000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.64864, meta-training acc: 0.84000, meta-val loss: 0.77664, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 1.50533, meta-training acc: 0.80000, meta-val loss: 0.77954, meta-val acc: 0.68000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.44692, meta-training acc: 0.84000, meta-val loss: 0.17891, meta-val acc: 0.92000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.23369, meta-training acc: 0.96000, meta-val loss: 0.97187, meta-val acc: 0.68000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.65632, meta-training acc: 0.68000, meta-val loss: 0.63599, meta-val acc: 0.76000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.61076, meta-training acc: 0.68000, meta-val loss: 0.39613, meta-val acc: 0.84000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12211, meta-training acc: 1.00000, meta-val loss: 0.23328, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.17744, meta-training acc: 0.88000, meta-val loss: 0.21359, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.15087, meta-training acc: 0.92000, meta-val loss: 0.12783, meta-val acc: 1.00000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.05731, meta-training acc: 1.00000, meta-val loss: 0.49010, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.10589, meta-training acc: 0.96000, meta-val loss: 0.43127, meta-val acc: 0.80000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.80961, meta-training acc: 0.76000, meta-val loss: 0.99608, meta-val acc: 0.56000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.47583, meta-training acc: 0.92000, meta-val loss: 0.22834, meta-val acc: 0.84000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.12115, meta-training acc: 0.92000, meta-val loss: 0.03239, meta-val acc: 1.00000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.04284, meta-training acc: 1.00000, meta-val loss: 0.08322, meta-val acc: 1.00000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.98286, acc: 0.72000\n",
      "[meta-test episode 100/1000] => loss: 1.30932, acc: 0.68000\n",
      "[meta-test episode 150/1000] => loss: 0.58332, acc: 0.88000\n",
      "[meta-test episode 200/1000] => loss: 1.30272, acc: 0.52000\n",
      "[meta-test episode 250/1000] => loss: 0.99331, acc: 0.64000\n",
      "[meta-test episode 300/1000] => loss: 1.15680, acc: 0.48000\n",
      "[meta-test episode 350/1000] => loss: 0.97384, acc: 0.56000\n",
      "[meta-test episode 400/1000] => loss: 1.10739, acc: 0.52000\n",
      "[meta-test episode 450/1000] => loss: 1.27737, acc: 0.56000\n",
      "[meta-test episode 500/1000] => loss: 0.92774, acc: 0.64000\n",
      "[meta-test episode 550/1000] => loss: 0.90398, acc: 0.68000\n",
      "[meta-test episode 600/1000] => loss: 0.65543, acc: 0.80000\n",
      "[meta-test episode 650/1000] => loss: 1.43401, acc: 0.64000\n",
      "[meta-test episode 700/1000] => loss: 1.38960, acc: 0.60000\n",
      "[meta-test episode 750/1000] => loss: 1.06071, acc: 0.64000\n",
      "[meta-test episode 800/1000] => loss: 1.10157, acc: 0.64000\n",
      "[meta-test episode 850/1000] => loss: 0.95201, acc: 0.56000\n",
      "[meta-test episode 900/1000] => loss: 1.24809, acc: 0.48000\n",
      "[meta-test episode 950/1000] => loss: 1.41594, acc: 0.48000\n",
      "[meta-test episode 1000/1000] => loss: 1.35030, acc: 0.40000\n",
      "Average Meta-Test Accuracy: 0.59724, Meta-Test Accuracy Std: 0.13952\n",
      "\n",
      "========================Performing shear=-0.6=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.20977, meta-training acc: 0.40000, meta-val loss: 1.13744, meta-val acc: 0.72000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.60897, meta-training acc: 0.84000, meta-val loss: 1.03526, meta-val acc: 0.60000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.65745, meta-training acc: 0.64000, meta-val loss: 0.65553, meta-val acc: 0.76000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.19105, meta-training acc: 0.52000, meta-val loss: 0.30799, meta-val acc: 0.96000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.54942, meta-training acc: 0.88000, meta-val loss: 0.64111, meta-val acc: 0.68000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.19136, meta-training acc: 0.56000, meta-val loss: 0.86081, meta-val acc: 0.56000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.61001, meta-training acc: 0.80000, meta-val loss: 0.95759, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.29034, meta-training acc: 0.88000, meta-val loss: 1.97615, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.69364, meta-training acc: 0.68000, meta-val loss: 0.69931, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.44958, meta-training acc: 0.88000, meta-val loss: 0.65392, meta-val acc: 0.64000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.79468, meta-training acc: 0.64000, meta-val loss: 0.45153, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.39299, meta-training acc: 0.96000, meta-val loss: 0.74152, meta-val acc: 0.68000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.12084, meta-training acc: 1.00000, meta-val loss: 0.86316, meta-val acc: 0.76000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.54821, meta-training acc: 0.88000, meta-val loss: 0.27545, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.62456, meta-training acc: 0.88000, meta-val loss: 0.55041, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.83361, meta-training acc: 0.80000, meta-val loss: 0.87168, meta-val acc: 0.68000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.31075, meta-training acc: 0.92000, meta-val loss: 0.66941, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.30448, meta-training acc: 0.92000, meta-val loss: 0.30051, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.57190, meta-training acc: 0.68000, meta-val loss: 1.03869, meta-val acc: 0.64000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.30742, meta-training acc: 0.64000, meta-val loss: 0.89467, meta-val acc: 0.60000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.19738, meta-training acc: 0.96000, meta-val loss: 0.26992, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.11518, meta-training acc: 0.96000, meta-val loss: 0.38494, meta-val acc: 0.84000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.30226, meta-training acc: 0.84000, meta-val loss: 0.25653, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.47361, meta-training acc: 0.76000, meta-val loss: 0.07315, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.71083, meta-training acc: 0.80000, meta-val loss: 0.22014, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.52814, meta-training acc: 0.80000, meta-val loss: 0.20555, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.68051, meta-training acc: 0.80000, meta-val loss: 0.58935, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.64791, meta-training acc: 0.80000, meta-val loss: 0.11167, meta-val acc: 0.96000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.12535, meta-training acc: 1.00000, meta-val loss: 1.14397, meta-val acc: 0.80000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.69448, meta-training acc: 0.76000, meta-val loss: 0.77201, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.68937, meta-training acc: 0.68000, meta-val loss: 0.37485, meta-val acc: 0.92000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12736, meta-training acc: 1.00000, meta-val loss: 0.24674, meta-val acc: 0.84000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.16454, meta-training acc: 0.92000, meta-val loss: 0.12861, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.11035, meta-training acc: 0.96000, meta-val loss: 0.49612, meta-val acc: 0.84000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.16533, meta-training acc: 0.92000, meta-val loss: 0.36891, meta-val acc: 0.84000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.06835, meta-training acc: 1.00000, meta-val loss: 0.24480, meta-val acc: 0.96000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.70887, meta-training acc: 0.76000, meta-val loss: 0.61753, meta-val acc: 0.76000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.34920, meta-training acc: 0.92000, meta-val loss: 0.19960, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.14266, meta-training acc: 1.00000, meta-val loss: 0.12539, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.02262, meta-training acc: 1.00000, meta-val loss: 0.32040, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 1.05997, acc: 0.76000\n",
      "[meta-test episode 100/1000] => loss: 0.99946, acc: 0.68000\n",
      "[meta-test episode 150/1000] => loss: 0.47904, acc: 0.88000\n",
      "[meta-test episode 200/1000] => loss: 1.05285, acc: 0.64000\n",
      "[meta-test episode 250/1000] => loss: 0.77427, acc: 0.60000\n",
      "[meta-test episode 300/1000] => loss: 1.26797, acc: 0.60000\n",
      "[meta-test episode 350/1000] => loss: 0.69293, acc: 0.76000\n",
      "[meta-test episode 400/1000] => loss: 1.45781, acc: 0.48000\n",
      "[meta-test episode 450/1000] => loss: 1.16095, acc: 0.60000\n",
      "[meta-test episode 500/1000] => loss: 0.61833, acc: 0.72000\n",
      "[meta-test episode 550/1000] => loss: 0.74052, acc: 0.80000\n",
      "[meta-test episode 600/1000] => loss: 0.44130, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 0.78208, acc: 0.76000\n",
      "[meta-test episode 700/1000] => loss: 1.00931, acc: 0.64000\n",
      "[meta-test episode 750/1000] => loss: 0.91586, acc: 0.68000\n",
      "[meta-test episode 800/1000] => loss: 1.02986, acc: 0.64000\n"
     ]
    }
   ],
   "source": [
    "for p in range(-8,10,2):\n",
    "    p = p/10\n",
    "    print(f'\\n========================Performing shear={p}=========================\\n')\n",
    "    run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,shear=p,scale=None,latent_dim = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Performing shear=-0.8=========================\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "[epo 1/50, epi 50/100] => meta-training loss: 0.16085, meta-training acc: 0.20000, meta-val loss: 0.16021, meta-val acc: 0.20000\n",
      "[epo 1/50, epi 100/100] => meta-training loss: 0.16059, meta-training acc: 0.20000, meta-val loss: 0.16013, meta-val acc: 0.20000\n",
      "[epo 2/50, epi 50/100] => meta-training loss: 0.16033, meta-training acc: 0.20000, meta-val loss: 0.16001, meta-val acc: 0.20000\n",
      "[epo 2/50, epi 100/100] => meta-training loss: 0.16011, meta-training acc: 0.20000, meta-val loss: 0.15955, meta-val acc: 0.20000\n",
      "[epo 3/50, epi 50/100] => meta-training loss: 0.15922, meta-training acc: 0.20000, meta-val loss: 0.15934, meta-val acc: 0.20000\n",
      "[epo 3/50, epi 100/100] => meta-training loss: 0.15748, meta-training acc: 0.28000, meta-val loss: 0.15556, meta-val acc: 0.36000\n",
      "[epo 4/50, epi 50/100] => meta-training loss: 0.16115, meta-training acc: 0.44000, meta-val loss: 0.15550, meta-val acc: 0.36000\n",
      "[epo 4/50, epi 100/100] => meta-training loss: 0.12799, meta-training acc: 0.56000, meta-val loss: 0.14161, meta-val acc: 0.52000\n",
      "[epo 5/50, epi 50/100] => meta-training loss: 0.12316, meta-training acc: 0.48000, meta-val loss: 0.12848, meta-val acc: 0.56000\n",
      "[epo 5/50, epi 100/100] => meta-training loss: 0.15450, meta-training acc: 0.36000, meta-val loss: 0.15239, meta-val acc: 0.32000\n",
      "[epo 6/50, epi 50/100] => meta-training loss: 0.14487, meta-training acc: 0.36000, meta-val loss: 0.12578, meta-val acc: 0.56000\n",
      "[epo 6/50, epi 100/100] => meta-training loss: 0.11133, meta-training acc: 0.68000, meta-val loss: 0.13935, meta-val acc: 0.52000\n",
      "[epo 7/50, epi 50/100] => meta-training loss: 0.11479, meta-training acc: 0.60000, meta-val loss: 0.15564, meta-val acc: 0.36000\n",
      "[epo 7/50, epi 100/100] => meta-training loss: 0.12780, meta-training acc: 0.72000, meta-val loss: 0.08828, meta-val acc: 0.88000\n",
      "[epo 8/50, epi 50/100] => meta-training loss: 0.10270, meta-training acc: 0.64000, meta-val loss: 0.12091, meta-val acc: 0.48000\n",
      "[epo 8/50, epi 100/100] => meta-training loss: 0.15233, meta-training acc: 0.48000, meta-val loss: 0.10776, meta-val acc: 0.64000\n",
      "[epo 9/50, epi 50/100] => meta-training loss: 0.12984, meta-training acc: 0.48000, meta-val loss: 0.10914, meta-val acc: 0.52000\n",
      "[epo 9/50, epi 100/100] => meta-training loss: 0.14910, meta-training acc: 0.48000, meta-val loss: 0.10902, meta-val acc: 0.72000\n",
      "[epo 10/50, epi 50/100] => meta-training loss: 0.13943, meta-training acc: 0.40000, meta-val loss: 0.16861, meta-val acc: 0.28000\n",
      "[epo 10/50, epi 100/100] => meta-training loss: 0.14380, meta-training acc: 0.48000, meta-val loss: 0.20774, meta-val acc: 0.20000\n",
      "[epo 11/50, epi 50/100] => meta-training loss: 0.10290, meta-training acc: 0.72000, meta-val loss: 0.11207, meta-val acc: 0.60000\n",
      "[epo 11/50, epi 100/100] => meta-training loss: 0.10266, meta-training acc: 0.64000, meta-val loss: 0.09972, meta-val acc: 0.68000\n",
      "[epo 12/50, epi 50/100] => meta-training loss: 0.10213, meta-training acc: 0.68000, meta-val loss: 0.11051, meta-val acc: 0.68000\n",
      "[epo 12/50, epi 100/100] => meta-training loss: 0.11874, meta-training acc: 0.56000, meta-val loss: 0.08664, meta-val acc: 0.92000\n",
      "[epo 13/50, epi 50/100] => meta-training loss: 0.13051, meta-training acc: 0.48000, meta-val loss: 0.14287, meta-val acc: 0.52000\n",
      "[epo 13/50, epi 100/100] => meta-training loss: 0.15933, meta-training acc: 0.36000, meta-val loss: 0.12258, meta-val acc: 0.56000\n",
      "[epo 14/50, epi 50/100] => meta-training loss: 0.14365, meta-training acc: 0.36000, meta-val loss: 0.15225, meta-val acc: 0.40000\n",
      "[epo 14/50, epi 100/100] => meta-training loss: 0.12237, meta-training acc: 0.44000, meta-val loss: 0.14331, meta-val acc: 0.40000\n",
      "[epo 15/50, epi 50/100] => meta-training loss: 0.13138, meta-training acc: 0.68000, meta-val loss: 0.11005, meta-val acc: 0.52000\n",
      "[epo 15/50, epi 100/100] => meta-training loss: 0.10710, meta-training acc: 0.64000, meta-val loss: 0.19736, meta-val acc: 0.32000\n",
      "[epo 16/50, epi 50/100] => meta-training loss: 0.11137, meta-training acc: 0.56000, meta-val loss: 0.10215, meta-val acc: 0.76000\n",
      "[epo 16/50, epi 100/100] => meta-training loss: 0.10965, meta-training acc: 0.68000, meta-val loss: 0.08212, meta-val acc: 0.80000\n",
      "[epo 17/50, epi 50/100] => meta-training loss: 0.13853, meta-training acc: 0.32000, meta-val loss: 0.13264, meta-val acc: 0.40000\n",
      "[epo 17/50, epi 100/100] => meta-training loss: 0.09499, meta-training acc: 0.80000, meta-val loss: 0.13641, meta-val acc: 0.52000\n",
      "[epo 18/50, epi 50/100] => meta-training loss: 0.12505, meta-training acc: 0.48000, meta-val loss: 0.09645, meta-val acc: 0.76000\n",
      "[epo 18/50, epi 100/100] => meta-training loss: 0.10096, meta-training acc: 0.68000, meta-val loss: 0.10404, meta-val acc: 0.76000\n",
      "[epo 19/50, epi 50/100] => meta-training loss: 0.13349, meta-training acc: 0.44000, meta-val loss: 0.11654, meta-val acc: 0.68000\n",
      "[epo 19/50, epi 100/100] => meta-training loss: 0.13761, meta-training acc: 0.40000, meta-val loss: 0.12861, meta-val acc: 0.52000\n",
      "[epo 20/50, epi 50/100] => meta-training loss: 0.11298, meta-training acc: 0.68000, meta-val loss: 0.11620, meta-val acc: 0.60000\n",
      "[epo 20/50, epi 100/100] => meta-training loss: 0.12692, meta-training acc: 0.56000, meta-val loss: 0.11802, meta-val acc: 0.68000\n",
      "[epo 21/50, epi 50/100] => meta-training loss: 0.13167, meta-training acc: 0.52000, meta-val loss: 0.13949, meta-val acc: 0.68000\n",
      "[epo 21/50, epi 100/100] => meta-training loss: 0.10602, meta-training acc: 0.64000, meta-val loss: 0.12137, meta-val acc: 0.68000\n",
      "[epo 22/50, epi 50/100] => meta-training loss: 0.13895, meta-training acc: 0.48000, meta-val loss: 0.11500, meta-val acc: 0.56000\n",
      "[epo 22/50, epi 100/100] => meta-training loss: 0.11279, meta-training acc: 0.72000, meta-val loss: 0.17357, meta-val acc: 0.44000\n",
      "[epo 23/50, epi 50/100] => meta-training loss: 0.07781, meta-training acc: 0.84000, meta-val loss: 0.10059, meta-val acc: 0.72000\n",
      "[epo 23/50, epi 100/100] => meta-training loss: 0.09780, meta-training acc: 0.64000, meta-val loss: 0.17340, meta-val acc: 0.36000\n",
      "[epo 24/50, epi 50/100] => meta-training loss: 0.09946, meta-training acc: 0.64000, meta-val loss: 0.07642, meta-val acc: 0.88000\n",
      "[epo 24/50, epi 100/100] => meta-training loss: 0.09092, meta-training acc: 0.76000, meta-val loss: 0.16504, meta-val acc: 0.28000\n",
      "[epo 25/50, epi 50/100] => meta-training loss: 0.10962, meta-training acc: 0.64000, meta-val loss: 0.14328, meta-val acc: 0.44000\n",
      "[epo 25/50, epi 100/100] => meta-training loss: 0.10433, meta-training acc: 0.64000, meta-val loss: 0.15033, meta-val acc: 0.32000\n",
      "[epo 26/50, epi 50/100] => meta-training loss: 0.11172, meta-training acc: 0.76000, meta-val loss: 0.15604, meta-val acc: 0.40000\n",
      "[epo 26/50, epi 100/100] => meta-training loss: 0.13406, meta-training acc: 0.44000, meta-val loss: 0.12538, meta-val acc: 0.44000\n",
      "[epo 27/50, epi 50/100] => meta-training loss: 0.09181, meta-training acc: 0.76000, meta-val loss: 0.08529, meta-val acc: 0.96000\n",
      "[epo 27/50, epi 100/100] => meta-training loss: 0.14641, meta-training acc: 0.44000, meta-val loss: 0.10950, meta-val acc: 0.64000\n",
      "[epo 28/50, epi 50/100] => meta-training loss: 0.15662, meta-training acc: 0.48000, meta-val loss: 0.10395, meta-val acc: 0.68000\n",
      "[epo 28/50, epi 100/100] => meta-training loss: 0.05634, meta-training acc: 0.88000, meta-val loss: 0.11931, meta-val acc: 0.64000\n",
      "[epo 29/50, epi 50/100] => meta-training loss: 0.12156, meta-training acc: 0.56000, meta-val loss: 0.10590, meta-val acc: 0.72000\n",
      "[epo 29/50, epi 100/100] => meta-training loss: 0.14362, meta-training acc: 0.32000, meta-val loss: 0.11556, meta-val acc: 0.64000\n",
      "[epo 30/50, epi 50/100] => meta-training loss: 0.07954, meta-training acc: 0.80000, meta-val loss: 0.14414, meta-val acc: 0.44000\n",
      "[epo 30/50, epi 100/100] => meta-training loss: 0.09446, meta-training acc: 0.80000, meta-val loss: 0.09043, meta-val acc: 0.84000\n",
      "[epo 31/50, epi 50/100] => meta-training loss: 0.11166, meta-training acc: 0.72000, meta-val loss: 0.09921, meta-val acc: 0.64000\n",
      "[epo 31/50, epi 100/100] => meta-training loss: 0.10235, meta-training acc: 0.68000, meta-val loss: 0.07530, meta-val acc: 0.84000\n",
      "[epo 32/50, epi 50/100] => meta-training loss: 0.10353, meta-training acc: 0.64000, meta-val loss: 0.10897, meta-val acc: 0.60000\n",
      "[epo 32/50, epi 100/100] => meta-training loss: 0.09326, meta-training acc: 0.60000, meta-val loss: 0.10012, meta-val acc: 0.72000\n",
      "[epo 33/50, epi 50/100] => meta-training loss: 0.14729, meta-training acc: 0.48000, meta-val loss: 0.11320, meta-val acc: 0.52000\n",
      "[epo 33/50, epi 100/100] => meta-training loss: 0.09102, meta-training acc: 0.48000, meta-val loss: 0.10795, meta-val acc: 0.68000\n",
      "[epo 34/50, epi 50/100] => meta-training loss: 0.12067, meta-training acc: 0.36000, meta-val loss: 0.08251, meta-val acc: 0.76000\n",
      "[epo 34/50, epi 100/100] => meta-training loss: 0.08148, meta-training acc: 0.80000, meta-val loss: 0.09695, meta-val acc: 0.56000\n",
      "[epo 35/50, epi 50/100] => meta-training loss: 0.09125, meta-training acc: 0.72000, meta-val loss: 0.10658, meta-val acc: 0.72000\n",
      "[epo 35/50, epi 100/100] => meta-training loss: 0.14486, meta-training acc: 0.44000, meta-val loss: 0.12818, meta-val acc: 0.68000\n",
      "[epo 36/50, epi 50/100] => meta-training loss: 0.13909, meta-training acc: 0.56000, meta-val loss: 0.08582, meta-val acc: 0.88000\n",
      "[epo 36/50, epi 100/100] => meta-training loss: 0.12132, meta-training acc: 0.52000, meta-val loss: 0.10607, meta-val acc: 0.76000\n",
      "[epo 37/50, epi 50/100] => meta-training loss: 0.07003, meta-training acc: 0.92000, meta-val loss: 0.12149, meta-val acc: 0.64000\n",
      "[epo 37/50, epi 100/100] => meta-training loss: 0.10460, meta-training acc: 0.68000, meta-val loss: 0.12468, meta-val acc: 0.60000\n",
      "[epo 38/50, epi 50/100] => meta-training loss: 0.07824, meta-training acc: 0.84000, meta-val loss: 0.12417, meta-val acc: 0.56000\n",
      "[epo 38/50, epi 100/100] => meta-training loss: 0.08485, meta-training acc: 0.76000, meta-val loss: 0.09965, meta-val acc: 0.60000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7430\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Reshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7431\u001b[0;31m         tld.op_callbacks, tensor, shape)\n\u001b[0m\u001b[1;32m   7432\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7a27ad1e3125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n========================Performing shear={p}=========================\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n\u001b[0;32m----> 5\u001b[0;31m                  k_meta_test_shot=1, n_meta_test_query=5,shear=p,scale=None,rel=True,n_epochs=50,latent_dim = 32)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b89a02ed1343>\u001b[0m in \u001b[0;36mrun_protonet\u001b[0;34m(data_path, n_way, k_shot, n_query, n_meta_test_way, k_meta_test_shot, n_meta_test_query, shear, scale, rel, n_epochs, latent_dim)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;31m#############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_net_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_ph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta_val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-b89a02ed1343>\u001b[0m in \u001b[0;36mproto_net_train_step\u001b[0;34m(model, optim, x, q, labels_ph, rel)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_TileGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    737\u001b[0m   \u001b[0;31m#   axes = [0, 2, 4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m   split_shape = array_ops.reshape(\n\u001b[0;32m--> 739\u001b[0;31m       array_ops.transpose(array_ops.stack([op.inputs[1], input_shape])), [-1])\n\u001b[0m\u001b[1;32m    740\u001b[0m   \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;31m# Sum reduces grad along the first dimension for IndexedSlices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   7434\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7435\u001b[0m         return reshape_eager_fallback(\n\u001b[0;32m-> 7436\u001b[0;31m             tensor, shape, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   7437\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7438\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape_eager_fallback\u001b[0;34m(tensor, shape, name, ctx)\u001b[0m\n\u001b[1;32m   7461\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tshape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7462\u001b[0m   _result = _execute.execute(b\"Reshape\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 7463\u001b[0;31m                              ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m   7464\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7465\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in range(-8,10,2):\n",
    "    p = p/10\n",
    "    print(f'\\n========================Performing shear={p}=========================\\n')\n",
    "    run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,shear=p,scale=None,rel=True,n_epochs=40,latent_dim = 32)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
