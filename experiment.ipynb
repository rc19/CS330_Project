{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Utility functions. \"\"\"\n",
    "import pdb\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "## Loss utilities\n",
    "def cross_entropy_loss(pred, label, k_shot):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data loading scripts\"\"\"\n",
    "from scipy import misc\n",
    "import imageio\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from PIL import Image\n",
    "\n",
    "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    Args:\n",
    "    paths: A list of character folders\n",
    "    labels: List or numpy array of same length as paths\n",
    "    n_samples: Number of images to retrieve per character\n",
    "    Returns:\n",
    "    List of (label, image_path) tuples\n",
    "    \"\"\"\n",
    "    if n_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, n_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "           for i, path in zip(labels, paths)\n",
    "           for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels\n",
    "\n",
    "\n",
    "def image_file_to_array(filename, dim_input, shear=None, scale=None):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns numpy array\n",
    "    Args:\n",
    "    filename: Image filename\n",
    "    dim_input: Flattened shape of image\n",
    "    Returns:\n",
    "    1 channel image\n",
    "    \"\"\"\n",
    "    image = imageio.imread(filename)\n",
    "    if shear is not None:\n",
    "        afine_tf = transform.AffineTransform(shear=shear)\n",
    "        image = transform.warp(image, inverse_map=afine_tf.inverse)\n",
    "\n",
    "    elif scale is not None:\n",
    "        afine_tf = transform.AffineTransform(scale=scale)\n",
    "        image = transform.warp(image, inverse_map=afine_tf.inverse)\n",
    "#     pdb.set_trace()\n",
    "    image = transform.resize(image,(28,28),anti_aliasing=False,order=0)*255\n",
    "#     pdb.set_trace()\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 1.0 - image\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_classes: Number of classes for classification (K-way)\n",
    "          num_samples_per_class: num samples to generate per class in one batch\n",
    "          num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
    "          num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
    "          batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "        self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
    "        self.num_meta_test_classes = num_meta_test_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './omniglot')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                   for family in os.listdir(data_folder)\n",
    "                   if os.path.isdir(os.path.join(data_folder, family))\n",
    "                   for character in os.listdir(os.path.join(data_folder, family))\n",
    "                   if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "\n",
    "        random.seed(42)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[\n",
    "          num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[\n",
    "          num_train + num_val:]\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False, shear=None, scale=None):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "          batch_type: meta_train/meta_val/meta_test\n",
    "          shuffle: randomly shuffle classes or not\n",
    "          swap: swap number of classes (N) and number of samples per class (K) or not\n",
    "        Returns:\n",
    "          A a tuple of (1) Image batch and (2) Label batch where\n",
    "          image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
    "          where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"meta_train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        elif batch_type == \"meta_val\":\n",
    "            folders = self.metaval_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "            num_classes = self.num_meta_test_classes\n",
    "            num_samples_per_class = self.num_meta_test_samples_per_class\n",
    "\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        for i in range(batch_size):\n",
    "            sampled_character_folders = random.sample(\n",
    "            folders, num_classes)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(\n",
    "            num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
    "            labels = [li[0] for li in labels_and_images]\n",
    "            images = [image_file_to_array(\n",
    "            li[1], self.dim_input,shear,scale) for li in labels_and_images]\n",
    "            images = np.stack(images)\n",
    "            labels = np.array(labels).astype(np.int32)\n",
    "            labels = np.reshape(\n",
    "            labels, (num_classes, num_samples_per_class))\n",
    "            labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
    "            images = np.reshape(\n",
    "            images, (num_classes, num_samples_per_class, -1))\n",
    "\n",
    "            batch = np.concatenate([labels, images], 2)\n",
    "            if shuffle:\n",
    "                for p in range(num_samples_per_class):\n",
    "                    np.random.shuffle(batch[:, p])\n",
    "\n",
    "            labels = batch[:, :, :num_classes]\n",
    "            images = batch[:, :, num_classes:]\n",
    "\n",
    "            if swap:\n",
    "                labels = np.swapaxes(labels, 0, 1)\n",
    "                images = np.swapaxes(images, 0, 1)\n",
    "\n",
    "            all_image_batches.append(images)\n",
    "            all_label_batches.append(labels)\n",
    "        all_image_batches = np.stack(all_image_batches)\n",
    "        all_label_batches = np.stack(all_label_batches)\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-33-03f8ab04e194>\u001b[0m(49)\u001b[0;36mimage_file_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m        \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mafine_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 49 \u001b[0;31m    \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manti_aliasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m    \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-33-03f8ab04e194>\u001b[0m(51)\u001b[0;36mimage_file_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     49 \u001b[0;31m    \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manti_aliasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 51 \u001b[0;31m    \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m    \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m    \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHRCAYAAAASbQJzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAANbUlEQVR4nO3dbYxtZ13G4fsPxSB9QywQqkiDtonW0Eo0kYiKClaJBAySaLEk+kFRiSYtNU0j8hKVxheiIfitEKVEEVuMQkkBUSwRjaRSSVGbim1qKwgIfROxwuOHvU8dxzntaU/pnOl9XcnOmVl77dnPWiczv3nW7Jln1loBgEaP2O8BAMB+EUEAaokgALVEEIBaIghALREEoJYI8rAwM6+amcv2exwHmXNIIxHkwJiZc2fmQzNz58z868y8a2aeuU9jOW1m/mxm/mNm/mFmnn0f+6+ZuWs79kO3n3+oxvulNjOPm5m3b4/xppk5d7/HBEfiuP0eAByJmTk/yUVJXprkqiT/leT7kjw/yQf2YUi/l+SDSZ67vf3hzJy+1vrkvTzmrLXWDQ/J6B56b8jm/+SJSc5O8s6ZuXatdd2+jgrug5kgx7yZOTnJa5L8zFrrirXWXWutu9daf7LWuvAwj3nbzHx8Zm6bmb+YmTN33PfcmfnozNwxM7fMzMu320+ZmXfMzGdn5t9n5uqZ+X+fIzNzRpKnJ3nlWutza63Lk3wkyQsf4PFdOTO/seP9t87MG7dvf+3MvG9mPj0zn5qZt8zMY3fse+PMXDgzf7edhV06M0/czpLvmJn3zsxXbPc9bTsj/YmZuXU7m77gXsb1rTPzl9vzce3MPOsw+x2/PfZXrLXuXGt9IMkfJznvgZwPeCiJIAfBM5I8Osnb78dj3pXk9CRPSHJNkrfsuO/SJD+51joxyTcmed92+wVJ/iXJ47OZ0VycZK+/K3hmko+tte7Yse3a7fYH4seTnDcz3z0zL07yLUl+bnvfJHltklOTfH2SJyd51a7HvzDJc5KckeR52Rz7xUlOyeZz/Gd37f9d2Zyb701y0V6Xcmfmq5K8M8kvJXlckpcnuXxmHr/H+M9I8oW11vU7th3N+YCHjAhyEHxlkk+ttf77SB+w1nrjWuuOtdbns4nGWdsZZZLcneQbZuaktdZn1lrX7Nj+pCRP2c40r157/3HdE5LctmvbbUlOvI9hXbOdVR26nbMd68ezucz7O0l+K8lLDgV2rXXDWus9a63Pby+1vi7Jd+76uK9fa31irXVLkquT/PVa62+3x/72JN+0a/9Xb2fTH0nypiQ/ssdYfzTJlWutK9daX1xrvSfJh7K59PtgnQ/YdyLIQfDpJKfMzBH9DHtmHjkzl8zMP83M7Ulu3N51yvbfF2bzxfymmXn/zDxju/3XktyQ5N0z87GZuegwT3FnkpN2bTspyR3b579ux4tfvn3HPk9faz12x+2qHfe9I8kjk/zj9nLioWN5wsz8/vay7e1JLttxHId8Ysfbn9vj/RN27X/zjrdvymaWudtTkrxoZ7STPDObbxJ2u9fzAccyEeQg+GCS/0zygiPc/9xsXjDz7CQnJzltu32SZK31N2ut52dzqfSPkvzBdvsda60L1lpPzeay4vkz8z17fPzrkjx1ZnbOdM7abs9a68y11gnb29VHOOZfTvL3SZ40MztnZq/N5pLs09ZaJ2UzQ5sj/JiH8+Qdb39Nklv32OfmJG/eFe3j11qX7LHv9UmOm5nTd2y753zAsUwEOeattW5L8otJ3jAzL5iZx8zMo2bm+2fmV/d4yIlJPp/NDPIxSX7l0B0z82Uz8+KZOXmtdXeS25N8YXvfD8zM183M7Nj+hT3Gc32SDyd55cw8emZ+MMnTklz+QI5vZr4jyY8lecn29vrtz+QOHcudST673bbnC4Hup1dsz+GZ2+d96x77XJbkeTNzznZm/eiZedbMfPXuHddadyW5IslrZub4mfm2bL4JefODMFb4khJBDoS11uuSnJ/kF5J8MpuZysuymcnt9rvZXOa7JclHk/zVrvvPS3Lj9vLiS7OZXSWbF4u8N5vofDDJb6+1/vwwQ/rhJN+c5DNJLknyQ/fx6xFJcu38398T/M2ZOWk73pettW7ZXgq9NMmbtjF+dTavRL0tmxeqXHEfz3Ek3p/NZd8/TfLra613795hrXVzNiG7OP97vi/M4b9m/HSSL0/yb9n8+shP+fUIDoKxqC50mJnTkvxzkkfdnxcZwcOZmSAAtUQQgFouhwJQy0wQgFoiCECte/0LHM95xItcKwXgQHvPF9922D8wYSYIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALXudT1BuD+uuvXD+/bc55x69r49N3BwmQkCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWpZS4h77uRTS0TqasVuGCXqZCQJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWsft9wA4dpxz6tn7PQSAh5SZIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCECt4/Z7AMDBdNWtH9635z7n1LP37bl5eDETBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBa1hPkQbOf68vtp/1c2671nMODxUwQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1LKUEvewLA/QxkwQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGpZT5B7nHPq2Uf1eOsRAgeNmSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1LKeIBxg1oCEo2MmCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGpZSokHzdEs62NJn/1xtEsxwUFnJghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtawnyDHBunbAfjATBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUEsEAaglggDUEkEAaokgALVEEIBaIghALREEoJYIAlBLBAGoJYIA1BJBAGqJIAC1RBCAWiIIQC0RBKCWCAJQSwQBqCWCANQSQQBqiSAAtUQQgFoiCEAtEQSglggCUGvWWvs9BgDYF2aCANQSQQBqiSAAtUQQgFoiCEAtEQSg1v8AnL0RUGzjqFYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### Debug \n",
    "# data_generator = DataGenerator(1, 1,1,1,config={'data_folder':'omniglot_resized'})\n",
    "# i, l = data_generator.sample_batch('train', 1, shear=0)\n",
    "# b = 0\n",
    "# from matplotlib import pyplot as plt\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# cnt = 0\n",
    "# for c in range(1):\n",
    "#     for k in range(1):\n",
    "#         plt.subplot(1, 1, cnt + 1)\n",
    "#         plt.title(f'Class {c}-Example {k}')\n",
    "#         image = i[b, c, k].reshape((28,28))\n",
    "#         plt.imshow(image)\n",
    "#         plt.axis('off');\n",
    "#         cnt += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RelationNet\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "class RelationNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_filters = 64, hidden_dim = 8):\n",
    "        super(RelationNet, self).__init__()\n",
    "#         self.num_filters = num_filters\n",
    "#         # self.latent_dim = latent_dim\n",
    "#         num_filter_list = [self.num_filters]*2\n",
    "#         self.convs = []\n",
    "#         for i, num_filter in enumerate(num_filter_list):\n",
    "#             block_parts = [\n",
    "#             layers.Conv2D(\n",
    "#               filters=num_filter,\n",
    "#               kernel_size=3,\n",
    "#               padding='SAME',\n",
    "#               activation='linear'),\n",
    "#             ]\n",
    "\n",
    "#             block_parts += [layers.BatchNormalization(momentum = 1)]\n",
    "#             block_parts += [layers.Activation('relu')]\n",
    "#             block_parts += [layers.MaxPool2D()]\n",
    "#             block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
    "#             self.__setattr__(\"conv%d\" % i, block)\n",
    "#             self.convs.append(block)\n",
    "#         self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = layers.Dense(hidden_dim,activation = 'relu')\n",
    "        self.fc2 = layers.Dense(1,activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inp):\n",
    "        out = inp\n",
    "        # for conv in self.convs:\n",
    "          # out = conv(out)\n",
    "        # out = self.flatten(out)\n",
    "        out = self.fc2(self.fc1(out))\n",
    "        return out\n",
    "\n",
    "def RelationLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries, relation_net):\n",
    "    \"\"\"\n",
    "    calculates the prototype network loss using the latent representation of x\n",
    "    and the latent representation of the query set\n",
    "    Args:\n",
    "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "      num_classes: number of classes (N) for classification\n",
    "      num_support: number of examples (S) in the support set\n",
    "      num_queries: number of examples (Q) in the query set\n",
    "    Returns:\n",
    "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "      acc: the accuracy of classification on the queries\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x_latent, shape=[num_classes, num_support, -1])\n",
    "    # print(x.numpy().shape)\n",
    "    prototypes = tf.reduce_mean(x, axis = 1, keepdims=False) #(N, D)\n",
    "    prototypes_ext = tf.repeat(prototypes, repeats = [num_classes*num_queries]*num_classes, axis = 0) #(N*Q*N, D)\n",
    "    q_ext = tf.tile(q_latent, multiples = [num_classes,1]) # (N*N*Q, D)\n",
    "    labels_ = tf.reshape(labels_onehot, [-1, num_classes]) #(N*Q, N)\n",
    "    scores_ = relation_net(tf.concat([q_ext, prototypes_ext], axis = -1)) #(N*NQ, 1)\n",
    "    scores_ = tf.split(scores_,num_classes,0)\n",
    "    scores = tf.concat(scores_, axis = -1) #(NQ,N)\n",
    "\n",
    "    loss = losses.mean_squared_error(labels_, scores) #(NQ,)\n",
    "    loss = tf.reduce_mean(loss)#+tf.reduce_sum(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)) #(1,)\n",
    "\n",
    "    # ce_loss = cross_entropy_loss(dist,tf.reshape(labels_onehot, [-1, num_classes]), k_shot=1)\n",
    "    acc = accuracy(tf.argmax(input=labels_, axis=1), tf.argmax(input=scores, axis=1))\n",
    "    # print(ce_loss.numpy())\n",
    "    # print(acc.numpy())\n",
    "\n",
    "    # print(error)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "#Shape checking\n",
    "\n",
    "# q = tf.random.uniform(shape = [10,5])\n",
    "# print(q.shape)\n",
    "# q_ext = tf.tile(q, multiples = [2,1])\n",
    "# q_rep = tf.repeat(q, repeats = [2], axis = 0)\n",
    "# print(q_rep.shape)\n",
    "# print(q)\n",
    "# print(q_rep)\n",
    "# print(tf.concat([q,q], axis = -1))\n",
    "\n",
    "\n",
    "# t = tf.random.uniform(shape = [12,1])\n",
    "# t_ = tf.split(t,3,0)\n",
    "# t__ = tf.concat(t_, axis = -1)\n",
    "\n",
    "# t1 = tf.random.uniform(shape = [12,1])\n",
    "# t1_ = tf.split(t1,3,0)\n",
    "# t1__ = tf.concat(t1_, axis = -1)\n",
    "\n",
    "# print(losses.mean_squared_error(t__,t1__,))\n",
    "\n",
    "# print(t)\n",
    "# print(t_)\n",
    "# print(t__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/ProtoNet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ProtoNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_filters, latent_dim):\n",
    "    super(ProtoNet, self).__init__()\n",
    "    self.num_filters = num_filters\n",
    "    self.latent_dim = latent_dim\n",
    "    num_filter_list = self.num_filters + [latent_dim]\n",
    "    self.convs = []\n",
    "    for i, num_filter in enumerate(num_filter_list):\n",
    "      block_parts = [\n",
    "        layers.Conv2D(\n",
    "          filters=num_filter,\n",
    "          kernel_size=3,\n",
    "          padding='SAME',\n",
    "          activation='linear'),\n",
    "      ]\n",
    "\n",
    "      block_parts += [layers.BatchNormalization()]\n",
    "      block_parts += [layers.Activation('relu')]\n",
    "      block_parts += [layers.MaxPool2D()]\n",
    "      block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
    "      self.__setattr__(\"conv%d\" % i, block)\n",
    "      self.convs.append(block)\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "  def call(self, inp):\n",
    "    out = inp\n",
    "    for conv in self.convs:\n",
    "      out = conv(out)\n",
    "    out = self.flatten(out)\n",
    "    return out\n",
    "\n",
    "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
    "  \"\"\"\n",
    "    calculates the prototype network loss using the latent representation of x\n",
    "    and the latent representation of the query set\n",
    "    Args:\n",
    "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "      num_classes: number of classes (N) for classification\n",
    "      num_support: number of examples (S) in the support set\n",
    "      num_queries: number of examples (Q) in the query set\n",
    "    Returns:\n",
    "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "      acc: the accuracy of classification on the queries\n",
    "  \"\"\"\n",
    "  x_latent = tf.reshape(x_latent,[num_classes,num_support,-1])\n",
    "  c = tf.reduce_mean(x_latent,axis=1)\n",
    "  def dist(q, c):\n",
    "    N_Q, D = q.shape\n",
    "    N = c.shape[0]\n",
    "    q = tf.tile(tf.expand_dims(q, axis=1), [1, N, 1])\n",
    "    c = tf.tile(tf.expand_dims(c, axis=0), [N_Q, 1, 1])\n",
    "    return tf.reduce_mean(tf.square(q-c), axis=2)\n",
    "  distances = tf.reshape(dist(q_latent, c), [num_classes,num_queries,num_classes])\n",
    "  ce_loss = cross_entropy_loss(-distances, labels_onehot, 1)\n",
    "  acc = accuracy(tf.argmax(labels_onehot, axis=-1),tf.argmax(tf.reshape(tf.nn.softmax(-distances), \n",
    "                                                                 [num_classes, num_queries, -1]), axis=-1))\n",
    "  #############################\n",
    "  return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ProtoNet\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def proto_net_train_step(model, optim, x, q, labels_ph, rel=None):\n",
    "    num_classes, num_support, im_height, im_width, channels = x.shape\n",
    "    num_queries = q.shape[1]\n",
    "    x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
    "    q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_latent = model(x)\n",
    "        q_latent = model(q)\n",
    "        if rel is None:\n",
    "            ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
    "        else:\n",
    "            ce_loss, acc = RelationLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries,rel)\n",
    "\n",
    "    if rel is None:\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    else:\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables+rel.trainable_variables)\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables+rel.trainable_variables))\n",
    "    \n",
    "    return ce_loss, acc\n",
    "\n",
    "def proto_net_eval(model, x, q, labels_ph,rel=None):\n",
    "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
    "  num_queries = q.shape[1]\n",
    "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
    "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
    "\n",
    "  x_latent = model(x)\n",
    "  q_latent = model(q)\n",
    "  if rel is None:\n",
    "    ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
    "  else:\n",
    "    ce_loss, acc = RelationLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries,rel)\n",
    "\n",
    "  return ce_loss, acc \n",
    "\n",
    "def run_protonet(data_path='./omniglot', n_way=20, k_shot=1, n_query=5, n_meta_test_way=20, \n",
    "                 k_meta_test_shot=5, n_meta_test_query=5,shear=None,scale=None, rel=False, n_epochs=20, latent_dim = 16):\n",
    "  \n",
    "  n_episodes = 100\n",
    "\n",
    "  im_width, im_height, channels = 28, 28, 1\n",
    "  num_filters = 32\n",
    "  num_conv_layers = 3\n",
    "  n_meta_test_episodes = 1000\n",
    "\n",
    "  model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
    "  if not rel:\n",
    "    rel_model=None\n",
    "  else:\n",
    "    rel_model = RelationNet()\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # call DataGenerator with k_shot+n_query samples per class\n",
    "  data_generator = DataGenerator(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query)\n",
    "  for ep in range(n_epochs):\n",
    "    for epi in range(n_episodes):\n",
    "      image_batch, label_batch = data_generator.sample_batch('meta_train',1,shuffle=False)\n",
    "      image, label = image_batch[0], label_batch[0]\n",
    "      support,query = image[:,:k_shot,:], image[:,k_shot:,:]\n",
    "      labels = label[:,k_shot:,:]\n",
    "      # Reshape\n",
    "      support = tf.reshape(support,[-1,k_shot,im_width, im_height, channels])\n",
    "      query = tf.reshape(query,[-1,n_query,im_width, im_height, channels])\n",
    "      #############################\n",
    "      ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels, rel=rel_model)\n",
    "      if (epi+1) % 50 == 0:\n",
    "        image_batch, label_batch = data_generator.sample_batch('meta_val',1,shuffle=False)\n",
    "        image, label = image_batch[0], label_batch[0]\n",
    "        support,query = image[:,:k_shot,:], image[:,k_shot:,:]\n",
    "        labels = label[:,k_shot:,:]\n",
    "        # Reshape\n",
    "        support = tf.reshape(support,[-1,k_shot,im_width, im_height, channels])\n",
    "        query = tf.reshape(query,[-1,n_query,im_width, im_height, channels])\n",
    "        #############################\n",
    "        val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels, rel=rel_model)\n",
    "        print('[epo {}/{}, epi {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
    "                                                                    n_epochs,\n",
    "                                                                    epi+1,\n",
    "                                                                    n_episodes,\n",
    "                                                                    ls,\n",
    "                                                                    ac,\n",
    "                                                                    val_ls,\n",
    "                                                                    val_ac))\n",
    "\n",
    "  print('Testing...')\n",
    "  meta_test_accuracies = []\n",
    "  for epi in range(n_meta_test_episodes):\n",
    "    # sample a batch of test data and partition into\n",
    "    # support and query sets\n",
    "    image_batch, label_batch = data_generator.sample_batch('meta_test',1,shuffle=False,shear=shear,scale=scale)\n",
    "    image, label = image_batch[0], label_batch[0]\n",
    "    support,query = image[:,:k_meta_test_shot,:], image[:,k_meta_test_shot:,:]\n",
    "    labels = label[:,k_meta_test_shot:,:]\n",
    "    # Reshape\n",
    "    support = tf.reshape(support,[-1,k_meta_test_shot,im_width, im_height, channels])\n",
    "    query = tf.reshape(query,[-1,n_meta_test_query,im_width, im_height, channels])\n",
    "    #############################\n",
    "    ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels,rel=rel_model)\n",
    "    meta_test_accuracies.append(ac)\n",
    "    if (epi+1) % 50 == 0:\n",
    "      print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
    "  avg_acc = np.mean(meta_test_accuracies)\n",
    "  stds = np.std(meta_test_accuracies)\n",
    "  print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))\n",
    "  # return meta_test_accuracies, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Performing shear=-0.8=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.11286, meta-training acc: 0.52000, meta-val loss: 1.04721, meta-val acc: 0.72000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.55008, meta-training acc: 0.80000, meta-val loss: 0.96159, meta-val acc: 0.60000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.57469, meta-training acc: 0.64000, meta-val loss: 0.43887, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.14046, meta-training acc: 0.48000, meta-val loss: 0.32656, meta-val acc: 0.92000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.40277, meta-training acc: 0.84000, meta-val loss: 0.63474, meta-val acc: 0.68000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.31668, meta-training acc: 0.60000, meta-val loss: 1.05875, meta-val acc: 0.64000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.45109, meta-training acc: 0.76000, meta-val loss: 1.06805, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.35643, meta-training acc: 0.92000, meta-val loss: 1.34486, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.66795, meta-training acc: 0.72000, meta-val loss: 0.54062, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.39953, meta-training acc: 0.88000, meta-val loss: 0.66890, meta-val acc: 0.64000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.71427, meta-training acc: 0.80000, meta-val loss: 0.44437, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.37677, meta-training acc: 0.92000, meta-val loss: 0.76530, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.30600, meta-training acc: 0.88000, meta-val loss: 0.89292, meta-val acc: 0.60000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.65071, meta-training acc: 0.84000, meta-val loss: 0.17466, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.43702, meta-training acc: 0.88000, meta-val loss: 0.32132, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.91698, meta-training acc: 0.60000, meta-val loss: 0.90846, meta-val acc: 0.60000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.67857, meta-training acc: 0.84000, meta-val loss: 0.36720, meta-val acc: 0.80000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.35238, meta-training acc: 0.84000, meta-val loss: 0.31408, meta-val acc: 0.88000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.81417, meta-training acc: 0.68000, meta-val loss: 0.94342, meta-val acc: 0.68000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.57124, meta-training acc: 0.60000, meta-val loss: 1.20247, meta-val acc: 0.56000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.54211, meta-training acc: 0.92000, meta-val loss: 0.21715, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.18298, meta-training acc: 0.96000, meta-val loss: 0.40398, meta-val acc: 0.84000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.38386, meta-training acc: 0.76000, meta-val loss: 0.39081, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.29159, meta-training acc: 0.84000, meta-val loss: 0.05162, meta-val acc: 0.96000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.58237, meta-training acc: 0.88000, meta-val loss: 0.70761, meta-val acc: 0.80000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.64864, meta-training acc: 0.84000, meta-val loss: 0.77664, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 1.50533, meta-training acc: 0.80000, meta-val loss: 0.77954, meta-val acc: 0.68000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.44692, meta-training acc: 0.84000, meta-val loss: 0.17891, meta-val acc: 0.92000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.23369, meta-training acc: 0.96000, meta-val loss: 0.97187, meta-val acc: 0.68000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.65632, meta-training acc: 0.68000, meta-val loss: 0.63599, meta-val acc: 0.76000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.61076, meta-training acc: 0.68000, meta-val loss: 0.39613, meta-val acc: 0.84000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12211, meta-training acc: 1.00000, meta-val loss: 0.23328, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.17744, meta-training acc: 0.88000, meta-val loss: 0.21359, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.15087, meta-training acc: 0.92000, meta-val loss: 0.12783, meta-val acc: 1.00000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.05731, meta-training acc: 1.00000, meta-val loss: 0.49010, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.10589, meta-training acc: 0.96000, meta-val loss: 0.43127, meta-val acc: 0.80000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.80961, meta-training acc: 0.76000, meta-val loss: 0.99608, meta-val acc: 0.56000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.47583, meta-training acc: 0.92000, meta-val loss: 0.22834, meta-val acc: 0.84000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.12115, meta-training acc: 0.92000, meta-val loss: 0.03239, meta-val acc: 1.00000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.04284, meta-training acc: 1.00000, meta-val loss: 0.08322, meta-val acc: 1.00000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.98286, acc: 0.72000\n",
      "[meta-test episode 100/1000] => loss: 1.30932, acc: 0.68000\n",
      "[meta-test episode 150/1000] => loss: 0.58332, acc: 0.88000\n",
      "[meta-test episode 200/1000] => loss: 1.30272, acc: 0.52000\n",
      "[meta-test episode 250/1000] => loss: 0.99331, acc: 0.64000\n",
      "[meta-test episode 300/1000] => loss: 1.15680, acc: 0.48000\n",
      "[meta-test episode 350/1000] => loss: 0.97384, acc: 0.56000\n",
      "[meta-test episode 400/1000] => loss: 1.10739, acc: 0.52000\n",
      "[meta-test episode 450/1000] => loss: 1.27737, acc: 0.56000\n",
      "[meta-test episode 500/1000] => loss: 0.92774, acc: 0.64000\n",
      "[meta-test episode 550/1000] => loss: 0.90398, acc: 0.68000\n",
      "[meta-test episode 600/1000] => loss: 0.65543, acc: 0.80000\n",
      "[meta-test episode 650/1000] => loss: 1.43401, acc: 0.64000\n",
      "[meta-test episode 700/1000] => loss: 1.38960, acc: 0.60000\n",
      "[meta-test episode 750/1000] => loss: 1.06071, acc: 0.64000\n",
      "[meta-test episode 800/1000] => loss: 1.10157, acc: 0.64000\n",
      "[meta-test episode 850/1000] => loss: 0.95201, acc: 0.56000\n",
      "[meta-test episode 900/1000] => loss: 1.24809, acc: 0.48000\n",
      "[meta-test episode 950/1000] => loss: 1.41594, acc: 0.48000\n",
      "[meta-test episode 1000/1000] => loss: 1.35030, acc: 0.40000\n",
      "Average Meta-Test Accuracy: 0.59724, Meta-Test Accuracy Std: 0.13952\n",
      "\n",
      "========================Performing shear=-0.6=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.20977, meta-training acc: 0.40000, meta-val loss: 1.13744, meta-val acc: 0.72000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.60897, meta-training acc: 0.84000, meta-val loss: 1.03526, meta-val acc: 0.60000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.65745, meta-training acc: 0.64000, meta-val loss: 0.65553, meta-val acc: 0.76000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.19105, meta-training acc: 0.52000, meta-val loss: 0.30799, meta-val acc: 0.96000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.54942, meta-training acc: 0.88000, meta-val loss: 0.64111, meta-val acc: 0.68000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.19136, meta-training acc: 0.56000, meta-val loss: 0.86081, meta-val acc: 0.56000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.61001, meta-training acc: 0.80000, meta-val loss: 0.95759, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.29034, meta-training acc: 0.88000, meta-val loss: 1.97615, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.69364, meta-training acc: 0.68000, meta-val loss: 0.69931, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.44958, meta-training acc: 0.88000, meta-val loss: 0.65392, meta-val acc: 0.64000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.79468, meta-training acc: 0.64000, meta-val loss: 0.45153, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.39299, meta-training acc: 0.96000, meta-val loss: 0.74152, meta-val acc: 0.68000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.12084, meta-training acc: 1.00000, meta-val loss: 0.86316, meta-val acc: 0.76000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.54821, meta-training acc: 0.88000, meta-val loss: 0.27545, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.62456, meta-training acc: 0.88000, meta-val loss: 0.55041, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.83361, meta-training acc: 0.80000, meta-val loss: 0.87168, meta-val acc: 0.68000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.31075, meta-training acc: 0.92000, meta-val loss: 0.66941, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.30448, meta-training acc: 0.92000, meta-val loss: 0.30051, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.57190, meta-training acc: 0.68000, meta-val loss: 1.03869, meta-val acc: 0.64000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.30742, meta-training acc: 0.64000, meta-val loss: 0.89467, meta-val acc: 0.60000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.19738, meta-training acc: 0.96000, meta-val loss: 0.26992, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.11518, meta-training acc: 0.96000, meta-val loss: 0.38494, meta-val acc: 0.84000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.30226, meta-training acc: 0.84000, meta-val loss: 0.25653, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.47361, meta-training acc: 0.76000, meta-val loss: 0.07315, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.71083, meta-training acc: 0.80000, meta-val loss: 0.22014, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.52814, meta-training acc: 0.80000, meta-val loss: 0.20555, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.68051, meta-training acc: 0.80000, meta-val loss: 0.58935, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.64791, meta-training acc: 0.80000, meta-val loss: 0.11167, meta-val acc: 0.96000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.12535, meta-training acc: 1.00000, meta-val loss: 1.14397, meta-val acc: 0.80000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.69448, meta-training acc: 0.76000, meta-val loss: 0.77201, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.68937, meta-training acc: 0.68000, meta-val loss: 0.37485, meta-val acc: 0.92000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12736, meta-training acc: 1.00000, meta-val loss: 0.24674, meta-val acc: 0.84000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.16454, meta-training acc: 0.92000, meta-val loss: 0.12861, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.11035, meta-training acc: 0.96000, meta-val loss: 0.49612, meta-val acc: 0.84000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.16533, meta-training acc: 0.92000, meta-val loss: 0.36891, meta-val acc: 0.84000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.06835, meta-training acc: 1.00000, meta-val loss: 0.24480, meta-val acc: 0.96000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.70887, meta-training acc: 0.76000, meta-val loss: 0.61753, meta-val acc: 0.76000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.34920, meta-training acc: 0.92000, meta-val loss: 0.19960, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.14266, meta-training acc: 1.00000, meta-val loss: 0.12539, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.02262, meta-training acc: 1.00000, meta-val loss: 0.32040, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 1.05997, acc: 0.76000\n",
      "[meta-test episode 100/1000] => loss: 0.99946, acc: 0.68000\n",
      "[meta-test episode 150/1000] => loss: 0.47904, acc: 0.88000\n",
      "[meta-test episode 200/1000] => loss: 1.05285, acc: 0.64000\n",
      "[meta-test episode 250/1000] => loss: 0.77427, acc: 0.60000\n",
      "[meta-test episode 300/1000] => loss: 1.26797, acc: 0.60000\n",
      "[meta-test episode 350/1000] => loss: 0.69293, acc: 0.76000\n",
      "[meta-test episode 400/1000] => loss: 1.45781, acc: 0.48000\n",
      "[meta-test episode 450/1000] => loss: 1.16095, acc: 0.60000\n",
      "[meta-test episode 500/1000] => loss: 0.61833, acc: 0.72000\n",
      "[meta-test episode 550/1000] => loss: 0.74052, acc: 0.80000\n",
      "[meta-test episode 600/1000] => loss: 0.44130, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 0.78208, acc: 0.76000\n",
      "[meta-test episode 700/1000] => loss: 1.00931, acc: 0.64000\n",
      "[meta-test episode 750/1000] => loss: 0.91586, acc: 0.68000\n",
      "[meta-test episode 800/1000] => loss: 1.02986, acc: 0.64000\n",
      "[meta-test episode 850/1000] => loss: 0.80603, acc: 0.64000\n",
      "[meta-test episode 900/1000] => loss: 1.35881, acc: 0.36000\n",
      "[meta-test episode 950/1000] => loss: 1.24176, acc: 0.56000\n",
      "[meta-test episode 1000/1000] => loss: 1.12968, acc: 0.56000\n",
      "Average Meta-Test Accuracy: 0.68068, Meta-Test Accuracy Std: 0.13681\n",
      "\n",
      "========================Performing shear=-0.4=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.17457, meta-training acc: 0.40000, meta-val loss: 1.18981, meta-val acc: 0.64000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.41895, meta-training acc: 0.84000, meta-val loss: 1.09624, meta-val acc: 0.52000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.51289, meta-training acc: 0.68000, meta-val loss: 0.56722, meta-val acc: 0.80000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.02121, meta-training acc: 0.56000, meta-val loss: 0.25984, meta-val acc: 0.96000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.33224, meta-training acc: 0.80000, meta-val loss: 0.55616, meta-val acc: 0.76000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.34914, meta-training acc: 0.48000, meta-val loss: 0.82510, meta-val acc: 0.64000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.49225, meta-training acc: 0.80000, meta-val loss: 1.01229, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.41204, meta-training acc: 0.92000, meta-val loss: 1.64227, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.68703, meta-training acc: 0.64000, meta-val loss: 0.44668, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.29452, meta-training acc: 0.96000, meta-val loss: 0.57270, meta-val acc: 0.80000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.59580, meta-training acc: 0.84000, meta-val loss: 0.35447, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.28494, meta-training acc: 0.96000, meta-val loss: 0.76153, meta-val acc: 0.68000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.22958, meta-training acc: 0.92000, meta-val loss: 0.77284, meta-val acc: 0.68000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.79058, meta-training acc: 0.84000, meta-val loss: 0.31634, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.61353, meta-training acc: 0.84000, meta-val loss: 0.46487, meta-val acc: 0.80000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.81517, meta-training acc: 0.72000, meta-val loss: 0.73277, meta-val acc: 0.68000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.27884, meta-training acc: 0.92000, meta-val loss: 0.45546, meta-val acc: 0.76000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.13097, meta-training acc: 0.96000, meta-val loss: 0.31255, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 1.03075, meta-training acc: 0.72000, meta-val loss: 1.43834, meta-val acc: 0.72000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.49132, meta-training acc: 0.64000, meta-val loss: 1.25680, meta-val acc: 0.52000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.72566, meta-training acc: 0.92000, meta-val loss: 0.20835, meta-val acc: 0.92000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.05839, meta-training acc: 0.96000, meta-val loss: 0.32964, meta-val acc: 0.80000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.13810, meta-training acc: 0.96000, meta-val loss: 0.18121, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.65781, meta-training acc: 0.64000, meta-val loss: 0.23971, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.62611, meta-training acc: 0.84000, meta-val loss: 0.16308, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.25711, meta-training acc: 0.88000, meta-val loss: 0.44471, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 1.03974, meta-training acc: 0.80000, meta-val loss: 0.54155, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.60895, meta-training acc: 0.88000, meta-val loss: 0.15768, meta-val acc: 0.92000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.50589, meta-training acc: 0.80000, meta-val loss: 0.71253, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.72652, meta-training acc: 0.68000, meta-val loss: 0.60062, meta-val acc: 0.72000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.65314, meta-training acc: 0.68000, meta-val loss: 0.29713, meta-val acc: 0.96000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12034, meta-training acc: 0.92000, meta-val loss: 0.15721, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.13961, meta-training acc: 0.96000, meta-val loss: 0.08672, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.35201, meta-training acc: 0.84000, meta-val loss: 0.07494, meta-val acc: 1.00000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.05886, meta-training acc: 1.00000, meta-val loss: 0.36112, meta-val acc: 0.88000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.34529, meta-training acc: 0.88000, meta-val loss: 0.18175, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 1.03621, meta-training acc: 0.64000, meta-val loss: 0.69620, meta-val acc: 0.64000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.23243, meta-training acc: 0.92000, meta-val loss: 0.24213, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.05217, meta-training acc: 1.00000, meta-val loss: 0.08165, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.01033, meta-training acc: 1.00000, meta-val loss: 0.09987, meta-val acc: 1.00000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.66833, acc: 0.76000\n",
      "[meta-test episode 100/1000] => loss: 0.62184, acc: 0.84000\n",
      "[meta-test episode 150/1000] => loss: 0.41922, acc: 0.76000\n",
      "[meta-test episode 200/1000] => loss: 0.85229, acc: 0.64000\n",
      "[meta-test episode 250/1000] => loss: 0.35096, acc: 0.92000\n",
      "[meta-test episode 300/1000] => loss: 0.83646, acc: 0.72000\n",
      "[meta-test episode 350/1000] => loss: 0.29417, acc: 0.88000\n",
      "[meta-test episode 400/1000] => loss: 0.72389, acc: 0.84000\n",
      "[meta-test episode 450/1000] => loss: 1.18984, acc: 0.64000\n",
      "[meta-test episode 500/1000] => loss: 0.56936, acc: 0.76000\n",
      "[meta-test episode 550/1000] => loss: 0.94773, acc: 0.56000\n",
      "[meta-test episode 600/1000] => loss: 0.24506, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 0.31031, acc: 0.96000\n",
      "[meta-test episode 700/1000] => loss: 0.56287, acc: 0.76000\n",
      "[meta-test episode 750/1000] => loss: 0.74145, acc: 0.72000\n",
      "[meta-test episode 800/1000] => loss: 0.98328, acc: 0.68000\n",
      "[meta-test episode 850/1000] => loss: 0.54669, acc: 0.84000\n",
      "[meta-test episode 900/1000] => loss: 0.88504, acc: 0.68000\n",
      "[meta-test episode 950/1000] => loss: 0.52728, acc: 0.80000\n",
      "[meta-test episode 1000/1000] => loss: 0.95709, acc: 0.56000\n",
      "Average Meta-Test Accuracy: 0.79812, Meta-Test Accuracy Std: 0.11307\n",
      "\n",
      "========================Performing shear=-0.2=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.19415, meta-training acc: 0.32000, meta-val loss: 0.99749, meta-val acc: 0.68000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.64641, meta-training acc: 0.80000, meta-val loss: 1.05954, meta-val acc: 0.60000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.56625, meta-training acc: 0.72000, meta-val loss: 0.53580, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.96822, meta-training acc: 0.68000, meta-val loss: 0.28929, meta-val acc: 0.96000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.22772, meta-training acc: 0.92000, meta-val loss: 0.61592, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.31175, meta-training acc: 0.60000, meta-val loss: 0.97875, meta-val acc: 0.72000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.48037, meta-training acc: 0.80000, meta-val loss: 1.25154, meta-val acc: 0.64000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.58783, meta-training acc: 0.80000, meta-val loss: 1.39656, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.77463, meta-training acc: 0.72000, meta-val loss: 0.55481, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.34444, meta-training acc: 0.92000, meta-val loss: 0.68096, meta-val acc: 0.68000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.56453, meta-training acc: 0.88000, meta-val loss: 0.68262, meta-val acc: 0.80000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.48433, meta-training acc: 0.72000, meta-val loss: 0.84512, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.17276, meta-training acc: 1.00000, meta-val loss: 0.58315, meta-val acc: 0.80000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.38036, meta-training acc: 0.92000, meta-val loss: 0.14800, meta-val acc: 1.00000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.23464, meta-training acc: 0.88000, meta-val loss: 0.23850, meta-val acc: 0.92000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.52360, meta-training acc: 0.92000, meta-val loss: 0.58735, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.11619, meta-training acc: 0.92000, meta-val loss: 0.44396, meta-val acc: 0.92000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.08801, meta-training acc: 1.00000, meta-val loss: 0.22191, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.55632, meta-training acc: 0.80000, meta-val loss: 0.60516, meta-val acc: 0.76000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.60103, meta-training acc: 0.80000, meta-val loss: 0.81388, meta-val acc: 0.76000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.24575, meta-training acc: 0.96000, meta-val loss: 0.09931, meta-val acc: 1.00000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.02454, meta-training acc: 1.00000, meta-val loss: 0.14220, meta-val acc: 1.00000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.27187, meta-training acc: 0.80000, meta-val loss: 0.09117, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.48823, meta-training acc: 0.76000, meta-val loss: 0.29905, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.61429, meta-training acc: 0.92000, meta-val loss: 0.12634, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.39073, meta-training acc: 0.80000, meta-val loss: 0.16797, meta-val acc: 0.88000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 1.49869, meta-training acc: 0.80000, meta-val loss: 0.49628, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.73086, meta-training acc: 0.80000, meta-val loss: 0.28849, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.23314, meta-training acc: 0.92000, meta-val loss: 0.89711, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.51694, meta-training acc: 0.80000, meta-val loss: 0.38383, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.47221, meta-training acc: 0.84000, meta-val loss: 0.20925, meta-val acc: 0.96000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.03962, meta-training acc: 1.00000, meta-val loss: 0.18263, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.09327, meta-training acc: 0.96000, meta-val loss: 0.26922, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.16703, meta-training acc: 0.92000, meta-val loss: 0.40819, meta-val acc: 0.88000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.08265, meta-training acc: 0.96000, meta-val loss: 0.63166, meta-val acc: 0.88000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.15654, meta-training acc: 0.92000, meta-val loss: 0.26327, meta-val acc: 0.88000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.76033, meta-training acc: 0.76000, meta-val loss: 0.84550, meta-val acc: 0.64000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.33106, meta-training acc: 0.92000, meta-val loss: 0.08130, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.16997, meta-training acc: 0.96000, meta-val loss: 0.14954, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.01024, meta-training acc: 1.00000, meta-val loss: 0.20895, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.30290, acc: 0.92000\n",
      "[meta-test episode 100/1000] => loss: 0.41251, acc: 0.92000\n",
      "[meta-test episode 150/1000] => loss: 0.21511, acc: 0.96000\n",
      "[meta-test episode 200/1000] => loss: 0.53291, acc: 0.84000\n",
      "[meta-test episode 250/1000] => loss: 0.36925, acc: 0.80000\n",
      "[meta-test episode 300/1000] => loss: 0.53995, acc: 0.76000\n",
      "[meta-test episode 350/1000] => loss: 0.28247, acc: 0.88000\n",
      "[meta-test episode 400/1000] => loss: 0.34773, acc: 0.92000\n",
      "[meta-test episode 450/1000] => loss: 0.48066, acc: 0.84000\n",
      "[meta-test episode 500/1000] => loss: 0.51771, acc: 0.76000\n",
      "[meta-test episode 550/1000] => loss: 0.41204, acc: 0.80000\n",
      "[meta-test episode 600/1000] => loss: 0.17589, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 0.25958, acc: 0.92000\n",
      "[meta-test episode 700/1000] => loss: 0.12388, acc: 0.92000\n",
      "[meta-test episode 750/1000] => loss: 0.62782, acc: 0.76000\n",
      "[meta-test episode 800/1000] => loss: 0.72730, acc: 0.80000\n",
      "[meta-test episode 850/1000] => loss: 0.31397, acc: 0.92000\n",
      "[meta-test episode 900/1000] => loss: 0.77984, acc: 0.76000\n",
      "[meta-test episode 950/1000] => loss: 0.42268, acc: 0.84000\n",
      "[meta-test episode 1000/1000] => loss: 0.50744, acc: 0.76000\n",
      "Average Meta-Test Accuracy: 0.86624, Meta-Test Accuracy Std: 0.09516\n",
      "\n",
      "========================Performing shear=0.0=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.12301, meta-training acc: 0.40000, meta-val loss: 1.18012, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.54875, meta-training acc: 0.84000, meta-val loss: 1.09167, meta-val acc: 0.60000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.59673, meta-training acc: 0.64000, meta-val loss: 0.54157, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.24233, meta-training acc: 0.56000, meta-val loss: 0.24573, meta-val acc: 0.96000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.50835, meta-training acc: 0.84000, meta-val loss: 0.63628, meta-val acc: 0.68000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.56847, meta-training acc: 0.48000, meta-val loss: 0.79129, meta-val acc: 0.76000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.41970, meta-training acc: 0.84000, meta-val loss: 1.04155, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.27943, meta-training acc: 0.96000, meta-val loss: 1.67836, meta-val acc: 0.60000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.66332, meta-training acc: 0.76000, meta-val loss: 0.44006, meta-val acc: 0.84000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.39551, meta-training acc: 0.92000, meta-val loss: 0.63173, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.59919, meta-training acc: 0.76000, meta-val loss: 0.45917, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.41895, meta-training acc: 0.88000, meta-val loss: 0.65799, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.20380, meta-training acc: 0.96000, meta-val loss: 0.94036, meta-val acc: 0.64000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.68040, meta-training acc: 0.80000, meta-val loss: 0.19379, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.35028, meta-training acc: 0.88000, meta-val loss: 0.30701, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.83830, meta-training acc: 0.84000, meta-val loss: 0.74185, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.47409, meta-training acc: 0.92000, meta-val loss: 0.46799, meta-val acc: 0.92000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.50774, meta-training acc: 0.80000, meta-val loss: 0.34610, meta-val acc: 0.88000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.53104, meta-training acc: 0.80000, meta-val loss: 0.91313, meta-val acc: 0.60000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.12978, meta-training acc: 0.60000, meta-val loss: 1.03370, meta-val acc: 0.60000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.35389, meta-training acc: 0.96000, meta-val loss: 0.29706, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.17249, meta-training acc: 1.00000, meta-val loss: 0.38739, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.23263, meta-training acc: 0.92000, meta-val loss: 0.53225, meta-val acc: 0.80000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.34236, meta-training acc: 0.92000, meta-val loss: 0.04347, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.79804, meta-training acc: 0.88000, meta-val loss: 0.70355, meta-val acc: 0.76000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.54785, meta-training acc: 0.80000, meta-val loss: 0.41147, meta-val acc: 0.88000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 1.43677, meta-training acc: 0.80000, meta-val loss: 0.62195, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.40840, meta-training acc: 0.84000, meta-val loss: 0.19041, meta-val acc: 0.92000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.27730, meta-training acc: 0.92000, meta-val loss: 0.53615, meta-val acc: 0.76000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.58257, meta-training acc: 0.68000, meta-val loss: 0.49117, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.88782, meta-training acc: 0.64000, meta-val loss: 0.43512, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.05766, meta-training acc: 1.00000, meta-val loss: 0.40133, meta-val acc: 0.80000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.29811, meta-training acc: 0.92000, meta-val loss: 0.04468, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.23060, meta-training acc: 0.88000, meta-val loss: 0.25043, meta-val acc: 0.96000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.14134, meta-training acc: 0.92000, meta-val loss: 0.47880, meta-val acc: 0.84000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.06503, meta-training acc: 1.00000, meta-val loss: 0.30819, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.87990, meta-training acc: 0.80000, meta-val loss: 0.81216, meta-val acc: 0.76000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.28097, meta-training acc: 0.96000, meta-val loss: 0.25338, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.18610, meta-training acc: 0.96000, meta-val loss: 0.17116, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.16452, meta-training acc: 0.92000, meta-val loss: 0.26221, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.39613, acc: 0.88000\n",
      "[meta-test episode 100/1000] => loss: 0.23380, acc: 0.92000\n",
      "[meta-test episode 150/1000] => loss: 0.05062, acc: 1.00000\n",
      "[meta-test episode 200/1000] => loss: 0.18383, acc: 0.96000\n",
      "[meta-test episode 250/1000] => loss: 0.06460, acc: 1.00000\n",
      "[meta-test episode 300/1000] => loss: 0.10570, acc: 0.96000\n",
      "[meta-test episode 350/1000] => loss: 0.26932, acc: 0.88000\n",
      "[meta-test episode 400/1000] => loss: 0.25363, acc: 0.88000\n",
      "[meta-test episode 450/1000] => loss: 0.37311, acc: 0.88000\n",
      "[meta-test episode 500/1000] => loss: 0.37580, acc: 0.88000\n",
      "[meta-test episode 550/1000] => loss: 0.37454, acc: 0.88000\n",
      "[meta-test episode 600/1000] => loss: 0.34006, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 0.03140, acc: 0.96000\n",
      "[meta-test episode 700/1000] => loss: 0.16957, acc: 0.92000\n",
      "[meta-test episode 750/1000] => loss: 0.34406, acc: 0.92000\n",
      "[meta-test episode 800/1000] => loss: 0.67576, acc: 0.84000\n",
      "[meta-test episode 850/1000] => loss: 0.25778, acc: 0.88000\n",
      "[meta-test episode 900/1000] => loss: 0.75525, acc: 0.68000\n",
      "[meta-test episode 950/1000] => loss: 0.20957, acc: 0.96000\n",
      "[meta-test episode 1000/1000] => loss: 0.62252, acc: 0.76000\n",
      "Average Meta-Test Accuracy: 0.89804, Meta-Test Accuracy Std: 0.08599\n",
      "\n",
      "========================Performing shear=0.2=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.08866, meta-training acc: 0.36000, meta-val loss: 1.09260, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.53932, meta-training acc: 0.84000, meta-val loss: 1.10687, meta-val acc: 0.64000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.52474, meta-training acc: 0.80000, meta-val loss: 0.50764, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.12405, meta-training acc: 0.60000, meta-val loss: 0.27300, meta-val acc: 1.00000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.50598, meta-training acc: 0.80000, meta-val loss: 0.52461, meta-val acc: 0.64000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.23739, meta-training acc: 0.64000, meta-val loss: 0.80081, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.58515, meta-training acc: 0.76000, meta-val loss: 1.09166, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.45421, meta-training acc: 0.88000, meta-val loss: 1.49436, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.64965, meta-training acc: 0.76000, meta-val loss: 0.68302, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.39379, meta-training acc: 0.84000, meta-val loss: 0.65462, meta-val acc: 0.60000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.62266, meta-training acc: 0.88000, meta-val loss: 0.20397, meta-val acc: 0.96000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.50125, meta-training acc: 0.84000, meta-val loss: 0.58421, meta-val acc: 0.76000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.28187, meta-training acc: 0.88000, meta-val loss: 0.86942, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.60845, meta-training acc: 0.80000, meta-val loss: 0.20716, meta-val acc: 0.92000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.40424, meta-training acc: 0.92000, meta-val loss: 0.44071, meta-val acc: 0.84000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 1.00028, meta-training acc: 0.60000, meta-val loss: 0.94125, meta-val acc: 0.64000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.36038, meta-training acc: 0.92000, meta-val loss: 0.55221, meta-val acc: 0.76000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.12077, meta-training acc: 0.96000, meta-val loss: 0.48258, meta-val acc: 0.84000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.75799, meta-training acc: 0.68000, meta-val loss: 1.33579, meta-val acc: 0.60000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.20668, meta-training acc: 0.68000, meta-val loss: 1.07670, meta-val acc: 0.64000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.71544, meta-training acc: 0.84000, meta-val loss: 0.21374, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.15730, meta-training acc: 0.96000, meta-val loss: 0.32303, meta-val acc: 0.92000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.14704, meta-training acc: 0.96000, meta-val loss: 0.46317, meta-val acc: 0.80000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.56757, meta-training acc: 0.76000, meta-val loss: 0.10848, meta-val acc: 0.96000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.80908, meta-training acc: 0.80000, meta-val loss: 0.31180, meta-val acc: 0.88000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.57321, meta-training acc: 0.76000, meta-val loss: 0.23038, meta-val acc: 0.96000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.88907, meta-training acc: 0.80000, meta-val loss: 0.67482, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.32789, meta-training acc: 0.88000, meta-val loss: 0.13040, meta-val acc: 0.96000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.26543, meta-training acc: 0.88000, meta-val loss: 1.03358, meta-val acc: 0.80000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.75545, meta-training acc: 0.72000, meta-val loss: 0.58768, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.50140, meta-training acc: 0.80000, meta-val loss: 0.29480, meta-val acc: 0.96000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.05992, meta-training acc: 1.00000, meta-val loss: 0.14433, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.29675, meta-training acc: 0.92000, meta-val loss: 0.20945, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.14992, meta-training acc: 0.88000, meta-val loss: 0.24384, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.04419, meta-training acc: 1.00000, meta-val loss: 0.20890, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.23676, meta-training acc: 0.92000, meta-val loss: 0.19648, meta-val acc: 0.96000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.76744, meta-training acc: 0.76000, meta-val loss: 0.73728, meta-val acc: 0.76000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.27858, meta-training acc: 0.96000, meta-val loss: 0.16938, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.12610, meta-training acc: 1.00000, meta-val loss: 0.28053, meta-val acc: 0.88000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.01040, meta-training acc: 1.00000, meta-val loss: 0.09597, meta-val acc: 1.00000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.97147, acc: 0.60000\n",
      "[meta-test episode 100/1000] => loss: 0.27495, acc: 0.88000\n",
      "[meta-test episode 150/1000] => loss: 0.34479, acc: 0.88000\n",
      "[meta-test episode 200/1000] => loss: 0.19712, acc: 0.92000\n",
      "[meta-test episode 250/1000] => loss: 0.52355, acc: 0.84000\n",
      "[meta-test episode 300/1000] => loss: 0.43800, acc: 0.88000\n",
      "[meta-test episode 350/1000] => loss: 0.80766, acc: 0.68000\n",
      "[meta-test episode 400/1000] => loss: 0.33897, acc: 0.92000\n",
      "[meta-test episode 450/1000] => loss: 0.40468, acc: 0.88000\n",
      "[meta-test episode 500/1000] => loss: 0.76245, acc: 0.68000\n",
      "[meta-test episode 550/1000] => loss: 0.16498, acc: 1.00000\n",
      "[meta-test episode 600/1000] => loss: 0.38402, acc: 0.88000\n",
      "[meta-test episode 650/1000] => loss: 0.18289, acc: 0.92000\n",
      "[meta-test episode 700/1000] => loss: 0.42200, acc: 0.88000\n",
      "[meta-test episode 750/1000] => loss: 0.45487, acc: 0.84000\n",
      "[meta-test episode 800/1000] => loss: 0.93581, acc: 0.60000\n",
      "[meta-test episode 850/1000] => loss: 0.17093, acc: 0.92000\n",
      "[meta-test episode 900/1000] => loss: 0.50536, acc: 0.80000\n",
      "[meta-test episode 950/1000] => loss: 0.50822, acc: 0.84000\n",
      "[meta-test episode 1000/1000] => loss: 0.98050, acc: 0.60000\n",
      "Average Meta-Test Accuracy: 0.83412, Meta-Test Accuracy Std: 0.10986\n",
      "\n",
      "========================Performing shear=0.4=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.43344, meta-training acc: 0.24000, meta-val loss: 1.18912, meta-val acc: 0.68000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.52949, meta-training acc: 0.80000, meta-val loss: 0.90814, meta-val acc: 0.72000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.40496, meta-training acc: 0.88000, meta-val loss: 0.42488, meta-val acc: 0.88000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.22479, meta-training acc: 0.52000, meta-val loss: 0.30209, meta-val acc: 0.96000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.40755, meta-training acc: 0.84000, meta-val loss: 0.48697, meta-val acc: 0.72000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.58810, meta-training acc: 0.48000, meta-val loss: 1.01390, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.62021, meta-training acc: 0.76000, meta-val loss: 1.03602, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.34061, meta-training acc: 0.92000, meta-val loss: 2.04845, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.57439, meta-training acc: 0.80000, meta-val loss: 0.43740, meta-val acc: 0.88000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.37891, meta-training acc: 0.96000, meta-val loss: 0.67541, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.49593, meta-training acc: 0.96000, meta-val loss: 0.47993, meta-val acc: 0.80000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.42970, meta-training acc: 0.84000, meta-val loss: 0.68327, meta-val acc: 0.80000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.15846, meta-training acc: 0.96000, meta-val loss: 0.83661, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.68970, meta-training acc: 0.76000, meta-val loss: 0.17657, meta-val acc: 1.00000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.47804, meta-training acc: 0.84000, meta-val loss: 0.30132, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.81349, meta-training acc: 0.72000, meta-val loss: 0.57752, meta-val acc: 0.80000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.52233, meta-training acc: 0.88000, meta-val loss: 0.56961, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.25352, meta-training acc: 0.92000, meta-val loss: 0.41557, meta-val acc: 0.84000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.73004, meta-training acc: 0.76000, meta-val loss: 1.71768, meta-val acc: 0.72000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.86390, meta-training acc: 0.56000, meta-val loss: 1.29799, meta-val acc: 0.60000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.36103, meta-training acc: 0.84000, meta-val loss: 0.26432, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.13185, meta-training acc: 1.00000, meta-val loss: 0.36614, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.29150, meta-training acc: 0.88000, meta-val loss: 0.43949, meta-val acc: 0.84000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.32577, meta-training acc: 0.92000, meta-val loss: 0.04497, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.76033, meta-training acc: 0.80000, meta-val loss: 0.45689, meta-val acc: 0.88000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.65829, meta-training acc: 0.80000, meta-val loss: 0.23088, meta-val acc: 0.88000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.67931, meta-training acc: 0.80000, meta-val loss: 0.69331, meta-val acc: 0.72000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.46207, meta-training acc: 0.84000, meta-val loss: 0.25889, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.30426, meta-training acc: 0.92000, meta-val loss: 0.90395, meta-val acc: 0.80000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.71534, meta-training acc: 0.76000, meta-val loss: 0.68355, meta-val acc: 0.72000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.67873, meta-training acc: 0.76000, meta-val loss: 0.58040, meta-val acc: 0.84000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.05927, meta-training acc: 1.00000, meta-val loss: 0.22262, meta-val acc: 0.84000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.25454, meta-training acc: 0.88000, meta-val loss: 0.09924, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.27414, meta-training acc: 0.92000, meta-val loss: 0.21001, meta-val acc: 0.96000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.09890, meta-training acc: 0.96000, meta-val loss: 0.65030, meta-val acc: 0.76000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.13504, meta-training acc: 0.96000, meta-val loss: 0.26582, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.89242, meta-training acc: 0.60000, meta-val loss: 0.72309, meta-val acc: 0.68000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.27929, meta-training acc: 0.96000, meta-val loss: 0.27114, meta-val acc: 0.88000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.08369, meta-training acc: 1.00000, meta-val loss: 0.12367, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.01338, meta-training acc: 1.00000, meta-val loss: 0.47593, meta-val acc: 0.84000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.80439, acc: 0.80000\n",
      "[meta-test episode 100/1000] => loss: 0.49149, acc: 0.72000\n",
      "[meta-test episode 150/1000] => loss: 0.54043, acc: 0.80000\n",
      "[meta-test episode 200/1000] => loss: 0.58707, acc: 0.84000\n",
      "[meta-test episode 250/1000] => loss: 0.50347, acc: 0.84000\n",
      "[meta-test episode 300/1000] => loss: 0.73915, acc: 0.68000\n",
      "[meta-test episode 350/1000] => loss: 1.05050, acc: 0.68000\n",
      "[meta-test episode 400/1000] => loss: 0.59132, acc: 0.76000\n",
      "[meta-test episode 450/1000] => loss: 0.95101, acc: 0.56000\n",
      "[meta-test episode 500/1000] => loss: 0.86666, acc: 0.64000\n",
      "[meta-test episode 550/1000] => loss: 0.38408, acc: 0.96000\n",
      "[meta-test episode 600/1000] => loss: 0.35967, acc: 0.84000\n",
      "[meta-test episode 650/1000] => loss: 0.41988, acc: 0.84000\n",
      "[meta-test episode 700/1000] => loss: 0.55052, acc: 0.68000\n",
      "[meta-test episode 750/1000] => loss: 0.88749, acc: 0.60000\n",
      "[meta-test episode 800/1000] => loss: 1.06609, acc: 0.68000\n",
      "[meta-test episode 850/1000] => loss: 0.44455, acc: 0.80000\n",
      "[meta-test episode 900/1000] => loss: 1.10136, acc: 0.64000\n",
      "[meta-test episode 950/1000] => loss: 0.85427, acc: 0.76000\n",
      "[meta-test episode 1000/1000] => loss: 0.89881, acc: 0.52000\n",
      "Average Meta-Test Accuracy: 0.75692, Meta-Test Accuracy Std: 0.12297\n",
      "\n",
      "========================Performing shear=0.6=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.10797, meta-training acc: 0.48000, meta-val loss: 0.99260, meta-val acc: 0.76000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.51876, meta-training acc: 0.88000, meta-val loss: 0.90138, meta-val acc: 0.64000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.50470, meta-training acc: 0.68000, meta-val loss: 0.59262, meta-val acc: 0.76000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.99405, meta-training acc: 0.60000, meta-val loss: 0.34982, meta-val acc: 0.92000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.42562, meta-training acc: 0.84000, meta-val loss: 0.60103, meta-val acc: 0.72000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.46300, meta-training acc: 0.48000, meta-val loss: 0.59232, meta-val acc: 0.84000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.57267, meta-training acc: 0.84000, meta-val loss: 0.77324, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.25101, meta-training acc: 0.96000, meta-val loss: 2.37732, meta-val acc: 0.72000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.74958, meta-training acc: 0.76000, meta-val loss: 0.61496, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.41421, meta-training acc: 0.84000, meta-val loss: 0.61040, meta-val acc: 0.72000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.60248, meta-training acc: 0.76000, meta-val loss: 0.23447, meta-val acc: 0.96000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.36221, meta-training acc: 0.92000, meta-val loss: 0.54429, meta-val acc: 0.80000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.15734, meta-training acc: 0.96000, meta-val loss: 0.85454, meta-val acc: 0.68000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.64706, meta-training acc: 0.80000, meta-val loss: 0.23025, meta-val acc: 0.92000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.45157, meta-training acc: 0.92000, meta-val loss: 0.33598, meta-val acc: 0.80000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.86802, meta-training acc: 0.72000, meta-val loss: 0.85282, meta-val acc: 0.56000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.48348, meta-training acc: 0.92000, meta-val loss: 0.44193, meta-val acc: 0.80000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.43799, meta-training acc: 0.84000, meta-val loss: 0.25868, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.64091, meta-training acc: 0.72000, meta-val loss: 1.57156, meta-val acc: 0.68000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.93967, meta-training acc: 0.64000, meta-val loss: 1.14064, meta-val acc: 0.68000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.37386, meta-training acc: 0.92000, meta-val loss: 0.33914, meta-val acc: 0.80000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.15172, meta-training acc: 0.92000, meta-val loss: 0.40878, meta-val acc: 0.80000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.42377, meta-training acc: 0.76000, meta-val loss: 0.53934, meta-val acc: 0.80000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.58662, meta-training acc: 0.84000, meta-val loss: 0.06008, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 1.03393, meta-training acc: 0.68000, meta-val loss: 0.28234, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.59685, meta-training acc: 0.80000, meta-val loss: 0.41132, meta-val acc: 0.84000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.59840, meta-training acc: 0.80000, meta-val loss: 0.52706, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.65225, meta-training acc: 0.84000, meta-val loss: 0.09386, meta-val acc: 0.96000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.16384, meta-training acc: 0.92000, meta-val loss: 0.64667, meta-val acc: 0.80000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.66544, meta-training acc: 0.64000, meta-val loss: 0.42284, meta-val acc: 0.76000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.58934, meta-training acc: 0.76000, meta-val loss: 0.21462, meta-val acc: 0.96000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12424, meta-training acc: 0.96000, meta-val loss: 0.31424, meta-val acc: 0.84000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.21983, meta-training acc: 0.88000, meta-val loss: 0.13105, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.18812, meta-training acc: 0.96000, meta-val loss: 0.40382, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.10623, meta-training acc: 0.96000, meta-val loss: 0.49011, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.25381, meta-training acc: 0.96000, meta-val loss: 0.29247, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 1.03157, meta-training acc: 0.56000, meta-val loss: 0.75352, meta-val acc: 0.76000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.20936, meta-training acc: 0.96000, meta-val loss: 0.25098, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.18345, meta-training acc: 0.92000, meta-val loss: 0.19056, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.01193, meta-training acc: 1.00000, meta-val loss: 0.30292, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.76764, acc: 0.64000\n",
      "[meta-test episode 100/1000] => loss: 0.67948, acc: 0.76000\n",
      "[meta-test episode 150/1000] => loss: 1.21540, acc: 0.80000\n",
      "[meta-test episode 200/1000] => loss: 0.45956, acc: 0.84000\n",
      "[meta-test episode 250/1000] => loss: 0.59889, acc: 0.72000\n",
      "[meta-test episode 300/1000] => loss: 0.91472, acc: 0.68000\n",
      "[meta-test episode 350/1000] => loss: 1.78029, acc: 0.64000\n",
      "[meta-test episode 400/1000] => loss: 0.99039, acc: 0.60000\n",
      "[meta-test episode 450/1000] => loss: 1.22531, acc: 0.72000\n",
      "[meta-test episode 500/1000] => loss: 1.13423, acc: 0.52000\n",
      "[meta-test episode 550/1000] => loss: 0.66345, acc: 0.76000\n",
      "[meta-test episode 600/1000] => loss: 0.48458, acc: 0.88000\n",
      "[meta-test episode 650/1000] => loss: 0.68672, acc: 0.84000\n",
      "[meta-test episode 700/1000] => loss: 1.30611, acc: 0.60000\n",
      "[meta-test episode 750/1000] => loss: 1.13404, acc: 0.56000\n",
      "[meta-test episode 800/1000] => loss: 1.29523, acc: 0.52000\n",
      "[meta-test episode 850/1000] => loss: 0.38269, acc: 0.88000\n",
      "[meta-test episode 900/1000] => loss: 1.14364, acc: 0.52000\n",
      "[meta-test episode 950/1000] => loss: 1.22856, acc: 0.68000\n",
      "[meta-test episode 1000/1000] => loss: 1.28742, acc: 0.44000\n",
      "Average Meta-Test Accuracy: 0.67396, Meta-Test Accuracy Std: 0.13804\n",
      "\n",
      "========================Performing shear=0.8=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.17013, meta-training acc: 0.44000, meta-val loss: 1.10929, meta-val acc: 0.76000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.52801, meta-training acc: 0.84000, meta-val loss: 1.06445, meta-val acc: 0.64000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.52532, meta-training acc: 0.76000, meta-val loss: 0.52584, meta-val acc: 0.80000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 1.08356, meta-training acc: 0.56000, meta-val loss: 0.24562, meta-val acc: 1.00000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.41975, meta-training acc: 0.84000, meta-val loss: 0.61674, meta-val acc: 0.72000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.37037, meta-training acc: 0.60000, meta-val loss: 0.76158, meta-val acc: 0.68000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.42798, meta-training acc: 0.92000, meta-val loss: 0.97102, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.37929, meta-training acc: 0.96000, meta-val loss: 1.29599, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.76379, meta-training acc: 0.68000, meta-val loss: 0.67784, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.41165, meta-training acc: 0.80000, meta-val loss: 0.57977, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.51845, meta-training acc: 0.80000, meta-val loss: 0.24044, meta-val acc: 0.88000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.42079, meta-training acc: 0.84000, meta-val loss: 0.64717, meta-val acc: 0.72000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.37992, meta-training acc: 0.88000, meta-val loss: 0.77629, meta-val acc: 0.60000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.58601, meta-training acc: 0.84000, meta-val loss: 0.21549, meta-val acc: 0.88000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.50147, meta-training acc: 0.88000, meta-val loss: 0.26711, meta-val acc: 0.84000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 1.27329, meta-training acc: 0.44000, meta-val loss: 0.70818, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.33670, meta-training acc: 0.92000, meta-val loss: 0.35107, meta-val acc: 0.84000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.12902, meta-training acc: 0.96000, meta-val loss: 0.38478, meta-val acc: 0.88000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.62939, meta-training acc: 0.68000, meta-val loss: 1.39343, meta-val acc: 0.76000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 1.35075, meta-training acc: 0.68000, meta-val loss: 0.62980, meta-val acc: 0.76000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.57024, meta-training acc: 0.92000, meta-val loss: 0.23924, meta-val acc: 0.84000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.07114, meta-training acc: 1.00000, meta-val loss: 0.21713, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.33775, meta-training acc: 0.88000, meta-val loss: 0.34587, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.53091, meta-training acc: 0.80000, meta-val loss: 0.10992, meta-val acc: 0.96000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.87634, meta-training acc: 0.72000, meta-val loss: 0.21057, meta-val acc: 0.96000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.40872, meta-training acc: 0.76000, meta-val loss: 0.24729, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 1.05529, meta-training acc: 0.80000, meta-val loss: 0.50234, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.46586, meta-training acc: 0.88000, meta-val loss: 0.11441, meta-val acc: 0.92000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.30188, meta-training acc: 0.88000, meta-val loss: 0.75636, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.55316, meta-training acc: 0.76000, meta-val loss: 0.31157, meta-val acc: 0.84000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.61899, meta-training acc: 0.80000, meta-val loss: 0.24246, meta-val acc: 0.96000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.04431, meta-training acc: 1.00000, meta-val loss: 0.31160, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.17099, meta-training acc: 0.92000, meta-val loss: 0.07825, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.18157, meta-training acc: 0.92000, meta-val loss: 0.21597, meta-val acc: 0.96000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.14195, meta-training acc: 0.92000, meta-val loss: 0.21171, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.13095, meta-training acc: 1.00000, meta-val loss: 0.26429, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.87949, meta-training acc: 0.56000, meta-val loss: 1.04399, meta-val acc: 0.72000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.27991, meta-training acc: 0.96000, meta-val loss: 0.46088, meta-val acc: 0.92000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.05246, meta-training acc: 1.00000, meta-val loss: 0.13700, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.00404, meta-training acc: 1.00000, meta-val loss: 0.07757, meta-val acc: 1.00000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 1.25288, acc: 0.56000\n",
      "[meta-test episode 100/1000] => loss: 0.94271, acc: 0.76000\n",
      "[meta-test episode 150/1000] => loss: 1.49897, acc: 0.64000\n",
      "[meta-test episode 200/1000] => loss: 0.89416, acc: 0.72000\n",
      "[meta-test episode 250/1000] => loss: 1.06874, acc: 0.64000\n",
      "[meta-test episode 300/1000] => loss: 0.93271, acc: 0.56000\n",
      "[meta-test episode 350/1000] => loss: 1.27406, acc: 0.52000\n",
      "[meta-test episode 400/1000] => loss: 1.25117, acc: 0.56000\n",
      "[meta-test episode 450/1000] => loss: 1.25841, acc: 0.56000\n",
      "[meta-test episode 500/1000] => loss: 1.10886, acc: 0.52000\n",
      "[meta-test episode 550/1000] => loss: 0.94263, acc: 0.56000\n",
      "[meta-test episode 600/1000] => loss: 0.93456, acc: 0.60000\n",
      "[meta-test episode 650/1000] => loss: 0.96452, acc: 0.80000\n",
      "[meta-test episode 700/1000] => loss: 1.05539, acc: 0.52000\n",
      "[meta-test episode 750/1000] => loss: 1.31751, acc: 0.44000\n",
      "[meta-test episode 800/1000] => loss: 1.53897, acc: 0.44000\n",
      "[meta-test episode 850/1000] => loss: 0.85969, acc: 0.72000\n",
      "[meta-test episode 900/1000] => loss: 1.38878, acc: 0.56000\n",
      "[meta-test episode 950/1000] => loss: 1.38131, acc: 0.60000\n",
      "[meta-test episode 1000/1000] => loss: 1.31379, acc: 0.44000\n",
      "Average Meta-Test Accuracy: 0.58240, Meta-Test Accuracy Std: 0.13842\n"
     ]
    }
   ],
   "source": [
    "for p in range(-8,10,2):\n",
    "    p = p/10\n",
    "    print(f'\\n========================Performing shear={p}=========================\\n')\n",
    "    run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,shear=p,scale=None,latent_dim = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Performing shear=0.0=========================\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py:563: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "[epo 1/50, epi 50/100] => meta-training loss: 0.16042, meta-training acc: 0.20000, meta-val loss: 0.16012, meta-val acc: 0.20000\n",
      "[epo 1/50, epi 100/100] => meta-training loss: 0.16053, meta-training acc: 0.20000, meta-val loss: 0.16029, meta-val acc: 0.20000\n",
      "[epo 2/50, epi 50/100] => meta-training loss: 0.16013, meta-training acc: 0.20000, meta-val loss: 0.16029, meta-val acc: 0.20000\n",
      "[epo 2/50, epi 100/100] => meta-training loss: 0.16011, meta-training acc: 0.20000, meta-val loss: 0.16005, meta-val acc: 0.20000\n",
      "[epo 3/50, epi 50/100] => meta-training loss: 0.16006, meta-training acc: 0.20000, meta-val loss: 0.16004, meta-val acc: 0.20000\n",
      "[epo 3/50, epi 100/100] => meta-training loss: 0.16010, meta-training acc: 0.20000, meta-val loss: 0.16004, meta-val acc: 0.20000\n",
      "[epo 4/50, epi 50/100] => meta-training loss: 0.16006, meta-training acc: 0.20000, meta-val loss: 0.16003, meta-val acc: 0.20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-c51a04b3f386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n========================Performing shear={p}=========================\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n\u001b[0;32m----> 5\u001b[0;31m                  k_meta_test_shot=1, n_meta_test_query=5,shear=p,scale=None,rel=True,n_epochs=50,latent_dim = 32)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-f5a13e2934de>\u001b[0m in \u001b[0;36mrun_protonet\u001b[0;34m(data_path, n_way, k_shot, n_query, n_meta_test_way, k_meta_test_shot, n_meta_test_query, shear, scale, rel, n_epochs, latent_dim)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;31m#############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_net_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_ph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta_val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f5a13e2934de>\u001b[0m in \u001b[0;36mproto_net_train_step\u001b[0;34m(model, optim, x, q, labels_ph, rel)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           kwargs={\"name\": name})\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_distributed_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1947\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1949\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   1955\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    486\u001b[0m           update_ops.extend(\n\u001b[1;32m    487\u001b[0m               distribution.extended.update(\n\u001b[0;32m--> 488\u001b[0;31m                   var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   1541\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[0;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m     \u001b[0;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2174\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   2178\u001b[0m     \u001b[0;31m# once that value is used for something.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"apply_state\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"apply_state\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[0;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0mcoefficients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m           use_locking=self._use_locking)\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vhat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/training/gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_locking\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_nesterov\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m         use_nesterov)\n\u001b[0m\u001b[1;32m   1413\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for p in range(0,10,2):\n",
    "    p = p/10\n",
    "    print(f'\\n========================Performing shear={p}=========================\\n')\n",
    "    run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,shear=p,scale=None,rel=True,n_epochs=50,latent_dim = 32)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
