{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Utility functions. \"\"\"\n",
    "import pdb\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "## Loss utilities\n",
    "def cross_entropy_loss(pred, label, k_shot):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data loading scripts\"\"\"\n",
    "from scipy import misc\n",
    "import imageio\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    Args:\n",
    "    paths: A list of character folders\n",
    "    labels: List or numpy array of same length as paths\n",
    "    n_samples: Number of images to retrieve per character\n",
    "    Returns:\n",
    "    List of (label, image_path) tuples\n",
    "    \"\"\"\n",
    "    if n_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, n_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "           for i, path in zip(labels, paths)\n",
    "           for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels\n",
    "\n",
    "\n",
    "def image_file_to_array(filename, dim_input, shear=None, scale=None):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns numpy array\n",
    "    Args:\n",
    "    filename: Image filename\n",
    "    dim_input: Flattened shape of image\n",
    "    Returns:\n",
    "    1 channel image\n",
    "    \"\"\"\n",
    "    image = imageio.imread(filename)\n",
    "    if 'mnist' in filename:\n",
    "        image = image>150\n",
    "        image = image.astype(np.int8)\n",
    "    if 'hand_digits' in filename:\n",
    "        image = image[:,:,0]\n",
    "        image = image>150\n",
    "        image = image.astype(np.int8)\n",
    "#         print(image)\n",
    "    \n",
    "    if shear is not None:\n",
    "        afine_tf = transform.AffineTransform(shear=shear)\n",
    "        image = transform.warp(image, inverse_map=afine_tf.inverse)\n",
    "\n",
    "    elif scale is not None:\n",
    "        afine_tf = transform.AffineTransform(scale=scale)\n",
    "        image = transform.warp(image, inverse_map=afine_tf.inverse)\n",
    "#     pdb.set_trace()\n",
    "    image = transform.resize(image,(28,28),anti_aliasing=False,order=0)*255\n",
    "#     pdb.set_trace()\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    if 'mnist' in filename:\n",
    "        return image\n",
    "    image = 1.0 - image\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_classes: Number of classes for classification (K-way)\n",
    "          num_samples_per_class: num samples to generate per class in one batch\n",
    "          num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
    "          num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
    "          batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "        self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
    "        self.num_meta_test_classes = num_meta_test_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './omniglot')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "        \n",
    "        if isinstance(data_folder,str):            \n",
    "\n",
    "            character_folders = [os.path.join(data_folder, family, character)\n",
    "                       for family in os.listdir(data_folder)\n",
    "                       if os.path.isdir(os.path.join(data_folder, family))\n",
    "                       for character in os.listdir(os.path.join(data_folder, family))\n",
    "                       if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "            random.seed(42)\n",
    "            random.shuffle(character_folders)\n",
    "            num_val = 100\n",
    "            num_train = 1100\n",
    "            self.metatrain_character_folders = character_folders[: num_train]\n",
    "            self.metaval_character_folders = character_folders[\n",
    "              num_train:num_train + num_val]\n",
    "            self.metatest_character_folders = character_folders[\n",
    "              num_train + num_val:]\n",
    "        else:\n",
    "            train_folders = [os.path.join(data_folder[0], family, character)\n",
    "                       for family in os.listdir(data_folder[0])\n",
    "                       if os.path.isdir(os.path.join(data_folder[0], family))\n",
    "                       for character in os.listdir(os.path.join(data_folder[0], family))\n",
    "                       if os.path.isdir(os.path.join(data_folder[0], family, character))]\n",
    "            test_folders = [os.path.join(data_folder[1], family, character)\n",
    "                       for family in os.listdir(data_folder[1])\n",
    "                       if os.path.isdir(os.path.join(data_folder[1], family))\n",
    "                       for character in os.listdir(os.path.join(data_folder[1], family))\n",
    "                       if os.path.isdir(os.path.join(data_folder[1], family, character))]\n",
    "            random.seed(42)\n",
    "            random.shuffle(train_folders)\n",
    "            self.metatrain_character_folders = copy.deepcopy(train_folders)\n",
    "            self.metaval_character_folders = copy.deepcopy(train_folders)\n",
    "            self.metatest_character_folders = test_folders\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False, shear=None, scale=None):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "          batch_type: meta_train/meta_val/meta_test\n",
    "          shuffle: randomly shuffle classes or not\n",
    "          swap: swap number of classes (N) and number of samples per class (K) or not\n",
    "        Returns:\n",
    "          A a tuple of (1) Image batch and (2) Label batch where\n",
    "          image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
    "          where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"meta_train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        elif batch_type == \"meta_val\":\n",
    "            folders = self.metaval_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "            num_classes = self.num_meta_test_classes\n",
    "            num_samples_per_class = self.num_meta_test_samples_per_class\n",
    "\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        for i in range(batch_size):\n",
    "            sampled_character_folders = random.sample(\n",
    "            folders, num_classes)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(\n",
    "            num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
    "            labels = [li[0] for li in labels_and_images]\n",
    "            images = [image_file_to_array(\n",
    "            li[1], self.dim_input,shear,scale) for li in labels_and_images]\n",
    "            images = np.stack(images)\n",
    "            labels = np.array(labels).astype(np.int32)\n",
    "            labels = np.reshape(\n",
    "            labels, (num_classes, num_samples_per_class))\n",
    "            labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
    "            images = np.reshape(\n",
    "            images, (num_classes, num_samples_per_class, -1))\n",
    "\n",
    "            batch = np.concatenate([labels, images], 2)\n",
    "            if shuffle:\n",
    "                for p in range(num_samples_per_class):\n",
    "                    np.random.shuffle(batch[:, p])\n",
    "\n",
    "            labels = batch[:, :, :num_classes]\n",
    "            images = batch[:, :, num_classes:]\n",
    "\n",
    "            if swap:\n",
    "                labels = np.swapaxes(labels, 0, 1)\n",
    "                images = np.swapaxes(images, 0, 1)\n",
    "\n",
    "            all_image_batches.append(images)\n",
    "            all_label_batches.append(labels)\n",
    "        all_image_batches = np.stack(all_image_batches)\n",
    "        all_label_batches = np.stack(all_label_batches)\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### Debug \n",
    "data_generator = DataGenerator(1, 1,1,1,config={'data_folder':['mnist', 'hand_digits']})\n",
    "i, l = data_generator.sample_batch('meta_test', 6)\n",
    "b = 1\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8, 8))\n",
    "cnt = 0\n",
    "for c in range(1):\n",
    "    for k in range(1):\n",
    "        plt.subplot(1, 1, cnt + 1)\n",
    "        plt.title(f'Class {c}-Example {k}')\n",
    "        image = i[b, c, k].reshape((28,28))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off');\n",
    "        cnt += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RelationNet\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "class RelationNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_filters = 64, hidden_dim = 16):\n",
    "        super(RelationNet, self).__init__()\n",
    "#         self.num_filters = num_filters\n",
    "#         # self.latent_dim = latent_dim\n",
    "#         num_filter_list = [self.num_filters]*2\n",
    "#         self.convs = []\n",
    "#         for i, num_filter in enumerate(num_filter_list):\n",
    "#             block_parts = [\n",
    "#             layers.Conv2D(\n",
    "#               filters=num_filter,\n",
    "#               kernel_size=3,\n",
    "#               padding='SAME',\n",
    "#               activation='linear'),\n",
    "#             ]\n",
    "\n",
    "#             block_parts += [layers.BatchNormalization(momentum = 1)]\n",
    "#             block_parts += [layers.Activation('relu')]\n",
    "#             block_parts += [layers.MaxPool2D()]\n",
    "#             block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
    "#             self.__setattr__(\"conv%d\" % i, block)\n",
    "#             self.convs.append(block)\n",
    "#         self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = layers.Dense(hidden_dim,activation = 'relu')\n",
    "        self.fc2 = layers.Dense(1,activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inp):\n",
    "        out = inp\n",
    "        # for conv in self.convs:\n",
    "          # out = conv(out)\n",
    "        # out = self.flatten(out)\n",
    "        out = self.fc2(self.fc1(out))\n",
    "        return out\n",
    "\n",
    "def RelationLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries, relation_net):\n",
    "    \"\"\"\n",
    "    calculates the prototype network loss using the latent representation of x\n",
    "    and the latent representation of the query set\n",
    "    Args:\n",
    "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "      num_classes: number of classes (N) for classification\n",
    "      num_support: number of examples (S) in the support set\n",
    "      num_queries: number of examples (Q) in the query set\n",
    "    Returns:\n",
    "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "      acc: the accuracy of classification on the queries\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x_latent, shape=[num_classes, num_support, -1])\n",
    "    # print(x.numpy().shape)\n",
    "    prototypes = tf.reduce_mean(x, axis = 1, keepdims=False) #(N, D)\n",
    "    prototypes_ext = tf.repeat(prototypes, repeats = [num_classes*num_queries]*num_classes, axis = 0) #(N*Q*N, D)\n",
    "    q_ext = tf.tile(q_latent, multiples = [num_classes,1]) # (N*N*Q, D)\n",
    "    labels_ = tf.reshape(labels_onehot, [-1, num_classes]) #(N*Q, N)\n",
    "    scores_ = relation_net(tf.concat([q_ext, prototypes_ext], axis = -1)) #(N*NQ, 1)\n",
    "    scores_ = tf.split(scores_,num_classes,0)\n",
    "    scores = tf.concat(scores_, axis = -1) #(NQ,N)\n",
    "\n",
    "    loss = losses.mean_squared_error(labels_, scores) #(NQ,)\n",
    "    loss = tf.reduce_mean(loss)#+tf.reduce_sum(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)) #(1,)\n",
    "\n",
    "#     loss = cross_entropy_loss(scores,labels_, k_shot=1)\n",
    "    acc = accuracy(tf.argmax(input=labels_, axis=1), tf.argmax(input=scores, axis=1))\n",
    "    # print(ce_loss.numpy())\n",
    "    # print(acc.numpy())\n",
    "\n",
    "    # print(error)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "#Shape checking\n",
    "\n",
    "# q = tf.random.uniform(shape = [10,5])\n",
    "# print(q.shape)\n",
    "# q_ext = tf.tile(q, multiples = [2,1])\n",
    "# q_rep = tf.repeat(q, repeats = [2], axis = 0)\n",
    "# print(q_rep.shape)\n",
    "# print(q)\n",
    "# print(q_rep)\n",
    "# print(tf.concat([q,q], axis = -1))\n",
    "\n",
    "\n",
    "# t = tf.random.uniform(shape = [12,1])\n",
    "# t_ = tf.split(t,3,0)\n",
    "# t__ = tf.concat(t_, axis = -1)\n",
    "\n",
    "# t1 = tf.random.uniform(shape = [12,1])\n",
    "# t1_ = tf.split(t1,3,0)\n",
    "# t1__ = tf.concat(t1_, axis = -1)\n",
    "\n",
    "# print(losses.mean_squared_error(t__,t1__,))\n",
    "\n",
    "# print(t)\n",
    "# print(t_)\n",
    "# print(t__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/ProtoNet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class ProtoNet(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_filters, latent_dim):\n",
    "    super(ProtoNet, self).__init__()\n",
    "    self.num_filters = num_filters\n",
    "    self.latent_dim = latent_dim\n",
    "    num_filter_list = self.num_filters + [latent_dim]\n",
    "    self.convs = []\n",
    "    for i, num_filter in enumerate(num_filter_list):\n",
    "      block_parts = [\n",
    "        layers.Conv2D(\n",
    "          filters=num_filter,\n",
    "          kernel_size=3,\n",
    "          padding='SAME',\n",
    "          activation='linear'),\n",
    "      ]\n",
    "\n",
    "      block_parts += [layers.BatchNormalization()]\n",
    "      block_parts += [layers.Activation('relu')]\n",
    "      block_parts += [layers.MaxPool2D()]\n",
    "      block = tf.keras.Sequential(block_parts, name='conv_block_%d' % i)\n",
    "      self.__setattr__(\"conv%d\" % i, block)\n",
    "      self.convs.append(block)\n",
    "    self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "  def call(self, inp):\n",
    "    out = inp\n",
    "    for conv in self.convs:\n",
    "      out = conv(out)\n",
    "    out = self.flatten(out)\n",
    "    return out\n",
    "\n",
    "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
    "  \"\"\"\n",
    "    calculates the prototype network loss using the latent representation of x\n",
    "    and the latent representation of the query set\n",
    "    Args:\n",
    "      x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "      q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "      labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "      num_classes: number of classes (N) for classification\n",
    "      num_support: number of examples (S) in the support set\n",
    "      num_queries: number of examples (Q) in the query set\n",
    "    Returns:\n",
    "      ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "      acc: the accuracy of classification on the queries\n",
    "  \"\"\"\n",
    "  x_latent = tf.reshape(x_latent,[num_classes,num_support,-1])\n",
    "  c = tf.reduce_mean(x_latent,axis=1)\n",
    "  def dist(q, c):\n",
    "    N_Q, D = q.shape\n",
    "    N = c.shape[0]\n",
    "    q = tf.tile(tf.expand_dims(q, axis=1), [1, N, 1])\n",
    "    c = tf.tile(tf.expand_dims(c, axis=0), [N_Q, 1, 1])\n",
    "    return tf.reduce_mean(tf.square(q-c), axis=2)\n",
    "  distances = tf.reshape(dist(q_latent, c), [num_classes,num_queries,num_classes])\n",
    "  ce_loss = cross_entropy_loss(-distances, labels_onehot, 1)\n",
    "  acc = accuracy(tf.argmax(labels_onehot, axis=-1),tf.argmax(tf.reshape(tf.nn.softmax(-distances), \n",
    "                                                                 [num_classes, num_queries, -1]), axis=-1))\n",
    "  #############################\n",
    "  return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ProtoNet\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def proto_net_train_step(model, optim, x, q, labels_ph, rel=None):\n",
    "    num_classes, num_support, im_height, im_width, channels = x.shape\n",
    "    num_queries = q.shape[1]\n",
    "    x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
    "    q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_latent = model(x)\n",
    "        q_latent = model(q)\n",
    "        if rel is None:\n",
    "            ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
    "        else:\n",
    "            ce_loss, acc = RelationLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries,rel)\n",
    "\n",
    "    if rel is None:\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    else:\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables+rel.trainable_variables)\n",
    "#         print(model.trainable_variables+rel.trainable_variables)\n",
    "#         1/0\n",
    "        optim.apply_gradients(zip(gradients, model.trainable_variables+rel.trainable_variables))\n",
    "    \n",
    "    return ce_loss, acc\n",
    "\n",
    "def proto_net_eval(model, x, q, labels_ph,rel=None):\n",
    "  num_classes, num_support, im_height, im_width, channels = x.shape\n",
    "  num_queries = q.shape[1]\n",
    "  x = tf.reshape(x, [-1, im_height, im_width, channels])\n",
    "  q = tf.reshape(q, [-1, im_height, im_width, channels])\n",
    "\n",
    "  x_latent = model(x)\n",
    "  q_latent = model(q)\n",
    "  if rel is None:\n",
    "    ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries)\n",
    "  else:\n",
    "    ce_loss, acc = RelationLoss(x_latent, q_latent, labels_ph, num_classes, num_support, num_queries,rel)\n",
    "\n",
    "  return ce_loss, acc \n",
    "\n",
    "def run_protonet(data_path='./omniglot', n_way=20, k_shot=1, n_query=5, n_meta_test_way=20, \n",
    "                 k_meta_test_shot=5, n_meta_test_query=5,shear=None,scale=None, rel=False, n_epochs=20, latent_dim = 16):\n",
    "  \n",
    "  n_episodes = 100\n",
    "\n",
    "  im_width, im_height, channels = 28, 28, 1\n",
    "  num_filters = 32\n",
    "  num_conv_layers = 3\n",
    "  n_meta_test_episodes = 1000\n",
    "\n",
    "  model = ProtoNet([num_filters]*num_conv_layers, latent_dim)\n",
    "  if not rel:\n",
    "    rel_model=None\n",
    "  else:\n",
    "    rel_model = RelationNet()\n",
    "  optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # call DataGenerator with k_shot+n_query samples per class\n",
    "  data_generator = DataGenerator(n_way, k_shot+n_query, n_meta_test_way, k_meta_test_shot+n_meta_test_query,\n",
    "                                 config={'data_folder':data_path})\n",
    "  for ep in range(n_epochs):\n",
    "    for epi in range(n_episodes):\n",
    "      image_batch, label_batch = data_generator.sample_batch('meta_train',1,shuffle=False)\n",
    "      image, label = image_batch[0], label_batch[0]\n",
    "      support,query = image[:,:k_shot,:], image[:,k_shot:,:]\n",
    "      labels = label[:,k_shot:,:]\n",
    "      # Reshape\n",
    "      support = tf.reshape(support,[-1,k_shot,im_width, im_height, channels])\n",
    "      query = tf.reshape(query,[-1,n_query,im_width, im_height, channels])\n",
    "      #############################\n",
    "      ls, ac = proto_net_train_step(model, optimizer, x=support, q=query, labels_ph=labels, rel=rel_model)\n",
    "      if (epi+1) % 50 == 0:\n",
    "        image_batch, label_batch = data_generator.sample_batch('meta_val',1,shuffle=False)\n",
    "        image, label = image_batch[0], label_batch[0]\n",
    "        support,query = image[:,:k_shot,:], image[:,k_shot:,:]\n",
    "        labels = label[:,k_shot:,:]\n",
    "        # Reshape\n",
    "        support = tf.reshape(support,[-1,k_shot,im_width, im_height, channels])\n",
    "        query = tf.reshape(query,[-1,n_query,im_width, im_height, channels])\n",
    "        #############################\n",
    "        val_ls, val_ac = proto_net_eval(model, x=support, q=query, labels_ph=labels, rel=rel_model)\n",
    "        print('[epo {}/{}, epi {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(ep+1,\n",
    "                                                                    n_epochs,\n",
    "                                                                    epi+1,\n",
    "                                                                    n_episodes,\n",
    "                                                                    ls,\n",
    "                                                                    ac,\n",
    "                                                                    val_ls,\n",
    "                                                                    val_ac))\n",
    "\n",
    "  print('Testing...')\n",
    "  meta_test_accuracies = []\n",
    "  for epi in range(n_meta_test_episodes):\n",
    "    # sample a batch of test data and partition into\n",
    "    # support and query sets\n",
    "    image_batch, label_batch = data_generator.sample_batch('meta_test',1,shuffle=False,shear=shear,scale=scale)\n",
    "    image, label = image_batch[0], label_batch[0]\n",
    "    support,query = image[:,:k_meta_test_shot,:], image[:,k_meta_test_shot:,:]\n",
    "    labels = label[:,k_meta_test_shot:,:]\n",
    "    # Reshape\n",
    "    support = tf.reshape(support,[-1,k_meta_test_shot,im_width, im_height, channels])\n",
    "    query = tf.reshape(query,[-1,n_meta_test_query,im_width, im_height, channels])\n",
    "    #############################\n",
    "    ls, ac = proto_net_eval(model, x=support, q=query, labels_ph=labels,rel=rel_model)\n",
    "    meta_test_accuracies.append(ac)\n",
    "    if (epi+1) % 50 == 0:\n",
    "      print('[meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(epi+1, n_meta_test_episodes, ls, ac))\n",
    "  avg_acc = np.mean(meta_test_accuracies)\n",
    "  stds = np.std(meta_test_accuracies)\n",
    "  print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))\n",
    "  # return meta_test_accuracies, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epo 1/20, epi 50/100] => meta-training loss: 0.16069, meta-training acc: 0.20000, meta-val loss: 0.16069, meta-val acc: 0.20000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.16000, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-8dad4b0d52fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m run_protonet(data_path=['mnist', 'hand_digits'], n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n\u001b[0;32m----> 2\u001b[0;31m                  k_meta_test_shot=1, n_meta_test_query=5,latent_dim = 16, rel=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-155-da708f2afe1a>\u001b[0m in \u001b[0;36mrun_protonet\u001b[0;34m(data_path, n_way, k_shot, n_query, n_meta_test_way, k_meta_test_shot, n_meta_test_query, shear, scale, rel, n_epochs, latent_dim)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mim_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0;31m#############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m       \u001b[0mls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproto_net_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_ph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrel_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta_val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-da708f2afe1a>\u001b[0m in \u001b[0;36mproto_net_train_step\u001b[0;34m(model, optim, x, q, labels_ph, rel)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mq_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProtoLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_queries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-513e8b5ec134>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fused_batch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;31m# Currently never reaches here since fused_batch_norm does not support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     output, mean, variance = tf_utils.smart_cond(\n\u001b[0;32m--> 553\u001b[0;31m         training, _fused_batch_norm_training, _fused_batch_norm_inference)\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bessels_correction_test_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m       \u001b[0;31m# Remove Bessel's correction to be consistent with non-fused batch norm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[1;32m     58\u001b[0m   return smart_module.smart_cond(\n\u001b[0;32m---> 59\u001b[0;31m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_fused_batch_norm_inference\u001b[0;34m()\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m           \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m           data_format=self._data_format)\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     output, mean, variance = tf_utils.smart_cond(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mfused_batch_norm\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m   1501\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_v3\u001b[0;34m(x, scale, offset, mean, variance, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4219\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epsilon\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4221\u001b[0;31m         epsilon, \"data_format\", data_format, \"is_training\", is_training)\n\u001b[0m\u001b[1;32m   4222\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FusedBatchNormV3Output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4223\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_protonet(data_path=['mnist', 'hand_digits'], n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,latent_dim = 16, rel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Performing scale=0.25=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.47260, meta-training acc: 0.44000, meta-val loss: 0.95442, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.52572, meta-training acc: 0.80000, meta-val loss: 0.52896, meta-val acc: 0.88000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.90389, meta-training acc: 0.64000, meta-val loss: 0.63522, meta-val acc: 0.76000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.59887, meta-training acc: 0.72000, meta-val loss: 0.62829, meta-val acc: 0.84000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.43263, meta-training acc: 0.84000, meta-val loss: 0.63060, meta-val acc: 0.88000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.10159, meta-training acc: 0.48000, meta-val loss: 0.58155, meta-val acc: 0.80000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.11795, meta-training acc: 0.64000, meta-val loss: 0.51839, meta-val acc: 0.88000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.56714, meta-training acc: 0.88000, meta-val loss: 0.41846, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.57797, meta-training acc: 0.76000, meta-val loss: 0.76173, meta-val acc: 0.68000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.69812, meta-training acc: 0.64000, meta-val loss: 0.62972, meta-val acc: 0.80000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.37220, meta-training acc: 0.92000, meta-val loss: 0.79011, meta-val acc: 0.68000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.35581, meta-training acc: 0.84000, meta-val loss: 0.07004, meta-val acc: 1.00000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.94244, meta-training acc: 0.64000, meta-val loss: 1.08196, meta-val acc: 0.60000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 1.00356, meta-training acc: 0.60000, meta-val loss: 0.81338, meta-val acc: 0.68000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.37860, meta-training acc: 0.84000, meta-val loss: 0.16267, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.40634, meta-training acc: 0.80000, meta-val loss: 0.17252, meta-val acc: 0.92000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.27680, meta-training acc: 0.88000, meta-val loss: 0.92425, meta-val acc: 0.76000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.71603, meta-training acc: 0.68000, meta-val loss: 0.15501, meta-val acc: 1.00000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.22556, meta-training acc: 0.96000, meta-val loss: 0.20142, meta-val acc: 0.96000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.50237, meta-training acc: 0.80000, meta-val loss: 0.19668, meta-val acc: 0.92000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.29380, meta-training acc: 0.88000, meta-val loss: 0.13202, meta-val acc: 1.00000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.53998, meta-training acc: 0.88000, meta-val loss: 0.34167, meta-val acc: 0.84000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.21850, meta-training acc: 0.88000, meta-val loss: 0.08337, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.54167, meta-training acc: 0.80000, meta-val loss: 0.23248, meta-val acc: 0.96000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.36155, meta-training acc: 0.92000, meta-val loss: 0.29873, meta-val acc: 0.84000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.25779, meta-training acc: 0.92000, meta-val loss: 0.35961, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.45867, meta-training acc: 0.88000, meta-val loss: 0.19032, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.25714, meta-training acc: 0.84000, meta-val loss: 0.36815, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.86044, meta-training acc: 0.80000, meta-val loss: 0.22501, meta-val acc: 0.92000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.18396, meta-training acc: 0.92000, meta-val loss: 0.35381, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.16645, meta-training acc: 0.92000, meta-val loss: 0.48647, meta-val acc: 0.84000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.27094, meta-training acc: 0.84000, meta-val loss: 0.11498, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.21605, meta-training acc: 0.96000, meta-val loss: 0.15814, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.30299, meta-training acc: 0.88000, meta-val loss: 0.28083, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.70110, meta-training acc: 0.84000, meta-val loss: 0.25009, meta-val acc: 0.88000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.12186, meta-training acc: 0.92000, meta-val loss: 0.42617, meta-val acc: 0.80000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.03201, meta-training acc: 1.00000, meta-val loss: 0.31124, meta-val acc: 0.96000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.25005, meta-training acc: 0.84000, meta-val loss: 0.87131, meta-val acc: 0.72000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.02243, meta-training acc: 1.00000, meta-val loss: 0.04769, meta-val acc: 1.00000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.69733, meta-training acc: 0.64000, meta-val loss: 0.52775, meta-val acc: 0.76000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 1.56317, acc: 0.20000\n",
      "[meta-test episode 100/1000] => loss: 1.58171, acc: 0.36000\n",
      "[meta-test episode 150/1000] => loss: 1.59439, acc: 0.28000\n",
      "[meta-test episode 200/1000] => loss: 1.63451, acc: 0.12000\n",
      "[meta-test episode 250/1000] => loss: 1.60392, acc: 0.28000\n",
      "[meta-test episode 300/1000] => loss: 1.58191, acc: 0.32000\n",
      "[meta-test episode 350/1000] => loss: 1.61375, acc: 0.16000\n",
      "[meta-test episode 400/1000] => loss: 1.57844, acc: 0.28000\n",
      "[meta-test episode 450/1000] => loss: 1.60575, acc: 0.28000\n",
      "[meta-test episode 500/1000] => loss: 1.55488, acc: 0.20000\n",
      "[meta-test episode 550/1000] => loss: 1.55295, acc: 0.56000\n",
      "[meta-test episode 600/1000] => loss: 1.60747, acc: 0.12000\n",
      "[meta-test episode 650/1000] => loss: 1.50211, acc: 0.40000\n",
      "[meta-test episode 700/1000] => loss: 1.62814, acc: 0.08000\n",
      "[meta-test episode 750/1000] => loss: 1.58993, acc: 0.32000\n",
      "[meta-test episode 800/1000] => loss: 1.48139, acc: 0.36000\n",
      "[meta-test episode 850/1000] => loss: 1.52199, acc: 0.40000\n",
      "[meta-test episode 900/1000] => loss: 1.57374, acc: 0.24000\n",
      "[meta-test episode 950/1000] => loss: 1.62882, acc: 0.16000\n",
      "[meta-test episode 1000/1000] => loss: 1.53065, acc: 0.32000\n",
      "Average Meta-Test Accuracy: 0.27852, Meta-Test Accuracy Std: 0.09895\n",
      "\n",
      "========================Performing scale=0.5=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.34293, meta-training acc: 0.44000, meta-val loss: 0.87795, meta-val acc: 0.64000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.49890, meta-training acc: 0.80000, meta-val loss: 0.45990, meta-val acc: 0.80000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.85719, meta-training acc: 0.68000, meta-val loss: 0.55554, meta-val acc: 0.72000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.65629, meta-training acc: 0.68000, meta-val loss: 0.54199, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.55962, meta-training acc: 0.80000, meta-val loss: 0.70413, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 0.84525, meta-training acc: 0.56000, meta-val loss: 0.79060, meta-val acc: 0.76000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.02553, meta-training acc: 0.56000, meta-val loss: 0.81014, meta-val acc: 0.60000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.70020, meta-training acc: 0.80000, meta-val loss: 0.49784, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.63190, meta-training acc: 0.80000, meta-val loss: 0.85530, meta-val acc: 0.60000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.46665, meta-training acc: 0.76000, meta-val loss: 0.41925, meta-val acc: 0.84000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.41139, meta-training acc: 0.88000, meta-val loss: 0.83878, meta-val acc: 0.68000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.72142, meta-training acc: 0.80000, meta-val loss: 0.14976, meta-val acc: 0.96000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 1.02971, meta-training acc: 0.68000, meta-val loss: 1.17009, meta-val acc: 0.52000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.64897, meta-training acc: 0.72000, meta-val loss: 1.18723, meta-val acc: 0.76000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.41915, meta-training acc: 0.80000, meta-val loss: 0.14557, meta-val acc: 1.00000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.41523, meta-training acc: 0.76000, meta-val loss: 0.22362, meta-val acc: 0.92000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.14182, meta-training acc: 0.96000, meta-val loss: 0.51558, meta-val acc: 0.80000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.81258, meta-training acc: 0.68000, meta-val loss: 0.36708, meta-val acc: 0.84000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.17614, meta-training acc: 0.96000, meta-val loss: 0.34754, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.83143, meta-training acc: 0.72000, meta-val loss: 0.24558, meta-val acc: 0.92000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.27782, meta-training acc: 0.92000, meta-val loss: 0.23977, meta-val acc: 0.96000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.28001, meta-training acc: 0.84000, meta-val loss: 0.03967, meta-val acc: 1.00000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.28923, meta-training acc: 0.96000, meta-val loss: 0.11912, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.97487, meta-training acc: 0.64000, meta-val loss: 0.41377, meta-val acc: 0.84000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.55070, meta-training acc: 0.84000, meta-val loss: 0.42102, meta-val acc: 0.84000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.33247, meta-training acc: 0.80000, meta-val loss: 0.28354, meta-val acc: 0.88000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.25526, meta-training acc: 0.88000, meta-val loss: 0.62293, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.55070, meta-training acc: 0.80000, meta-val loss: 0.70227, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.60914, meta-training acc: 0.72000, meta-val loss: 0.37056, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.25759, meta-training acc: 0.92000, meta-val loss: 0.18076, meta-val acc: 1.00000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.40130, meta-training acc: 0.84000, meta-val loss: 0.45452, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.20596, meta-training acc: 0.96000, meta-val loss: 0.24580, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.29962, meta-training acc: 0.92000, meta-val loss: 0.33597, meta-val acc: 0.80000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.41689, meta-training acc: 0.84000, meta-val loss: 0.58272, meta-val acc: 0.76000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.51671, meta-training acc: 0.72000, meta-val loss: 0.29683, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.29225, meta-training acc: 0.92000, meta-val loss: 0.39036, meta-val acc: 0.88000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.05659, meta-training acc: 1.00000, meta-val loss: 0.67185, meta-val acc: 0.80000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.21334, meta-training acc: 0.88000, meta-val loss: 0.50633, meta-val acc: 0.84000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.00693, meta-training acc: 1.00000, meta-val loss: 0.10422, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.43875, meta-training acc: 0.88000, meta-val loss: 0.42625, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 1.50074, acc: 0.28000\n",
      "[meta-test episode 100/1000] => loss: 1.62767, acc: 0.20000\n",
      "[meta-test episode 150/1000] => loss: 1.32919, acc: 0.52000\n",
      "[meta-test episode 200/1000] => loss: 1.56891, acc: 0.36000\n",
      "[meta-test episode 250/1000] => loss: 1.69652, acc: 0.24000\n",
      "[meta-test episode 300/1000] => loss: 1.30330, acc: 0.64000\n",
      "[meta-test episode 350/1000] => loss: 1.22019, acc: 0.44000\n",
      "[meta-test episode 400/1000] => loss: 1.57149, acc: 0.32000\n",
      "[meta-test episode 450/1000] => loss: 1.55222, acc: 0.36000\n",
      "[meta-test episode 500/1000] => loss: 1.78761, acc: 0.32000\n",
      "[meta-test episode 550/1000] => loss: 1.31548, acc: 0.44000\n",
      "[meta-test episode 600/1000] => loss: 1.26662, acc: 0.40000\n",
      "[meta-test episode 650/1000] => loss: 1.69203, acc: 0.36000\n",
      "[meta-test episode 700/1000] => loss: 1.37668, acc: 0.44000\n",
      "[meta-test episode 750/1000] => loss: 1.02027, acc: 0.52000\n",
      "[meta-test episode 800/1000] => loss: 1.41651, acc: 0.48000\n",
      "[meta-test episode 850/1000] => loss: 0.97523, acc: 0.60000\n",
      "[meta-test episode 900/1000] => loss: 0.87733, acc: 0.64000\n",
      "[meta-test episode 950/1000] => loss: 1.59116, acc: 0.40000\n",
      "[meta-test episode 1000/1000] => loss: 1.17671, acc: 0.56000\n",
      "Average Meta-Test Accuracy: 0.43524, Meta-Test Accuracy Std: 0.12955\n",
      "\n",
      "========================Performing scale=0.75=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.37228, meta-training acc: 0.56000, meta-val loss: 0.98253, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.56009, meta-training acc: 0.68000, meta-val loss: 0.53578, meta-val acc: 0.88000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 1.11748, meta-training acc: 0.44000, meta-val loss: 0.59306, meta-val acc: 0.72000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.47926, meta-training acc: 0.80000, meta-val loss: 0.58290, meta-val acc: 0.84000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.55616, meta-training acc: 0.80000, meta-val loss: 0.55196, meta-val acc: 0.88000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 0.96094, meta-training acc: 0.52000, meta-val loss: 0.62016, meta-val acc: 0.84000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.17965, meta-training acc: 0.64000, meta-val loss: 0.47674, meta-val acc: 0.88000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.71121, meta-training acc: 0.84000, meta-val loss: 0.52061, meta-val acc: 0.84000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.61138, meta-training acc: 0.76000, meta-val loss: 0.68171, meta-val acc: 0.68000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.61368, meta-training acc: 0.68000, meta-val loss: 0.47920, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.30243, meta-training acc: 0.84000, meta-val loss: 1.03806, meta-val acc: 0.68000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.57985, meta-training acc: 0.88000, meta-val loss: 0.17344, meta-val acc: 0.96000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.95621, meta-training acc: 0.76000, meta-val loss: 1.07772, meta-val acc: 0.60000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.87113, meta-training acc: 0.76000, meta-val loss: 0.67987, meta-val acc: 0.72000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.24113, meta-training acc: 0.92000, meta-val loss: 0.11056, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.35724, meta-training acc: 0.92000, meta-val loss: 0.16405, meta-val acc: 0.96000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.22950, meta-training acc: 0.92000, meta-val loss: 0.69392, meta-val acc: 0.64000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.59484, meta-training acc: 0.76000, meta-val loss: 0.32195, meta-val acc: 0.84000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.31497, meta-training acc: 0.88000, meta-val loss: 0.27501, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.52465, meta-training acc: 0.84000, meta-val loss: 0.21993, meta-val acc: 0.96000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.14546, meta-training acc: 0.92000, meta-val loss: 0.38051, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.47098, meta-training acc: 0.80000, meta-val loss: 0.31196, meta-val acc: 0.92000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.33950, meta-training acc: 0.92000, meta-val loss: 0.01141, meta-val acc: 1.00000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.72423, meta-training acc: 0.60000, meta-val loss: 0.29092, meta-val acc: 0.88000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.36404, meta-training acc: 0.76000, meta-val loss: 0.42596, meta-val acc: 0.88000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.17626, meta-training acc: 0.92000, meta-val loss: 0.63876, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.27335, meta-training acc: 0.84000, meta-val loss: 0.27733, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.61247, meta-training acc: 0.80000, meta-val loss: 0.92507, meta-val acc: 0.80000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.34916, meta-training acc: 0.88000, meta-val loss: 0.45335, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.16282, meta-training acc: 0.96000, meta-val loss: 0.41988, meta-val acc: 0.84000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.13667, meta-training acc: 1.00000, meta-val loss: 0.46203, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.12350, meta-training acc: 1.00000, meta-val loss: 0.04606, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.21148, meta-training acc: 0.96000, meta-val loss: 0.31842, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.18354, meta-training acc: 0.96000, meta-val loss: 0.21468, meta-val acc: 0.96000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.29697, meta-training acc: 0.84000, meta-val loss: 0.17748, meta-val acc: 0.96000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.29180, meta-training acc: 0.84000, meta-val loss: 0.54953, meta-val acc: 0.68000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.01719, meta-training acc: 1.00000, meta-val loss: 0.51654, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.27389, meta-training acc: 0.96000, meta-val loss: 0.68761, meta-val acc: 0.88000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.08302, meta-training acc: 0.96000, meta-val loss: 0.08006, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.44109, meta-training acc: 0.92000, meta-val loss: 0.52850, meta-val acc: 0.80000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.87132, acc: 0.68000\n",
      "[meta-test episode 100/1000] => loss: 0.57876, acc: 0.88000\n",
      "[meta-test episode 150/1000] => loss: 0.90648, acc: 0.68000\n",
      "[meta-test episode 200/1000] => loss: 1.20992, acc: 0.56000\n",
      "[meta-test episode 250/1000] => loss: 1.29091, acc: 0.28000\n",
      "[meta-test episode 300/1000] => loss: 0.69773, acc: 0.76000\n",
      "[meta-test episode 350/1000] => loss: 0.78288, acc: 0.64000\n",
      "[meta-test episode 400/1000] => loss: 0.47725, acc: 0.96000\n",
      "[meta-test episode 450/1000] => loss: 1.08745, acc: 0.48000\n",
      "[meta-test episode 500/1000] => loss: 0.81469, acc: 0.76000\n",
      "[meta-test episode 550/1000] => loss: 0.82155, acc: 0.68000\n",
      "[meta-test episode 600/1000] => loss: 0.71407, acc: 0.80000\n",
      "[meta-test episode 650/1000] => loss: 0.78941, acc: 0.72000\n",
      "[meta-test episode 700/1000] => loss: 0.72321, acc: 0.68000\n",
      "[meta-test episode 750/1000] => loss: 0.62095, acc: 0.72000\n",
      "[meta-test episode 800/1000] => loss: 0.86203, acc: 0.64000\n",
      "[meta-test episode 850/1000] => loss: 0.44465, acc: 0.80000\n",
      "[meta-test episode 900/1000] => loss: 0.35175, acc: 0.92000\n",
      "[meta-test episode 950/1000] => loss: 1.19709, acc: 0.64000\n",
      "[meta-test episode 1000/1000] => loss: 0.49029, acc: 0.76000\n",
      "Average Meta-Test Accuracy: 0.77736, Meta-Test Accuracy Std: 0.12344\n",
      "\n",
      "========================Performing scale=1.0=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.36501, meta-training acc: 0.36000, meta-val loss: 0.92325, meta-val acc: 0.56000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.65319, meta-training acc: 0.64000, meta-val loss: 0.73765, meta-val acc: 0.76000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 1.11773, meta-training acc: 0.44000, meta-val loss: 0.89430, meta-val acc: 0.68000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.55299, meta-training acc: 0.84000, meta-val loss: 0.44036, meta-val acc: 0.88000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.55071, meta-training acc: 0.80000, meta-val loss: 0.52647, meta-val acc: 0.84000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.23956, meta-training acc: 0.52000, meta-val loss: 0.62425, meta-val acc: 0.72000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.52760, meta-training acc: 0.44000, meta-val loss: 0.64263, meta-val acc: 0.80000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.51801, meta-training acc: 0.80000, meta-val loss: 0.45549, meta-val acc: 0.88000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.37759, meta-training acc: 0.88000, meta-val loss: 0.91428, meta-val acc: 0.60000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.30024, meta-training acc: 0.84000, meta-val loss: 0.61742, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.31666, meta-training acc: 0.88000, meta-val loss: 0.72621, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.28698, meta-training acc: 0.88000, meta-val loss: 0.25896, meta-val acc: 0.96000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.38065, meta-training acc: 0.96000, meta-val loss: 0.89798, meta-val acc: 0.68000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.79229, meta-training acc: 0.64000, meta-val loss: 0.50315, meta-val acc: 0.80000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.18926, meta-training acc: 0.92000, meta-val loss: 0.15290, meta-val acc: 0.92000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.38365, meta-training acc: 0.88000, meta-val loss: 0.13783, meta-val acc: 1.00000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.14868, meta-training acc: 0.92000, meta-val loss: 0.65453, meta-val acc: 0.80000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.56031, meta-training acc: 0.84000, meta-val loss: 0.22082, meta-val acc: 0.92000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.04783, meta-training acc: 1.00000, meta-val loss: 0.12102, meta-val acc: 0.96000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.59054, meta-training acc: 0.84000, meta-val loss: 0.14841, meta-val acc: 0.96000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.18943, meta-training acc: 0.92000, meta-val loss: 0.26783, meta-val acc: 0.96000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.28931, meta-training acc: 0.84000, meta-val loss: 0.21171, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.12757, meta-training acc: 0.96000, meta-val loss: 0.06157, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.51513, meta-training acc: 0.80000, meta-val loss: 0.22843, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.31640, meta-training acc: 0.88000, meta-val loss: 0.38269, meta-val acc: 0.84000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.16097, meta-training acc: 0.96000, meta-val loss: 0.29685, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.10066, meta-training acc: 0.96000, meta-val loss: 0.28192, meta-val acc: 0.84000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.26861, meta-training acc: 0.88000, meta-val loss: 0.85454, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.80439, meta-training acc: 0.60000, meta-val loss: 0.15727, meta-val acc: 0.96000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.16788, meta-training acc: 0.96000, meta-val loss: 0.44185, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.18332, meta-training acc: 0.96000, meta-val loss: 0.30866, meta-val acc: 0.88000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.20348, meta-training acc: 0.96000, meta-val loss: 0.06293, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.16086, meta-training acc: 0.92000, meta-val loss: 0.19604, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.28441, meta-training acc: 0.88000, meta-val loss: 0.40242, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.33841, meta-training acc: 0.92000, meta-val loss: 0.32364, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.28100, meta-training acc: 0.92000, meta-val loss: 0.42532, meta-val acc: 0.80000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.01710, meta-training acc: 1.00000, meta-val loss: 0.32411, meta-val acc: 0.88000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.32826, meta-training acc: 0.84000, meta-val loss: 0.73278, meta-val acc: 0.68000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.01283, meta-training acc: 1.00000, meta-val loss: 0.12811, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.82640, meta-training acc: 0.60000, meta-val loss: 0.31539, meta-val acc: 0.88000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.48288, acc: 0.80000\n",
      "[meta-test episode 100/1000] => loss: 0.09152, acc: 1.00000\n",
      "[meta-test episode 150/1000] => loss: 0.21040, acc: 0.96000\n",
      "[meta-test episode 200/1000] => loss: 0.09945, acc: 1.00000\n",
      "[meta-test episode 250/1000] => loss: 0.83171, acc: 0.68000\n",
      "[meta-test episode 300/1000] => loss: 0.43275, acc: 0.88000\n",
      "[meta-test episode 350/1000] => loss: 0.82470, acc: 0.72000\n",
      "[meta-test episode 400/1000] => loss: 0.11154, acc: 0.96000\n",
      "[meta-test episode 450/1000] => loss: 0.13841, acc: 1.00000\n",
      "[meta-test episode 500/1000] => loss: 0.44534, acc: 0.88000\n",
      "[meta-test episode 550/1000] => loss: 0.35736, acc: 0.88000\n",
      "[meta-test episode 600/1000] => loss: 0.38760, acc: 0.88000\n",
      "[meta-test episode 650/1000] => loss: 0.46329, acc: 0.72000\n",
      "[meta-test episode 700/1000] => loss: 0.47509, acc: 0.88000\n",
      "[meta-test episode 750/1000] => loss: 0.42026, acc: 0.92000\n",
      "[meta-test episode 800/1000] => loss: 0.61566, acc: 0.80000\n",
      "[meta-test episode 850/1000] => loss: 0.19112, acc: 0.92000\n",
      "[meta-test episode 900/1000] => loss: 0.06188, acc: 0.96000\n",
      "[meta-test episode 950/1000] => loss: 0.47984, acc: 0.92000\n",
      "[meta-test episode 1000/1000] => loss: 0.26013, acc: 0.92000\n",
      "Average Meta-Test Accuracy: 0.90260, Meta-Test Accuracy Std: 0.08743\n",
      "\n",
      "========================Performing scale=1.25=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.34008, meta-training acc: 0.48000, meta-val loss: 1.04349, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.53243, meta-training acc: 0.76000, meta-val loss: 0.59556, meta-val acc: 0.80000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 0.97752, meta-training acc: 0.68000, meta-val loss: 0.68264, meta-val acc: 0.68000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.59235, meta-training acc: 0.80000, meta-val loss: 0.60741, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.53373, meta-training acc: 0.80000, meta-val loss: 0.58985, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.01625, meta-training acc: 0.60000, meta-val loss: 0.45712, meta-val acc: 0.84000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.13874, meta-training acc: 0.40000, meta-val loss: 0.68021, meta-val acc: 0.80000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.58048, meta-training acc: 0.80000, meta-val loss: 0.49968, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.50555, meta-training acc: 0.88000, meta-val loss: 1.00692, meta-val acc: 0.56000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.44499, meta-training acc: 0.84000, meta-val loss: 0.45451, meta-val acc: 0.72000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.31830, meta-training acc: 0.96000, meta-val loss: 1.19596, meta-val acc: 0.60000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.56932, meta-training acc: 0.84000, meta-val loss: 0.21268, meta-val acc: 0.96000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.87959, meta-training acc: 0.68000, meta-val loss: 1.32564, meta-val acc: 0.40000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 1.17253, meta-training acc: 0.60000, meta-val loss: 0.56404, meta-val acc: 0.80000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.26146, meta-training acc: 0.92000, meta-val loss: 0.17330, meta-val acc: 0.92000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.48379, meta-training acc: 0.84000, meta-val loss: 0.23560, meta-val acc: 0.96000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.25332, meta-training acc: 0.92000, meta-val loss: 0.47390, meta-val acc: 0.88000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.77224, meta-training acc: 0.72000, meta-val loss: 0.37388, meta-val acc: 0.80000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.22132, meta-training acc: 0.96000, meta-val loss: 0.18139, meta-val acc: 0.96000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.67954, meta-training acc: 0.76000, meta-val loss: 0.14706, meta-val acc: 0.96000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.23637, meta-training acc: 0.84000, meta-val loss: 0.39988, meta-val acc: 0.92000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.60323, meta-training acc: 0.84000, meta-val loss: 0.14582, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.14661, meta-training acc: 0.96000, meta-val loss: 0.01419, meta-val acc: 1.00000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.57677, meta-training acc: 0.80000, meta-val loss: 0.32572, meta-val acc: 0.88000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.17205, meta-training acc: 0.96000, meta-val loss: 0.47329, meta-val acc: 0.84000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.22415, meta-training acc: 0.88000, meta-val loss: 0.28879, meta-val acc: 0.92000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.51021, meta-training acc: 0.80000, meta-val loss: 0.73201, meta-val acc: 0.68000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.41182, meta-training acc: 0.88000, meta-val loss: 0.35090, meta-val acc: 0.96000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.35159, meta-training acc: 0.84000, meta-val loss: 0.37601, meta-val acc: 0.84000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.35316, meta-training acc: 0.92000, meta-val loss: 0.43058, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.13857, meta-training acc: 0.96000, meta-val loss: 0.54842, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.11712, meta-training acc: 1.00000, meta-val loss: 0.18894, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.33938, meta-training acc: 0.84000, meta-val loss: 0.27254, meta-val acc: 0.92000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.30814, meta-training acc: 0.92000, meta-val loss: 0.50849, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.43465, meta-training acc: 0.80000, meta-val loss: 0.21020, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.45240, meta-training acc: 0.76000, meta-val loss: 0.31623, meta-val acc: 0.88000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.07761, meta-training acc: 0.96000, meta-val loss: 0.43005, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.11974, meta-training acc: 0.96000, meta-val loss: 0.61097, meta-val acc: 0.84000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.00347, meta-training acc: 1.00000, meta-val loss: 0.06141, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.51042, meta-training acc: 0.80000, meta-val loss: 0.33495, meta-val acc: 0.92000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.68619, acc: 0.56000\n",
      "[meta-test episode 100/1000] => loss: 0.32278, acc: 0.92000\n",
      "[meta-test episode 150/1000] => loss: 0.62457, acc: 0.72000\n",
      "[meta-test episode 200/1000] => loss: 0.44325, acc: 0.88000\n",
      "[meta-test episode 250/1000] => loss: 1.45905, acc: 0.68000\n",
      "[meta-test episode 300/1000] => loss: 0.50546, acc: 0.84000\n",
      "[meta-test episode 350/1000] => loss: 0.88042, acc: 0.60000\n",
      "[meta-test episode 400/1000] => loss: 0.40349, acc: 0.88000\n",
      "[meta-test episode 450/1000] => loss: 0.91305, acc: 0.68000\n",
      "[meta-test episode 500/1000] => loss: 0.66395, acc: 0.60000\n",
      "[meta-test episode 550/1000] => loss: 0.55758, acc: 0.68000\n",
      "[meta-test episode 600/1000] => loss: 0.57093, acc: 0.80000\n",
      "[meta-test episode 650/1000] => loss: 0.74451, acc: 0.68000\n",
      "[meta-test episode 700/1000] => loss: 1.60618, acc: 0.52000\n",
      "[meta-test episode 750/1000] => loss: 0.28748, acc: 0.92000\n",
      "[meta-test episode 800/1000] => loss: 1.04097, acc: 0.68000\n",
      "[meta-test episode 850/1000] => loss: 0.26762, acc: 0.92000\n",
      "[meta-test episode 900/1000] => loss: 0.40000, acc: 0.84000\n",
      "[meta-test episode 950/1000] => loss: 1.20946, acc: 0.56000\n",
      "[meta-test episode 1000/1000] => loss: 0.46559, acc: 0.84000\n",
      "Average Meta-Test Accuracy: 0.80860, Meta-Test Accuracy Std: 0.11598\n",
      "\n",
      "========================Performing scale=1.5=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.34128, meta-training acc: 0.48000, meta-val loss: 1.06017, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.53562, meta-training acc: 0.80000, meta-val loss: 0.56356, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 1.03532, meta-training acc: 0.48000, meta-val loss: 0.58376, meta-val acc: 0.72000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.61755, meta-training acc: 0.80000, meta-val loss: 0.54699, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.55649, meta-training acc: 0.80000, meta-val loss: 0.66947, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 0.96575, meta-training acc: 0.60000, meta-val loss: 0.50966, meta-val acc: 0.80000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.16858, meta-training acc: 0.44000, meta-val loss: 0.59299, meta-val acc: 0.80000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.72997, meta-training acc: 0.64000, meta-val loss: 0.49675, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.52416, meta-training acc: 0.80000, meta-val loss: 0.91553, meta-val acc: 0.60000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.58428, meta-training acc: 0.64000, meta-val loss: 0.55572, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.43108, meta-training acc: 0.88000, meta-val loss: 1.09251, meta-val acc: 0.64000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.79395, meta-training acc: 0.88000, meta-val loss: 0.09709, meta-val acc: 1.00000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 1.01026, meta-training acc: 0.48000, meta-val loss: 1.39748, meta-val acc: 0.36000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.82027, meta-training acc: 0.72000, meta-val loss: 0.59829, meta-val acc: 0.76000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.55195, meta-training acc: 0.76000, meta-val loss: 0.16370, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.40870, meta-training acc: 0.84000, meta-val loss: 0.29077, meta-val acc: 0.96000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.21554, meta-training acc: 0.92000, meta-val loss: 0.98512, meta-val acc: 0.76000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.88906, meta-training acc: 0.68000, meta-val loss: 0.37637, meta-val acc: 0.80000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.12813, meta-training acc: 0.96000, meta-val loss: 0.18822, meta-val acc: 0.96000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.58761, meta-training acc: 0.80000, meta-val loss: 0.23102, meta-val acc: 0.92000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.22219, meta-training acc: 0.96000, meta-val loss: 0.63563, meta-val acc: 0.84000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.71063, meta-training acc: 0.76000, meta-val loss: 0.15465, meta-val acc: 0.92000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.09672, meta-training acc: 1.00000, meta-val loss: 0.09561, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.51441, meta-training acc: 0.80000, meta-val loss: 0.23480, meta-val acc: 1.00000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.27685, meta-training acc: 0.92000, meta-val loss: 0.47207, meta-val acc: 0.80000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.24846, meta-training acc: 0.92000, meta-val loss: 0.45205, meta-val acc: 0.84000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.09800, meta-training acc: 0.96000, meta-val loss: 0.33320, meta-val acc: 0.84000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.32419, meta-training acc: 0.84000, meta-val loss: 0.47652, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.63933, meta-training acc: 0.60000, meta-val loss: 0.22874, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.19426, meta-training acc: 0.92000, meta-val loss: 0.18145, meta-val acc: 1.00000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.11597, meta-training acc: 1.00000, meta-val loss: 0.51044, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.15720, meta-training acc: 0.96000, meta-val loss: 0.17146, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.28212, meta-training acc: 0.92000, meta-val loss: 0.30601, meta-val acc: 0.88000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.35316, meta-training acc: 0.88000, meta-val loss: 0.28520, meta-val acc: 0.88000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.26759, meta-training acc: 0.84000, meta-val loss: 0.18997, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.36324, meta-training acc: 0.80000, meta-val loss: 0.19541, meta-val acc: 0.88000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.05497, meta-training acc: 1.00000, meta-val loss: 0.35171, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.28220, meta-training acc: 0.88000, meta-val loss: 0.58992, meta-val acc: 0.80000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.00319, meta-training acc: 1.00000, meta-val loss: 0.03561, meta-val acc: 1.00000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.54070, meta-training acc: 0.84000, meta-val loss: 0.56370, meta-val acc: 0.80000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.93099, acc: 0.60000\n",
      "[meta-test episode 100/1000] => loss: 0.86031, acc: 0.68000\n",
      "[meta-test episode 150/1000] => loss: 1.17279, acc: 0.40000\n",
      "[meta-test episode 200/1000] => loss: 1.07397, acc: 0.76000\n",
      "[meta-test episode 250/1000] => loss: 1.87197, acc: 0.36000\n",
      "[meta-test episode 300/1000] => loss: 0.66025, acc: 0.68000\n",
      "[meta-test episode 350/1000] => loss: 1.93356, acc: 0.56000\n",
      "[meta-test episode 400/1000] => loss: 0.82589, acc: 0.68000\n",
      "[meta-test episode 450/1000] => loss: 1.61478, acc: 0.60000\n",
      "[meta-test episode 500/1000] => loss: 1.17287, acc: 0.44000\n",
      "[meta-test episode 550/1000] => loss: 1.11802, acc: 0.68000\n",
      "[meta-test episode 600/1000] => loss: 0.44346, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 1.49065, acc: 0.48000\n",
      "[meta-test episode 700/1000] => loss: 2.21216, acc: 0.60000\n",
      "[meta-test episode 750/1000] => loss: 0.19382, acc: 0.96000\n",
      "[meta-test episode 800/1000] => loss: 0.59143, acc: 0.76000\n",
      "[meta-test episode 850/1000] => loss: 0.39428, acc: 0.88000\n",
      "[meta-test episode 900/1000] => loss: 0.54543, acc: 0.68000\n",
      "[meta-test episode 950/1000] => loss: 1.28568, acc: 0.48000\n",
      "[meta-test episode 1000/1000] => loss: 0.98282, acc: 0.60000\n",
      "Average Meta-Test Accuracy: 0.68588, Meta-Test Accuracy Std: 0.13768\n",
      "\n",
      "========================Performing scale=1.75=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.34789, meta-training acc: 0.40000, meta-val loss: 0.99826, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.68057, meta-training acc: 0.68000, meta-val loss: 0.60804, meta-val acc: 0.84000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 1.01585, meta-training acc: 0.56000, meta-val loss: 0.51704, meta-val acc: 0.72000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.64908, meta-training acc: 0.68000, meta-val loss: 0.61949, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.47284, meta-training acc: 0.80000, meta-val loss: 0.60132, meta-val acc: 0.84000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 1.15604, meta-training acc: 0.40000, meta-val loss: 0.58503, meta-val acc: 0.88000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.32987, meta-training acc: 0.52000, meta-val loss: 0.65047, meta-val acc: 0.76000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.55536, meta-training acc: 0.92000, meta-val loss: 0.40474, meta-val acc: 0.92000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.45566, meta-training acc: 0.92000, meta-val loss: 0.90546, meta-val acc: 0.64000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.71952, meta-training acc: 0.56000, meta-val loss: 0.57500, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.41586, meta-training acc: 0.92000, meta-val loss: 1.06783, meta-val acc: 0.72000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.76610, meta-training acc: 0.76000, meta-val loss: 0.15803, meta-val acc: 0.96000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 1.19531, meta-training acc: 0.44000, meta-val loss: 1.35105, meta-val acc: 0.40000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 0.94461, meta-training acc: 0.60000, meta-val loss: 0.57598, meta-val acc: 0.76000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.38618, meta-training acc: 0.84000, meta-val loss: 0.12252, meta-val acc: 1.00000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.40259, meta-training acc: 0.92000, meta-val loss: 0.25916, meta-val acc: 0.96000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.23777, meta-training acc: 0.92000, meta-val loss: 0.79041, meta-val acc: 0.76000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.86320, meta-training acc: 0.60000, meta-val loss: 0.37937, meta-val acc: 0.96000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.13816, meta-training acc: 0.96000, meta-val loss: 0.30697, meta-val acc: 0.88000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.52513, meta-training acc: 0.80000, meta-val loss: 0.13283, meta-val acc: 0.96000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.30437, meta-training acc: 0.84000, meta-val loss: 0.40517, meta-val acc: 0.80000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.52999, meta-training acc: 0.80000, meta-val loss: 0.18519, meta-val acc: 0.96000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.26047, meta-training acc: 0.88000, meta-val loss: 0.16126, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.67809, meta-training acc: 0.84000, meta-val loss: 0.60345, meta-val acc: 0.76000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.14526, meta-training acc: 1.00000, meta-val loss: 0.36726, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.30193, meta-training acc: 0.88000, meta-val loss: 0.49906, meta-val acc: 0.88000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.24039, meta-training acc: 0.96000, meta-val loss: 0.55518, meta-val acc: 0.76000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.40012, meta-training acc: 0.84000, meta-val loss: 0.75818, meta-val acc: 0.76000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 0.41949, meta-training acc: 0.76000, meta-val loss: 0.39810, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.20034, meta-training acc: 0.96000, meta-val loss: 0.56451, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.25190, meta-training acc: 0.92000, meta-val loss: 0.75713, meta-val acc: 0.68000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.14940, meta-training acc: 0.96000, meta-val loss: 0.13342, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.36069, meta-training acc: 0.80000, meta-val loss: 0.21531, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.26534, meta-training acc: 0.88000, meta-val loss: 0.55433, meta-val acc: 0.80000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.23538, meta-training acc: 0.92000, meta-val loss: 0.26879, meta-val acc: 0.92000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.14970, meta-training acc: 0.92000, meta-val loss: 0.15942, meta-val acc: 0.96000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.04400, meta-training acc: 1.00000, meta-val loss: 0.38027, meta-val acc: 0.92000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.11695, meta-training acc: 1.00000, meta-val loss: 0.46931, meta-val acc: 0.88000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.00840, meta-training acc: 1.00000, meta-val loss: 0.12057, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.28271, meta-training acc: 0.92000, meta-val loss: 0.53891, meta-val acc: 0.76000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.56545, acc: 0.72000\n",
      "[meta-test episode 100/1000] => loss: 0.60120, acc: 0.80000\n",
      "[meta-test episode 150/1000] => loss: 1.23765, acc: 0.56000\n",
      "[meta-test episode 200/1000] => loss: 1.21653, acc: 0.68000\n",
      "[meta-test episode 250/1000] => loss: 2.38154, acc: 0.24000\n",
      "[meta-test episode 300/1000] => loss: 1.19654, acc: 0.72000\n",
      "[meta-test episode 350/1000] => loss: 1.90884, acc: 0.60000\n",
      "[meta-test episode 400/1000] => loss: 0.78148, acc: 0.68000\n",
      "[meta-test episode 450/1000] => loss: 1.63550, acc: 0.60000\n",
      "[meta-test episode 500/1000] => loss: 1.84706, acc: 0.36000\n",
      "[meta-test episode 550/1000] => loss: 1.12968, acc: 0.60000\n",
      "[meta-test episode 600/1000] => loss: 0.74892, acc: 0.76000\n",
      "[meta-test episode 650/1000] => loss: 1.70031, acc: 0.56000\n",
      "[meta-test episode 700/1000] => loss: 2.02027, acc: 0.60000\n",
      "[meta-test episode 750/1000] => loss: 0.32710, acc: 0.88000\n",
      "[meta-test episode 800/1000] => loss: 0.60709, acc: 0.68000\n",
      "[meta-test episode 850/1000] => loss: 0.64299, acc: 0.76000\n",
      "[meta-test episode 900/1000] => loss: 0.70895, acc: 0.68000\n",
      "[meta-test episode 950/1000] => loss: 1.59489, acc: 0.48000\n",
      "[meta-test episode 1000/1000] => loss: 0.74517, acc: 0.76000\n",
      "Average Meta-Test Accuracy: 0.61984, Meta-Test Accuracy Std: 0.13670\n",
      "\n",
      "========================Performing scale=2.0=========================\n",
      "\n",
      "[epo 1/20, epi 50/100] => meta-training loss: 1.47095, meta-training acc: 0.44000, meta-val loss: 1.16210, meta-val acc: 0.60000\n",
      "[epo 1/20, epi 100/100] => meta-training loss: 0.59679, meta-training acc: 0.76000, meta-val loss: 0.69139, meta-val acc: 0.80000\n",
      "[epo 2/20, epi 50/100] => meta-training loss: 1.16835, meta-training acc: 0.52000, meta-val loss: 0.58620, meta-val acc: 0.72000\n",
      "[epo 2/20, epi 100/100] => meta-training loss: 0.65939, meta-training acc: 0.64000, meta-val loss: 0.34601, meta-val acc: 0.92000\n",
      "[epo 3/20, epi 50/100] => meta-training loss: 0.48710, meta-training acc: 0.80000, meta-val loss: 0.61856, meta-val acc: 0.80000\n",
      "[epo 3/20, epi 100/100] => meta-training loss: 0.94706, meta-training acc: 0.44000, meta-val loss: 0.46361, meta-val acc: 0.80000\n",
      "[epo 4/20, epi 50/100] => meta-training loss: 1.27813, meta-training acc: 0.44000, meta-val loss: 0.51463, meta-val acc: 0.84000\n",
      "[epo 4/20, epi 100/100] => meta-training loss: 0.58608, meta-training acc: 0.84000, meta-val loss: 0.39008, meta-val acc: 0.80000\n",
      "[epo 5/20, epi 50/100] => meta-training loss: 0.54068, meta-training acc: 0.76000, meta-val loss: 0.68710, meta-val acc: 0.76000\n",
      "[epo 5/20, epi 100/100] => meta-training loss: 0.60204, meta-training acc: 0.72000, meta-val loss: 0.67299, meta-val acc: 0.72000\n",
      "[epo 6/20, epi 50/100] => meta-training loss: 0.37762, meta-training acc: 0.92000, meta-val loss: 0.94841, meta-val acc: 0.76000\n",
      "[epo 6/20, epi 100/100] => meta-training loss: 0.58243, meta-training acc: 0.84000, meta-val loss: 0.08018, meta-val acc: 0.96000\n",
      "[epo 7/20, epi 50/100] => meta-training loss: 0.90698, meta-training acc: 0.76000, meta-val loss: 1.11169, meta-val acc: 0.44000\n",
      "[epo 7/20, epi 100/100] => meta-training loss: 1.02043, meta-training acc: 0.64000, meta-val loss: 0.40129, meta-val acc: 0.80000\n",
      "[epo 8/20, epi 50/100] => meta-training loss: 0.40975, meta-training acc: 0.84000, meta-val loss: 0.10872, meta-val acc: 0.96000\n",
      "[epo 8/20, epi 100/100] => meta-training loss: 0.36920, meta-training acc: 0.88000, meta-val loss: 0.12516, meta-val acc: 1.00000\n",
      "[epo 9/20, epi 50/100] => meta-training loss: 0.18483, meta-training acc: 1.00000, meta-val loss: 0.67390, meta-val acc: 0.72000\n",
      "[epo 9/20, epi 100/100] => meta-training loss: 0.69334, meta-training acc: 0.72000, meta-val loss: 0.19806, meta-val acc: 0.96000\n",
      "[epo 10/20, epi 50/100] => meta-training loss: 0.15815, meta-training acc: 0.88000, meta-val loss: 0.08347, meta-val acc: 1.00000\n",
      "[epo 10/20, epi 100/100] => meta-training loss: 0.60508, meta-training acc: 0.76000, meta-val loss: 0.09763, meta-val acc: 1.00000\n",
      "[epo 11/20, epi 50/100] => meta-training loss: 0.24073, meta-training acc: 0.84000, meta-val loss: 0.30616, meta-val acc: 0.88000\n",
      "[epo 11/20, epi 100/100] => meta-training loss: 0.31836, meta-training acc: 0.88000, meta-val loss: 0.25173, meta-val acc: 0.88000\n",
      "[epo 12/20, epi 50/100] => meta-training loss: 0.34404, meta-training acc: 0.88000, meta-val loss: 0.07817, meta-val acc: 0.92000\n",
      "[epo 12/20, epi 100/100] => meta-training loss: 0.79166, meta-training acc: 0.68000, meta-val loss: 0.40950, meta-val acc: 0.88000\n",
      "[epo 13/20, epi 50/100] => meta-training loss: 0.26856, meta-training acc: 0.96000, meta-val loss: 0.35027, meta-val acc: 0.92000\n",
      "[epo 13/20, epi 100/100] => meta-training loss: 0.08807, meta-training acc: 0.96000, meta-val loss: 0.70879, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 50/100] => meta-training loss: 0.18892, meta-training acc: 0.92000, meta-val loss: 0.53112, meta-val acc: 0.80000\n",
      "[epo 14/20, epi 100/100] => meta-training loss: 0.65267, meta-training acc: 0.80000, meta-val loss: 0.62616, meta-val acc: 0.76000\n",
      "[epo 15/20, epi 50/100] => meta-training loss: 1.15649, meta-training acc: 0.60000, meta-val loss: 0.23473, meta-val acc: 0.88000\n",
      "[epo 15/20, epi 100/100] => meta-training loss: 0.13867, meta-training acc: 0.96000, meta-val loss: 0.37385, meta-val acc: 0.92000\n",
      "[epo 16/20, epi 50/100] => meta-training loss: 0.42729, meta-training acc: 0.88000, meta-val loss: 0.53135, meta-val acc: 0.80000\n",
      "[epo 16/20, epi 100/100] => meta-training loss: 0.11225, meta-training acc: 1.00000, meta-val loss: 0.10483, meta-val acc: 1.00000\n",
      "[epo 17/20, epi 50/100] => meta-training loss: 0.15660, meta-training acc: 0.96000, meta-val loss: 0.14712, meta-val acc: 0.96000\n",
      "[epo 17/20, epi 100/100] => meta-training loss: 0.39254, meta-training acc: 0.76000, meta-val loss: 0.36460, meta-val acc: 0.84000\n",
      "[epo 18/20, epi 50/100] => meta-training loss: 0.64073, meta-training acc: 0.80000, meta-val loss: 0.15462, meta-val acc: 0.96000\n",
      "[epo 18/20, epi 100/100] => meta-training loss: 0.24304, meta-training acc: 0.92000, meta-val loss: 0.14912, meta-val acc: 0.96000\n",
      "[epo 19/20, epi 50/100] => meta-training loss: 0.09521, meta-training acc: 1.00000, meta-val loss: 0.41208, meta-val acc: 0.88000\n",
      "[epo 19/20, epi 100/100] => meta-training loss: 0.31740, meta-training acc: 0.84000, meta-val loss: 0.60043, meta-val acc: 0.88000\n",
      "[epo 20/20, epi 50/100] => meta-training loss: 0.02098, meta-training acc: 1.00000, meta-val loss: 0.09124, meta-val acc: 0.96000\n",
      "[epo 20/20, epi 100/100] => meta-training loss: 0.50869, meta-training acc: 0.72000, meta-val loss: 0.32161, meta-val acc: 0.84000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.89685, acc: 0.56000\n",
      "[meta-test episode 100/1000] => loss: 0.96333, acc: 0.56000\n",
      "[meta-test episode 150/1000] => loss: 1.86148, acc: 0.56000\n",
      "[meta-test episode 200/1000] => loss: 1.26719, acc: 0.72000\n",
      "[meta-test episode 250/1000] => loss: 3.69095, acc: 0.12000\n",
      "[meta-test episode 300/1000] => loss: 1.35638, acc: 0.52000\n",
      "[meta-test episode 350/1000] => loss: 1.82460, acc: 0.48000\n",
      "[meta-test episode 400/1000] => loss: 0.79853, acc: 0.72000\n",
      "[meta-test episode 450/1000] => loss: 2.36857, acc: 0.52000\n",
      "[meta-test episode 500/1000] => loss: 2.54281, acc: 0.36000\n",
      "[meta-test episode 550/1000] => loss: 2.81748, acc: 0.36000\n",
      "[meta-test episode 600/1000] => loss: 1.00878, acc: 0.68000\n",
      "[meta-test episode 650/1000] => loss: 1.86173, acc: 0.52000\n",
      "[meta-test episode 700/1000] => loss: 1.94342, acc: 0.64000\n",
      "[meta-test episode 750/1000] => loss: 0.72174, acc: 0.72000\n",
      "[meta-test episode 800/1000] => loss: 0.82412, acc: 0.72000\n",
      "[meta-test episode 850/1000] => loss: 0.78804, acc: 0.56000\n",
      "[meta-test episode 900/1000] => loss: 1.30955, acc: 0.72000\n",
      "[meta-test episode 950/1000] => loss: 1.51549, acc: 0.36000\n",
      "[meta-test episode 1000/1000] => loss: 1.77507, acc: 0.36000\n",
      "Average Meta-Test Accuracy: 0.55500, Meta-Test Accuracy Std: 0.13758\n"
     ]
    }
   ],
   "source": [
    "for p in range(25,201,25):\n",
    "    p = p/100\n",
    "    print(f'\\n========================Performing scale={p}=========================\\n')\n",
    "    run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,shear=None,scale=p,latent_dim = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================Performing scale=1.0=========================\n",
      "\n",
      "[epo 1/50, epi 50/100] => meta-training loss: 0.16025, meta-training acc: 0.20000, meta-val loss: 0.16035, meta-val acc: 0.20000\n",
      "[epo 1/50, epi 100/100] => meta-training loss: 0.15992, meta-training acc: 0.20000, meta-val loss: 0.16000, meta-val acc: 0.20000\n",
      "[epo 2/50, epi 50/100] => meta-training loss: 0.15985, meta-training acc: 0.20000, meta-val loss: 0.15979, meta-val acc: 0.20000\n",
      "[epo 2/50, epi 100/100] => meta-training loss: 0.15984, meta-training acc: 0.20000, meta-val loss: 0.15946, meta-val acc: 0.20000\n",
      "[epo 3/50, epi 50/100] => meta-training loss: 0.15277, meta-training acc: 0.56000, meta-val loss: 0.15348, meta-val acc: 0.56000\n",
      "[epo 3/50, epi 100/100] => meta-training loss: 0.14445, meta-training acc: 0.44000, meta-val loss: 0.14640, meta-val acc: 0.44000\n",
      "[epo 4/50, epi 50/100] => meta-training loss: 0.15471, meta-training acc: 0.40000, meta-val loss: 0.15154, meta-val acc: 0.44000\n",
      "[epo 4/50, epi 100/100] => meta-training loss: 0.13697, meta-training acc: 0.44000, meta-val loss: 0.12665, meta-val acc: 0.60000\n",
      "[epo 5/50, epi 50/100] => meta-training loss: 0.12202, meta-training acc: 0.60000, meta-val loss: 0.16203, meta-val acc: 0.32000\n",
      "[epo 5/50, epi 100/100] => meta-training loss: 0.17272, meta-training acc: 0.20000, meta-val loss: 0.11756, meta-val acc: 0.56000\n",
      "[epo 6/50, epi 50/100] => meta-training loss: 0.13713, meta-training acc: 0.52000, meta-val loss: 0.14000, meta-val acc: 0.48000\n",
      "[epo 6/50, epi 100/100] => meta-training loss: 0.11493, meta-training acc: 0.48000, meta-val loss: 0.10723, meta-val acc: 0.84000\n",
      "[epo 7/50, epi 50/100] => meta-training loss: 0.17281, meta-training acc: 0.12000, meta-val loss: 0.15767, meta-val acc: 0.36000\n",
      "[epo 7/50, epi 100/100] => meta-training loss: 0.13903, meta-training acc: 0.40000, meta-val loss: 0.13765, meta-val acc: 0.44000\n",
      "[epo 8/50, epi 50/100] => meta-training loss: 0.13321, meta-training acc: 0.44000, meta-val loss: 0.11219, meta-val acc: 0.52000\n",
      "[epo 8/50, epi 100/100] => meta-training loss: 0.11533, meta-training acc: 0.56000, meta-val loss: 0.16853, meta-val acc: 0.32000\n",
      "[epo 9/50, epi 50/100] => meta-training loss: 0.10908, meta-training acc: 0.76000, meta-val loss: 0.09917, meta-val acc: 0.60000\n",
      "[epo 9/50, epi 100/100] => meta-training loss: 0.14673, meta-training acc: 0.48000, meta-val loss: 0.13559, meta-val acc: 0.44000\n",
      "[epo 10/50, epi 50/100] => meta-training loss: 0.09446, meta-training acc: 0.72000, meta-val loss: 0.14890, meta-val acc: 0.28000\n",
      "[epo 10/50, epi 100/100] => meta-training loss: 0.12994, meta-training acc: 0.68000, meta-val loss: 0.21236, meta-val acc: 0.60000\n",
      "[epo 11/50, epi 50/100] => meta-training loss: 0.12043, meta-training acc: 0.48000, meta-val loss: 0.17132, meta-val acc: 0.28000\n",
      "[epo 11/50, epi 100/100] => meta-training loss: 0.15503, meta-training acc: 0.32000, meta-val loss: 0.14853, meta-val acc: 0.36000\n",
      "[epo 12/50, epi 50/100] => meta-training loss: 0.12588, meta-training acc: 0.48000, meta-val loss: 0.12945, meta-val acc: 0.44000\n",
      "[epo 12/50, epi 100/100] => meta-training loss: 0.12451, meta-training acc: 0.40000, meta-val loss: 0.13960, meta-val acc: 0.56000\n",
      "[epo 13/50, epi 50/100] => meta-training loss: 0.15799, meta-training acc: 0.28000, meta-val loss: 0.11844, meta-val acc: 0.52000\n",
      "[epo 13/50, epi 100/100] => meta-training loss: 0.09355, meta-training acc: 0.72000, meta-val loss: 0.09098, meta-val acc: 0.72000\n",
      "[epo 14/50, epi 50/100] => meta-training loss: 0.12665, meta-training acc: 0.56000, meta-val loss: 0.14115, meta-val acc: 0.60000\n",
      "[epo 14/50, epi 100/100] => meta-training loss: 0.12790, meta-training acc: 0.44000, meta-val loss: 0.12658, meta-val acc: 0.48000\n",
      "[epo 15/50, epi 50/100] => meta-training loss: 0.09249, meta-training acc: 0.68000, meta-val loss: 0.11972, meta-val acc: 0.60000\n",
      "[epo 15/50, epi 100/100] => meta-training loss: 0.12554, meta-training acc: 0.56000, meta-val loss: 0.12947, meta-val acc: 0.56000\n",
      "[epo 16/50, epi 50/100] => meta-training loss: 0.11464, meta-training acc: 0.64000, meta-val loss: 0.11988, meta-val acc: 0.60000\n",
      "[epo 16/50, epi 100/100] => meta-training loss: 0.14277, meta-training acc: 0.24000, meta-val loss: 0.07462, meta-val acc: 0.84000\n",
      "[epo 17/50, epi 50/100] => meta-training loss: 0.11290, meta-training acc: 0.64000, meta-val loss: 0.12524, meta-val acc: 0.72000\n",
      "[epo 17/50, epi 100/100] => meta-training loss: 0.15039, meta-training acc: 0.44000, meta-val loss: 0.14018, meta-val acc: 0.44000\n",
      "[epo 18/50, epi 50/100] => meta-training loss: 0.13549, meta-training acc: 0.52000, meta-val loss: 0.09240, meta-val acc: 0.80000\n",
      "[epo 18/50, epi 100/100] => meta-training loss: 0.11166, meta-training acc: 0.52000, meta-val loss: 0.11202, meta-val acc: 0.60000\n",
      "[epo 19/50, epi 50/100] => meta-training loss: 0.08383, meta-training acc: 0.80000, meta-val loss: 0.10334, meta-val acc: 0.64000\n",
      "[epo 19/50, epi 100/100] => meta-training loss: 0.12082, meta-training acc: 0.72000, meta-val loss: 0.17634, meta-val acc: 0.28000\n",
      "[epo 20/50, epi 50/100] => meta-training loss: 0.08481, meta-training acc: 0.84000, meta-val loss: 0.08358, meta-val acc: 0.72000\n",
      "[epo 20/50, epi 100/100] => meta-training loss: 0.08213, meta-training acc: 0.80000, meta-val loss: 0.10901, meta-val acc: 0.68000\n",
      "[epo 21/50, epi 50/100] => meta-training loss: 0.11141, meta-training acc: 0.60000, meta-val loss: 0.09394, meta-val acc: 0.84000\n",
      "[epo 21/50, epi 100/100] => meta-training loss: 0.11085, meta-training acc: 0.64000, meta-val loss: 0.12098, meta-val acc: 0.56000\n",
      "[epo 22/50, epi 50/100] => meta-training loss: 0.11720, meta-training acc: 0.52000, meta-val loss: 0.08789, meta-val acc: 0.80000\n",
      "[epo 22/50, epi 100/100] => meta-training loss: 0.10943, meta-training acc: 0.52000, meta-val loss: 0.10991, meta-val acc: 0.76000\n",
      "[epo 23/50, epi 50/100] => meta-training loss: 0.13633, meta-training acc: 0.48000, meta-val loss: 0.10813, meta-val acc: 0.72000\n",
      "[epo 23/50, epi 100/100] => meta-training loss: 0.09975, meta-training acc: 0.72000, meta-val loss: 0.12682, meta-val acc: 0.56000\n",
      "[epo 24/50, epi 50/100] => meta-training loss: 0.15610, meta-training acc: 0.56000, meta-val loss: 0.11946, meta-val acc: 0.68000\n",
      "[epo 24/50, epi 100/100] => meta-training loss: 0.11549, meta-training acc: 0.64000, meta-val loss: 0.11680, meta-val acc: 0.48000\n",
      "[epo 25/50, epi 50/100] => meta-training loss: 0.10518, meta-training acc: 0.68000, meta-val loss: 0.07694, meta-val acc: 0.76000\n",
      "[epo 25/50, epi 100/100] => meta-training loss: 0.10288, meta-training acc: 0.72000, meta-val loss: 0.11931, meta-val acc: 0.60000\n",
      "[epo 26/50, epi 50/100] => meta-training loss: 0.11917, meta-training acc: 0.60000, meta-val loss: 0.14473, meta-val acc: 0.44000\n",
      "[epo 26/50, epi 100/100] => meta-training loss: 0.10760, meta-training acc: 0.60000, meta-val loss: 0.10767, meta-val acc: 0.64000\n",
      "[epo 27/50, epi 50/100] => meta-training loss: 0.09147, meta-training acc: 0.80000, meta-val loss: 0.08226, meta-val acc: 0.84000\n",
      "[epo 27/50, epi 100/100] => meta-training loss: 0.11368, meta-training acc: 0.56000, meta-val loss: 0.11224, meta-val acc: 0.64000\n",
      "[epo 28/50, epi 50/100] => meta-training loss: 0.10860, meta-training acc: 0.68000, meta-val loss: 0.10454, meta-val acc: 0.64000\n",
      "[epo 28/50, epi 100/100] => meta-training loss: 0.15120, meta-training acc: 0.68000, meta-val loss: 0.07635, meta-val acc: 0.88000\n",
      "[epo 29/50, epi 50/100] => meta-training loss: 0.13492, meta-training acc: 0.52000, meta-val loss: 0.11118, meta-val acc: 0.64000\n",
      "[epo 29/50, epi 100/100] => meta-training loss: 0.11039, meta-training acc: 0.52000, meta-val loss: 0.15220, meta-val acc: 0.44000\n",
      "[epo 30/50, epi 50/100] => meta-training loss: 0.11127, meta-training acc: 0.60000, meta-val loss: 0.11677, meta-val acc: 0.64000\n",
      "[epo 30/50, epi 100/100] => meta-training loss: 0.07960, meta-training acc: 0.76000, meta-val loss: 0.08417, meta-val acc: 0.68000\n",
      "[epo 31/50, epi 50/100] => meta-training loss: 0.09463, meta-training acc: 0.80000, meta-val loss: 0.10432, meta-val acc: 0.64000\n",
      "[epo 31/50, epi 100/100] => meta-training loss: 0.08294, meta-training acc: 0.84000, meta-val loss: 0.11273, meta-val acc: 0.56000\n",
      "[epo 32/50, epi 50/100] => meta-training loss: 0.07632, meta-training acc: 0.84000, meta-val loss: 0.08716, meta-val acc: 0.84000\n",
      "[epo 32/50, epi 100/100] => meta-training loss: 0.09391, meta-training acc: 0.68000, meta-val loss: 0.07600, meta-val acc: 0.88000\n",
      "[epo 33/50, epi 50/100] => meta-training loss: 0.07631, meta-training acc: 0.76000, meta-val loss: 0.10674, meta-val acc: 0.64000\n",
      "[epo 33/50, epi 100/100] => meta-training loss: 0.14488, meta-training acc: 0.40000, meta-val loss: 0.07439, meta-val acc: 0.80000\n",
      "[epo 34/50, epi 50/100] => meta-training loss: 0.10858, meta-training acc: 0.72000, meta-val loss: 0.08234, meta-val acc: 0.68000\n",
      "[epo 34/50, epi 100/100] => meta-training loss: 0.09649, meta-training acc: 0.72000, meta-val loss: 0.10542, meta-val acc: 0.84000\n",
      "[epo 35/50, epi 50/100] => meta-training loss: 0.08809, meta-training acc: 0.68000, meta-val loss: 0.10606, meta-val acc: 0.64000\n",
      "[epo 35/50, epi 100/100] => meta-training loss: 0.07902, meta-training acc: 0.80000, meta-val loss: 0.12099, meta-val acc: 0.60000\n",
      "[epo 36/50, epi 50/100] => meta-training loss: 0.07642, meta-training acc: 0.88000, meta-val loss: 0.05499, meta-val acc: 0.84000\n",
      "[epo 36/50, epi 100/100] => meta-training loss: 0.09915, meta-training acc: 0.72000, meta-val loss: 0.08216, meta-val acc: 0.68000\n",
      "[epo 37/50, epi 50/100] => meta-training loss: 0.14348, meta-training acc: 0.52000, meta-val loss: 0.08473, meta-val acc: 0.88000\n",
      "[epo 37/50, epi 100/100] => meta-training loss: 0.09803, meta-training acc: 0.68000, meta-val loss: 0.08247, meta-val acc: 0.88000\n",
      "[epo 38/50, epi 50/100] => meta-training loss: 0.07178, meta-training acc: 0.84000, meta-val loss: 0.09172, meta-val acc: 0.64000\n",
      "[epo 38/50, epi 100/100] => meta-training loss: 0.09894, meta-training acc: 0.84000, meta-val loss: 0.08860, meta-val acc: 0.88000\n",
      "[epo 39/50, epi 50/100] => meta-training loss: 0.12230, meta-training acc: 0.60000, meta-val loss: 0.11425, meta-val acc: 0.60000\n",
      "[epo 39/50, epi 100/100] => meta-training loss: 0.09469, meta-training acc: 0.72000, meta-val loss: 0.09674, meta-val acc: 0.76000\n",
      "[epo 40/50, epi 50/100] => meta-training loss: 0.07970, meta-training acc: 0.88000, meta-val loss: 0.07609, meta-val acc: 0.84000\n",
      "[epo 40/50, epi 100/100] => meta-training loss: 0.06461, meta-training acc: 0.80000, meta-val loss: 0.08541, meta-val acc: 0.76000\n",
      "[epo 41/50, epi 50/100] => meta-training loss: 0.03594, meta-training acc: 0.92000, meta-val loss: 0.06979, meta-val acc: 0.80000\n",
      "[epo 41/50, epi 100/100] => meta-training loss: 0.06047, meta-training acc: 0.88000, meta-val loss: 0.09804, meta-val acc: 0.72000\n",
      "[epo 42/50, epi 50/100] => meta-training loss: 0.06562, meta-training acc: 0.96000, meta-val loss: 0.09961, meta-val acc: 0.76000\n",
      "[epo 42/50, epi 100/100] => meta-training loss: 0.07373, meta-training acc: 0.88000, meta-val loss: 0.14935, meta-val acc: 0.44000\n",
      "[epo 43/50, epi 50/100] => meta-training loss: 0.06368, meta-training acc: 0.84000, meta-val loss: 0.11564, meta-val acc: 0.76000\n",
      "[epo 43/50, epi 100/100] => meta-training loss: 0.06310, meta-training acc: 0.96000, meta-val loss: 0.04254, meta-val acc: 1.00000\n",
      "[epo 44/50, epi 50/100] => meta-training loss: 0.12119, meta-training acc: 0.80000, meta-val loss: 0.12314, meta-val acc: 0.80000\n",
      "[epo 44/50, epi 100/100] => meta-training loss: 0.07570, meta-training acc: 0.88000, meta-val loss: 0.11567, meta-val acc: 0.52000\n",
      "[epo 45/50, epi 50/100] => meta-training loss: 0.06293, meta-training acc: 0.92000, meta-val loss: 0.07467, meta-val acc: 0.72000\n",
      "[epo 45/50, epi 100/100] => meta-training loss: 0.07300, meta-training acc: 0.96000, meta-val loss: 0.09521, meta-val acc: 0.64000\n",
      "[epo 46/50, epi 50/100] => meta-training loss: 0.07408, meta-training acc: 0.88000, meta-val loss: 0.04622, meta-val acc: 0.96000\n",
      "[epo 46/50, epi 100/100] => meta-training loss: 0.06116, meta-training acc: 0.96000, meta-val loss: 0.12394, meta-val acc: 0.56000\n",
      "[epo 47/50, epi 50/100] => meta-training loss: 0.07285, meta-training acc: 0.80000, meta-val loss: 0.13812, meta-val acc: 0.52000\n",
      "[epo 47/50, epi 100/100] => meta-training loss: 0.11665, meta-training acc: 0.60000, meta-val loss: 0.04929, meta-val acc: 1.00000\n",
      "[epo 48/50, epi 50/100] => meta-training loss: 0.06615, meta-training acc: 0.92000, meta-val loss: 0.05762, meta-val acc: 0.88000\n",
      "[epo 48/50, epi 100/100] => meta-training loss: 0.05844, meta-training acc: 0.96000, meta-val loss: 0.16517, meta-val acc: 0.56000\n",
      "[epo 49/50, epi 50/100] => meta-training loss: 0.05348, meta-training acc: 0.88000, meta-val loss: 0.08945, meta-val acc: 0.72000\n",
      "[epo 49/50, epi 100/100] => meta-training loss: 0.04808, meta-training acc: 0.92000, meta-val loss: 0.06996, meta-val acc: 0.88000\n",
      "[epo 50/50, epi 50/100] => meta-training loss: 0.05321, meta-training acc: 0.88000, meta-val loss: 0.07365, meta-val acc: 0.84000\n",
      "[epo 50/50, epi 100/100] => meta-training loss: 0.06067, meta-training acc: 0.88000, meta-val loss: 0.08908, meta-val acc: 0.80000\n",
      "Testing...\n",
      "[meta-test episode 50/1000] => loss: 0.11245, acc: 0.60000\n",
      "[meta-test episode 100/1000] => loss: 0.04593, acc: 1.00000\n",
      "[meta-test episode 150/1000] => loss: 0.04212, acc: 1.00000\n",
      "[meta-test episode 200/1000] => loss: 0.08363, acc: 0.80000\n",
      "[meta-test episode 250/1000] => loss: 0.04825, acc: 0.88000\n",
      "[meta-test episode 300/1000] => loss: 0.07017, acc: 0.80000\n",
      "[meta-test episode 350/1000] => loss: 0.09858, acc: 0.76000\n",
      "[meta-test episode 400/1000] => loss: 0.05687, acc: 0.80000\n",
      "[meta-test episode 450/1000] => loss: 0.08859, acc: 0.96000\n",
      "[meta-test episode 500/1000] => loss: 0.08249, acc: 0.88000\n",
      "[meta-test episode 550/1000] => loss: 0.03556, acc: 0.96000\n",
      "[meta-test episode 600/1000] => loss: 0.05220, acc: 0.92000\n",
      "[meta-test episode 650/1000] => loss: 0.09422, acc: 0.68000\n",
      "[meta-test episode 700/1000] => loss: 0.06321, acc: 0.92000\n",
      "[meta-test episode 750/1000] => loss: 0.06047, acc: 0.88000\n",
      "[meta-test episode 800/1000] => loss: 0.05480, acc: 0.88000\n",
      "[meta-test episode 850/1000] => loss: 0.05189, acc: 0.92000\n",
      "[meta-test episode 900/1000] => loss: 0.11841, acc: 0.64000\n",
      "[meta-test episode 950/1000] => loss: 0.07955, acc: 0.92000\n",
      "[meta-test episode 1000/1000] => loss: 0.11149, acc: 0.68000\n",
      "Average Meta-Test Accuracy: 0.82832, Meta-Test Accuracy Std: 0.11502\n"
     ]
    }
   ],
   "source": [
    "for p in range(100,101,25):\n",
    "    p = p/100\n",
    "    print(f'\\n========================Performing scale={p}=========================\\n')\n",
    "    run_protonet(data_path='./omniglot', n_way=5, k_shot=1, n_query=5, n_meta_test_way=5,\n",
    "                 k_meta_test_shot=1, n_meta_test_query=5,shear=None,scale=p,rel=True,n_epochs=50,latent_dim = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
